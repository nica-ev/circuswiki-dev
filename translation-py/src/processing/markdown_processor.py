# translation-py/src/processing/markdown_processor.py

from typing import List, Tuple, Dict, Any, Optional, MutableMapping
import markdown_it
from markdown_it.token import Token
from ..utils.types import TranslationMap, TextSegment # Uncommented import
import re # Add import for regex
import copy # For deep copying tokens if needed
# Placeholder types if real ones aren't ready
# class TranslationMap:
#     def __init__(self):
#         self.segments = []
#     def addSegment(self, segment):
#         self.segments.append(segment)
# class TextSegment:
#     def __init__(self, text, type, path):
#         self.text = text
#         self.type = type
#         self.path = path
# Import the custom plugin
from .md_plugins.wikilinks import wikilinks_plugin
# Import the new attributes plugin
from .md_plugins.attributes import attributes_plugin
import logging # Added logging

def get_element_path(
    token_stack: List[Token],
    counts: MutableMapping[str, int], # Use the passed-in counts again!
    table_context: Optional[Dict] = None
) -> str:
    """Generates a path based on token stack, using and updating the provided counts dict."""
    if not token_stack: 
        return ""
        
    path_parts = []
    # Use a temporary dict just to track counts *within this specific stack trace* for suffix generation
    current_stack_counts = {} 

    for i, t in enumerate(token_stack):
        base_part_key = f"{t.type}_{t.level}"
        if t.tag: 
            base_part_key += f"_{t.tag}"
        
        # Increment the *overall* count for this key in the passed-in dict
        counts[base_part_key] = counts.get(base_part_key, 0) + 1
        count_suffix = f"_{counts[base_part_key]-1}" 
        
        part = base_part_key + count_suffix
        path_parts.append(part)

    # --- Revised Table Path Logic --- 
    # If the last token is a table cell opening, enhance the last path part?
    # Or just return the full stack path? Let's try returning the full stack path
    # generated by the loop, as maybe the table context was never needed here.
    # last_token = token_stack[-1]
    # if table_context and last_token.type in ['th_open', 'td_open']:
        # table_part = f"table_{table_context.get('table_index', '?')}"
        # row_part = f"tr_{table_context.get('row_index', '?')}"
        # cell_part = f"{table_context.get('cell_type', 'cell')}_{table_context.get('cell_index', '?')}"
        # # Maybe append this to the existing path?
        # path_parts[-1] += f" [Table:{table_part}>{row_part}>{cell_part}]" 
        # # Or maybe the simple stack path was correct all along?
        # pass # Do nothing special, let the stack path be returned
    # --- End Revised Table Path Logic ---

    # Return the path generated purely from the stack and overall counts
    return ' > '.join(path_parts)

class MarkdownProcessor:
    """Processes Markdown content to extract translatable segments."""

    def __init__(self):
        # Initialize markdown-it parser, enable table, use wikilinks and attributes plugins
        self.md = markdown_it.MarkdownIt()\
            .enable('table')\
            .use(wikilinks_plugin)\
            .use(attributes_plugin) # Enable attributes plugin
        self.logger = logging.getLogger(__name__) # Added logger

    def _extract_inline_text(self, tokens: List[Token]) -> str:
        """
        Extracts and concatenates translatable text content directly from
        a list of markdown-it inline tokens.
        """
        text = ""
        i = 0
        while i < len(tokens):
            token = tokens[i]
            # print(f"DEBUG _extract: Token={token.type}, Content='{token.content[:20]}'") # Optional debug

            if token.type == 'text':
                text += token.content
                i += 1
            elif token.type == 'softbreak':
                text += ' '
                i += 1
            elif token.type == 'hardbreak':
                text += '\n'
                i += 1
            elif token.type == 'link_open':
                # Find matching link_close, extract text in between
                j = i + 1
                link_children = []
                nesting_level = 1
                while j < len(tokens):
                    inner = tokens[j]
                    if inner.type == 'link_open': nesting_level += 1
                    elif inner.type == 'link_close':
                        nesting_level -= 1
                        if nesting_level == 0:
                            # Recursively process children
                            text += self._extract_inline_text(link_children)
                            i = j + 1 # Move past link_close
                            break
                    elif nesting_level == 1: # Only add direct children
                        link_children.append(inner)
                    j += 1
                if nesting_level != 0: # Fallback if no close found
                    i += 1
            elif token.type == 'image':
                # Extract alt text
                text += token.content
                i += 1
            elif token.type == 'wikilink_open':
                # Find alias or close
                j = i + 1
                alias_found = False
                while j < len(tokens):
                    inner = tokens[j]
                    if inner.type == 'wikilink_alias':
                        text += inner.content # Add alias
                        alias_found = True
                    elif inner.type == 'wikilink_close':
                        i = j + 1 # Move past close
                        break
                    j += 1
                if i == j: # Fallback if no close found
                     i += 1
            elif token.type == 'html_inline' and token.content.strip().lower() in ['<br>', '<br/>']:
                text += ' ' # Treat <br> like a space
                i += 1
            elif token.type in [
                'em_open', 'em_close', 'strong_open', 'strong_close',
                'code_inline', 
                'html_inline', # Skip other inline HTML
                'link_close', # Already handled by link_open logic
                'wikilink_target', 'wikilink_separator', 'wikilink_close', # Handled by wikilink_open
                'attribute_open', 'attribute_content', 'attribute_close',
                # Potentially other non-text types
            ]:
                # Skip these tokens entirely
                i += 1
            else:
                # Unknown token type, skip it for safety
                # print(f"WARN: Skipping unknown inline token type: {token.type}")
                i += 1

        # Final whitespace cleanup
        return re.sub(r'\s+', ' ', text).strip()

    def extract_translatable_segments(self, markdown_content: str) -> TranslationMap:
        """
        Parses Markdown content and extracts translatable text segments.
        Focus for Task 7.3: Paragraphs, Headings, List Items, Blockquotes.

        Args:
            markdown_content: The Markdown string to process.

        Returns:
            A TranslationMap object containing the extracted segments.
        """
        tokens: List[Token] = self.md.parse(markdown_content)
        translation_map = TranslationMap()
        token_stack: List[Token] = []
        i = 0
        table_context: Optional[Dict] = None
        table_count = 0
        path_counts: Dict[str, int] = {}

        while i < len(tokens):
            token = tokens[i]

            # --- Skip Code/HTML Blocks --- 
            if token.type in ['fence', 'code_block', 'html_block']:
                i += 1
                continue
            
            # --- Manage Table Context --- 
            if token.type == 'table_open':
                table_count += 1
                table_context = {'table_index': table_count, 'row_index': 0, 'cell_index': 0, 'in_header': False}
            elif token.type == 'table_close' and table_context:
                table_context = None 
            elif table_context:
                 if token.type == 'thead_open': table_context['in_header'] = True; table_context['row_index'] = 0 
                 elif token.type == 'tbody_open': table_context['in_header'] = False; table_context['row_index'] = 0 
                 elif token.type == 'tr_open': table_context['row_index'] += 1; table_context['cell_index'] = 0 
                 elif token.type == 'th_open' or token.type == 'td_open': table_context['cell_index'] += 1; table_context['cell_type'] = token.tag
            
            # --- Process Translatable Blocks --- 
            segment_text: Optional[str] = None
            segment_type: Optional[str] = None
            segment_path: Optional[str] = None
            is_block_opening = token.type.endswith('_open')
            is_block_closing = token.type.endswith('_close')

            if is_block_opening:
                token_stack.append(token)
                
                # Check if this block start immediately contains inline content
                if token.type in ['heading_open', 'paragraph_open', 'th_open', 'td_open', 'list_item_open', 'blockquote_open']:
                    if i + 1 < len(tokens) and tokens[i+1].type == 'inline':
                        inline_token = tokens[i+1]
                        segment_text = self._extract_inline_text(inline_token.children or [])
                        segment_text = re.sub(r'\s+', ' ', segment_text).strip()
                        
                        # Determine segment type based on the opening token
                        if token.type == 'heading_open': segment_type = f'heading_{token.tag[1]}'
                        elif token.type == 'th_open': segment_type = 'table_header_cell'
                        elif token.type == 'td_open': segment_type = 'table_data_cell'
                        elif token.type == 'paragraph_open':
                            parent_token = token_stack[-2] if len(token_stack) > 1 else None
                            if parent_token and parent_token.type == 'list_item_open': segment_type = 'paragraph_in_list' 
                            elif parent_token and parent_token.type == 'blockquote_open': segment_type = 'paragraph_in_blockquote'
                            else: segment_type = 'paragraph'
                        elif token.type == 'list_item_open':
                             # List items might have paragraph inside, handle paragraph_open instead?
                             # For now, let's assume direct inline content in list items is possible but less common
                             segment_type = 'list_item' # Or maybe skip direct list item content?
                        elif token.type == 'blockquote_open':
                             # Similar to list items, usually contains paragraph_open
                             segment_type = 'blockquote' # Or maybe skip?

                        # Generate path and add segment *immediately* after pushing the opening token
                        if segment_text is not None and segment_type and segment_text:
                            segment_path = get_element_path(token_stack, path_counts, table_context)
                            translation_map.addSegment(TextSegment(segment_text, segment_type, segment_path))
                            # print(f"DEBUG Extract: Added Segment - Path={segment_path}, Type={segment_type}, Text='{segment_text[:20]}...'")

            elif is_block_closing:
                # Pop matching opening tag from stack
                if token_stack and token_stack[-1].type == token.type.replace('_close', '_open'):
                    token_stack.pop()
                else:
                    self.logger.warning(f"Mismatched closing tag: {token.type} vs stack top {token_stack[-1].type if token_stack else 'Empty'}")
            
            # Always advance by 1 with this simplified logic
            i += 1
            
        if token_stack: # Check for unclosed tags at the end
             self.logger.warning(f"Unclosed tags at end of parsing: {[t.type for t in token_stack]}")

        return translation_map

    def reassemble_markdown(self, original_markdown: str, translated_segments: TranslationMap) -> str:
        """
        Reassembles the Markdown document with translated text segments.
        Currently uses Strategy A: Replaces the content of the first text node
        within an inline sequence.
        """
        try:
            tokens = self.md.parse(original_markdown)
        except Exception as e:
            self.logger.exception(f"Markdown parsing failed during reassembly: {e}")
            return original_markdown # Return original if parsing fails
            
        path_counts: MutableMapping[str, int] = {}
        token_stack: List[Token] = []
        table_context: Optional[Dict] = None # For table cell tracking

        # Deep copy tokens to avoid modifying the original parse result if reused
        try:
            tokens_copy = copy.deepcopy(tokens)
        except Exception as e:
             self.logger.exception(f"Failed to deep copy tokens: {e}")
             # Fallback or re-raise? For now, maybe try rendering original tokens?
             return self.md.renderer.render(tokens, self.md.options, {}) 

        i = 0
        # Initialize table_count for reassembly loop
        table_count = 0 
        while i < len(tokens_copy):
            token = tokens_copy[i]

            # --- Context Management (Path Stack & Table Tracking) ---
            is_block_opening = token.type.endswith('_open') and token.nesting == 1
            is_block_closing = token.type.endswith('_close') and token.nesting == -1

            # === COPY EXACT Table Context Logic from Extraction ===
            if token.type == 'table_open':
                table_count += 1
                table_context = {'table_index': table_count, 'row_index': 0, 'cell_index': 0, 'in_header': False}
            elif token.type == 'table_close' and table_context:
                table_context = None
            elif table_context:
                 if token.type == 'thead_open':
                     table_context['in_header'] = True
                     table_context['row_index'] = 0
                 elif token.type == 'thead_close' and table_context: table_context['in_header'] = False
                 elif token.type == 'tr_open' and table_context: table_context['row_index'] += 1; table_context['cell_index'] = 0
                 elif token.type in ['th_open', 'td_open'] and table_context: table_context['cell_index'] += 1
            # === END COPIED LOGIC ===

            # --- Stack Management --- (Same as before)
            if is_block_opening:
                token_stack.append(token)
            elif is_block_closing and token_stack and token_stack[-1].type == token.type.replace('_close', '_open'):
                token_stack.pop()
            elif not token_stack and token.nesting != 0:
                 self.logger.warning(f"Token stack empty but token has nesting: {token}")
                 pass

            # --- Translation Replacement (Strategy A) ---
            is_translatable_block_start = token.type in [
                'paragraph_open', 'heading_open',
                'list_item_open', 'blockquote_open',
                'th_open', 'td_open'
            ]

            if is_translatable_block_start and i + 1 < len(tokens_copy) and tokens_copy[i+1].type == 'inline':
                inline_token = tokens_copy[i+1]
                try:
                    # REMOVED DEBUG PRINT - Reassembly
                    current_path = get_element_path(list(token_stack), path_counts, table_context)
                except Exception as e:
                    self.logger.exception(f"Error generating path for token {token}: {e}")
                    current_path = None
                    
                if current_path:
                    translated_text = translated_segments.get(current_path)
                    if translated_text is not None:
                        # Strategy A: Find the first text token in children and replace its content
                        first_text_child_found = False
                        if inline_token.children:
                            for child_idx, child_token in enumerate(inline_token.children):
                                if child_token.type == 'text':
                                    self.logger.debug(f"Replacing text in path '{current_path}'. Original: '{child_token.content[:30]}...'")
                                    child_token.content = translated_text # Replace content
                                    inline_token.children = inline_token.children[:child_idx+1] 
                                    first_text_child_found = True
                                    break
                        
                        if not first_text_child_found:
                             self.logger.debug(f"No text child found for path '{current_path}', inserting new text token.")
                             inline_token.children.insert(0, Token(type='text', tag='', nesting=0, content=translated_text))
            i += 1 # Move to the next token

        # Render the modified token stream back to HTML
        try:
            rendered_html = self.md.renderer.render(tokens_copy, self.md.options, {})
            return rendered_html
        except Exception as e:
             self.logger.exception(f"Error rendering modified tokens to HTML: {e}")
             # Fallback: Try rendering original tokens?
             try:
                 return self.md.renderer.render(tokens, self.md.options, {})
             except Exception as re:
                  self.logger.exception(f"Fallback rendering also failed: {re}")
                  return f"<p>Error during reassembly: {e}</p>" # Return error message

# Example Usage (for testing/dev):
if __name__ == '__main__':
    processor = MarkdownProcessor()
    md_example = """
# Header 1

This is a paragraph with *emphasis*.

- List item 1
- List item 2

> A blockquote here.

```python
print("ignore me")
```
    """
    tmap = processor.extract_translatable_segments(md_example)
    for segment in tmap.segments:
        print(f"Type: {segment.type}, Path: {segment.path}, Text: {segment.text}") 