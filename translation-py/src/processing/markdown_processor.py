# translation-py/src/processing/markdown_processor.py

from typing import List, Tuple, Dict, Any, Optional, MutableMapping, Callable
import markdown_it
from markdown_it.token import Token
from bs4 import BeautifulSoup, Tag, NavigableString # Added BeautifulSoup
from ..utils.types import TranslationMap, TextSegment, SegmentType # Updated import
import re # Add import for regex
import copy # For deep copying tokens if needed
import logging # Added logging
import yaml
# Corrected import for ConfigLoader (one level up)
from ..config_loader import ConfigLoader 
from pathlib import Path # Added for config loading
# Keep html_config import relative to current package (processing)
# It should be in src/html_config.py, so import from ..html_config
from ..html_config import HtmlProcessingConfig 

# Placeholder types if real ones aren't ready
# class TranslationMap:
#     def __init__(self):
#         self.segments = []
#     def addSegment(self, segment):
#         self.segments.append(segment)
# class TextSegment:
#     def __init__(self, text, type, path):
#         self.text = text
#         self.type = type
#         self.path = path
# Import the custom plugin
from .md_plugins.wikilinks import wikilinks_plugin
# Import the new attributes plugin
from .md_plugins.attributes import attributes_plugin

# Custom Exception for Processing Errors
class MarkdownProcessingError(Exception):
    """Exception raised for errors in the Markdown processing pipeline."""
    pass

def get_element_path(
    token_stack: List[Token],
    counts: MutableMapping[str, int], # Use the passed-in counts again!
    table_context: Optional[Dict] = None,
    html_context: Optional[List[str]] = None # Added context for HTML path
) -> str:
    """Generates a path based on token stack, using and updating the provided counts dict."""
    if not token_stack: 
        return ""
        
    path_parts = []
    # Use a temporary dict just to track counts *within this specific stack trace* for suffix generation
    current_stack_counts = {} 

    for i, t in enumerate(token_stack):
        base_part_key = f"{t.type}_{t.level}"
        if t.tag: 
            base_part_key += f"_{t.tag}"
        
        # Increment the *overall* count for this key in the passed-in dict
        counts[base_part_key] = counts.get(base_part_key, 0) + 1
        count_suffix = f"_{counts[base_part_key]-1}" 
        
        part = base_part_key + count_suffix
        path_parts.append(part)

    # --- Append HTML context path if available --- 
    if html_context:
        path_parts.extend(html_context)

    # --- Revised Table Path Logic --- 
    # If the last token is a table cell opening, enhance the last path part?
    # Or just return the full stack path? Let's try returning the full stack path
    # generated by the loop, as maybe the table context was never needed here.
    # last_token = token_stack[-1]
    # if table_context and last_token.type in ['th_open', 'td_open']:
        # table_part = f"table_{table_context.get('table_index', '?')}"
        # row_part = f"tr_{table_context.get('row_index', '?')}"
        # cell_part = f"{table_context.get('cell_type', 'cell')}_{table_context.get('cell_index', '?')}"
        # # Maybe append this to the existing path?
        # path_parts[-1] += f" [Table:{table_part}>{row_part}>{cell_part}]" 
        # # Or maybe the simple stack path was correct all along?
        # pass # Do nothing special, let the stack path be returned
    # --- End Revised Table Path Logic ---

    # Return the path generated purely from the stack and overall counts
    return ' > '.join(path_parts)

class MarkdownProcessor:
    """Processes Markdown content to extract translatable segments."""

    def __init__(self, config_loader: Optional[ConfigLoader] = None):
        # Initialize markdown-it parser, enable table, use wikilinks and attributes plugins
        self.md = markdown_it.MarkdownIt()\
            .enable('table')\
            .use(wikilinks_plugin)\
            .use(attributes_plugin) # Enable attributes plugin
        self.logger = logging.getLogger(__name__) # Added logger
        # Load configuration
        if config_loader is None:
            self.logger.warning("No ConfigLoader provided, attempting to load defaults.")
            try:
                 # Define paths relative to project root or use absolute paths
                 project_root = Path(__file__).parent.parent.parent # Adjust as needed
                 config_loader = ConfigLoader(
                     env_file=project_root / '.env', 
                     settings_file=project_root / 'settings.yaml',
                     html_config_file=project_root / 'config/html_config.yaml'
                 )
            except Exception as e:
                 self.logger.error(f"Failed to load default config: {e}. HTML features may be disabled.")
                 class DummyConfigLoader:
                     def get_html_config(self): return {}
                     def get_yaml_translate_fields(self): return []
                     def is_test_mode(self): return False
                 config_loader = DummyConfigLoader()
        
        self.config_loader = config_loader
        try:
            # Load the raw config dictionary
            raw_html_config_data = self.config_loader.get_html_config()
            # Instantiate the HtmlProcessingConfig class
            self.html_processing_config = HtmlProcessingConfig(raw_html_config_data)

            # Keep yaml_translate_fields and test_mode as before
            self.yaml_translate_fields = self.config_loader.get_yaml_translate_fields()
            self.test_mode = self.config_loader.is_test_mode()

        except Exception as e:
            self.logger.error(f"Error initializing configurations: {e}. Using empty/default configs.", exc_info=True)
            # Fallback: Create an empty config object
            self.html_processing_config = HtmlProcessingConfig({})
            self.yaml_translate_fields = []
            self.test_mode = False
        
        self.renderer = self.md.renderer # Store renderer instance
        self.logger.info("MarkdownProcessor initialized.")
        self.logger.debug(f"HTML Config loaded: {bool(self.html_processing_config)}") # Check the object
        if not self.html_processing_config:
            self.logger.warning("HTML config is empty. HTML processing will be skipped.")

        # Add a dictionary to store parsed soup objects for reassembly
        self.original_soup_cache: Dict[int, BeautifulSoup] = {}

    def parse(self, text: Optional[str]) -> List[Token]:
        """Parse markdown text into a list of markdown-it tokens."""
        if text is None:
            self.logger.error("Input text cannot be None")
            raise TypeError("Input text cannot be None")
        if not text:
            return []
        try:
            tokens = self.md.parse(text)
            self.logger.debug(f"Successfully parsed text into {len(tokens)} tokens.")
            # Convert token objects to dictionaries for easier handling downstream if needed
            # return [token.as_dict() for token in tokens]
            return tokens # Return raw Token objects for now
        except Exception as e:
            self.logger.exception(f"Error parsing Markdown text: {e}")
            raise MarkdownProcessingError(f"Failed to parse Markdown: {e}") from e

    def extract_frontmatter(self, text: str) -> Tuple[Dict[str, Any], str]:
        """Extract YAML frontmatter and the remaining content."""
        # Regex pattern to match frontmatter between triple dashes
        pattern = r'^---\s*\n(.*?)\n---\s*\n(.*)$'
        match = re.match(pattern, text, re.DOTALL | re.MULTILINE) # Added MULTILINE flag

        if not match:
            return {}, text
        
        yaml_part = match.group(1).strip()
        content_part = match.group(2).strip()
        
        if not yaml_part: # Handle empty block
            return {}, content_part
            
        try:
            frontmatter = yaml.safe_load(yaml_part)
            if isinstance(frontmatter, dict):
                self.logger.debug("Successfully extracted frontmatter.")
                return frontmatter, content_part
            else:
                self.logger.warning(f"Parsed frontmatter is not a dictionary (type: {type(frontmatter)}). Treating as no frontmatter.")
                return {}, text # Return original text if frontmatter isn't a dict
        except yaml.YAMLError as e:
            self.logger.warning(f"Could not parse YAML frontmatter: {e}")
            return {}, text # Return original text on YAML error

    def _traverse_html(
        self, 
        node: Tag | NavigableString, 
        translation_map: TranslationMap, 
        md_token_stack: List[Token], 
        md_token_counts: MutableMapping[str, int],
        current_html_path: List[str] # Track path within HTML
    ):
        """Recursively traverses the parsed HTML structure (BeautifulSoup node)."""
        if isinstance(node, NavigableString):
            parent_tag_name = node.parent.name.lower() if node.parent else None
            # Use the config object's method to check parent
            if parent_tag_name and self.html_processing_config.should_extract_content(parent_tag_name):
                text = str(node).strip()
                if text:
                    # Add 'text_0' for path stability, assuming one text node direct child for now
                    path = get_element_path(md_token_stack, md_token_counts, html_context=current_html_path + ['text_0'])
                    segment = TextSegment(text=text, segment_type=SegmentType.HTML_CONTENT, path=path)
                    translation_map.addSegment(segment) # Assuming addSegment exists
                    self.logger.debug(f"HTML Extracted Text: '{text[:50]}...' at path {path}")
            return

        if isinstance(node, Tag):
            tag_name = node.name.lower()
            # Generate HTML path part (e.g., p_0, div_1)
            sibling_index = 0
            # Calculate index among siblings of the same tag type
            if node.parent:
                siblings_of_type = node.parent.find_all(tag_name, recursive=False)
                try:
                    sibling_index = siblings_of_type.index(node)
                except ValueError:
                     self.logger.warning(f"Could not find node index for {tag_name} in parent {node.parent.name}. Using 0.")
            html_path_part = f"{tag_name}_{sibling_index}"
            new_html_path = current_html_path + [html_path_part]

            # 1. Check if tag should be preserved entirely using the config object
            if self.html_processing_config.should_preserve_tag(tag_name):
                self.logger.debug(f"HTML Preserving tag: <{tag_name}> at path {'.'.join(new_html_path)}")
                return # Skip this tag and its children

            # 2. Check for attribute extraction using the config object
            extractable_attrs = self.html_processing_config.get_extractable_attributes(tag_name)
            if extractable_attrs:
                for attr_name in extractable_attrs:
                    if node.has_attr(attr_name):
                        attr_value = node[attr_name].strip()
                        if attr_value:
                            # Add attribute name to the path
                            attr_path = get_element_path(md_token_stack, md_token_counts, html_context=new_html_path + [f'attr_{attr_name}'])
                            segment = TextSegment(text=attr_value, segment_type=SegmentType.HTML_ATTRIBUTE, path=attr_path)
                            translation_map.addSegment(segment) # Assuming addSegment exists
                            self.logger.debug(f"HTML Extracted Attr '{attr_name}': '{attr_value[:50]}...' from <{tag_name}> at path {attr_path}")

            # 3. Decide whether to traverse children based on config object methods
            # Traverse if content should be extracted OR if only attributes were extracted (need to go deeper)
            # OR if default behaviour is extract and it's not explicitly preserved/content-extracted
            should_traverse = (self.html_processing_config.should_extract_content(tag_name) or 
                               bool(extractable_attrs) or 
                               (self.html_processing_config.default_tag_behavior == 'extract' and 
                                not self.html_processing_config.should_preserve_tag(tag_name) and 
                                tag_name not in self.html_processing_config.extract_content_tags))

            if should_traverse:
                self.logger.debug(f"HTML Traversing children of <{tag_name}> at path {'.'.join(new_html_path)}")
                for child in node.children:
                    # Pass the new HTML path context down
                    self._traverse_html(child, translation_map, md_token_stack, md_token_counts, new_html_path)
            else:
                 self.logger.debug(f"HTML Skipping children of tag: <{tag_name}> based on config at path {'.'.join(new_html_path)}")

    def _process_html_block(self, token_index: int, html_content: str, translation_map: TranslationMap, md_token_stack: List[Token], path_counts: MutableMapping[str, int]):
        """Parses and extracts translatable segments from an HTML block."""
        self.logger.debug(f"Processing HTML block at index {token_index}: {html_content[:100]}...")
        
        # Check config *before* parsing. Skip if config object doesn't exist
        # or if it has no extraction rules defined.
        if not self.html_processing_config or \
           not (self.html_processing_config.extract_content_tags or 
                self.html_processing_config.extract_attribute_tags or 
                self.html_processing_config.default_tag_behavior == 'extract'): 
             self.logger.warning("Skipping HTML block processing: HTML config missing or no extraction rules active.")
             return
             
        try:
            # Parse only if config allows potential extraction
            soup = BeautifulSoup(html_content, 'lxml')
            # Store the original parsed soup object using the token's index as the key
            self.original_soup_cache[token_index] = soup
            
            if soup.contents:
                 self._traverse_html(soup, translation_map, md_token_stack, path_counts, []) 
            else:
                self.logger.debug("HTML block produced empty soup content.")

        except Exception as e:
            self.logger.error(f"Error parsing HTML block with BeautifulSoup: {e}\\nContent: {html_content[:200]}", exc_info=True)

    def _extract_inline_text(
        self, 
        tokens: List[Token], 
        translation_map: TranslationMap, 
        md_token_stack: List[Token], 
        md_token_counts: MutableMapping[str, int],
        parent_token_index: int # Need index of parent inline token
    ):
        """
        Extracts translatable text from markdown-it inline tokens, handling nesting.
        Adds extracted segments directly to the translation_map.
        Now handles html_inline tokens using BeautifulSoup.
        """
        i = 0
        while i < len(tokens):
            token = tokens[i]
            current_text = ""
            start_index = i

            # Accumulate consecutive text/softbreak/hardbreak into one segment
            while i < len(tokens) and tokens[i].type in ['text', 'softbreak', 'hardbreak']:
                t = tokens[i]
                if t.type == 'text': current_text += t.content
                elif t.type == 'softbreak': current_text += ' '
                elif t.type == 'hardbreak': current_text += '\n'
                i += 1
            
            stripped_text = current_text.strip()
            if stripped_text:
                # Use the first token of the sequence for path context
                path = get_element_path(md_token_stack + [tokens[start_index]], md_token_counts)
                segment = TextSegment(text=stripped_text, segment_type=SegmentType.TEXT, path=path)
                translation_map.addSegment(segment)
                self.logger.debug(f"Inline Extracted Text: '{segment.text[:50]}...' at path {path}")
            
            # If we didn't advance (didn't process text tokens), process the non-text token
            if i == start_index:
                token = tokens[i]
                if token.type == 'link_open':
                    # Find matching close tag and recurse on children
                    j, link_children = self._find_matching_close(tokens, i, 'link_open', 'link_close')
                    if j > i: # Found matching close
                         md_token_stack.append(token)
                         self._extract_inline_text(link_children, translation_map, md_token_stack, md_token_counts, parent_token_index)
                         md_token_stack.pop()
                         i = j # Move past the processed block (including close tag)
                    else: 
                         i += 1 # Skip unclosed tag
                elif token.type == 'image':
                    alt_text = token.content.strip()
                    if alt_text:
                        # Path context uses the image token itself
                        path = get_element_path(md_token_stack + [token], md_token_counts, html_context=['attr_alt'])
                        segment = TextSegment(text=alt_text, segment_type=SegmentType.IMAGE_ALT, path=path)
                        translation_map.addSegment(segment)
                        self.logger.debug(f"Image Alt Extracted: '{segment.text[:50]}...' at path {path}")
                    i += 1
                elif token.type == 'wikilink_open':
                    # Find alias within the wikilink structure
                    j, alias_token = self._find_wikilink_alias(tokens, i)
                    if alias_token:
                        alias_text = alias_token.content.strip()
                        if alias_text:
                            # Path context uses the alias token
                            path = get_element_path(md_token_stack + [alias_token], md_token_counts)
                            segment = TextSegment(text=alias_text, segment_type=SegmentType.WIKILINK_ALIAS, path=path)
                            translation_map.addSegment(segment)
                            self.logger.debug(f"Wikilink Alias Extracted: '{segment.text[:50]}...' at path {path}")
                    i = j # Move past the wikilink block (found or not)
                elif token.type == 'html_inline': 
                    # Process Inline HTML using BeautifulSoup
                    inline_token_index = parent_token_index # Use parent inline token's index
                    try:
                         soup = BeautifulSoup(token.content, 'lxml') 
                         # Store the soup object for this inline HTML
                         self.original_soup_cache[inline_token_index] = soup
                         
                         # Path generation needs context from the MD token stack
                         html_token_path_part = f"html_inline_{md_token_counts.get('html_inline_0', 0)}"
                         md_token_counts['html_inline_0'] = md_token_counts.get('html_inline_0', 0) + 1
                         
                         md_token_stack.append(token)
                         html_root_path = [html_token_path_part]
                         for child_node in soup.contents: 
                              self._traverse_html(child_node, translation_map, md_token_stack, md_token_counts, html_root_path)
                         md_token_stack.pop()
                    except Exception as e:
                         self.logger.error(f"Error parsing inline HTML: {token.content}. Error: {e}", exc_info=True)
                    i += 1
                elif token.type in ['em_open', 'strong_open']:
                    # Find matching close tag and recurse on children
                    close_type = token.type.replace('_open', '_close')
                    j, nested_children = self._find_matching_close(tokens, i, token.type, close_type)
                    if j > i:
                        md_token_stack.append(token)
                        self._extract_inline_text(nested_children, translation_map, md_token_stack, md_token_counts, parent_token_index)
                        md_token_stack.pop()
                        i = j
                    else:
                        i += 1
                else:
                    # Skip other tokens (code_inline, attribute*, close tags handled by callers, etc.)
                    self.logger.debug(f"Skipping inline token type: {token.type}")
                    i += 1

    def _find_matching_close(self, tokens: List[Token], open_index: int, open_type: str, close_type: str) -> Tuple[int, List[Token]]:
        """Helper to find matching close token and extract children."""
        j = open_index + 1
        children = []
        nesting_level = 1
        while j < len(tokens):
            inner = tokens[j]
            if inner.type == open_type: nesting_level += 1
            elif inner.type == close_type:
                nesting_level -= 1
                if nesting_level == 0:
                    return j + 1, children # Return index *after* close tag, and children
            elif nesting_level == 1:
                children.append(inner)
            j += 1
        self.logger.warning(f"Could not find matching close tag for {open_type} starting at index {open_index}")
        return open_index + 1, [] # Indicate failure by returning next index and no children

    def _find_wikilink_alias(self, tokens: List[Token], open_index: int) -> Tuple[int, Optional[Token]]:
        """Helper to find wikilink alias token and end index."""
        j = open_index + 1
        alias_token = None
        while j < len(tokens):
            inner = tokens[j]
            if inner.type == 'wikilink_alias':
                alias_token = inner
            elif inner.type == 'wikilink_close':
                return j + 1, alias_token # Return index *after* close, and alias token (or None)
            j += 1
        self.logger.warning(f"Could not find wikilink_close for wikilink_open at index {open_index}")
        return open_index + 1, None # Indicate failure
        
    def extract_translatable_segments(
        self, 
        markdown_content: str, 
        frontmatter: Optional[Dict[str, Any]] = None
    ) -> TranslationMap:
        """
        Parses Markdown content and specified YAML frontmatter fields,
        extracting translatable text segments.

        Args:
            markdown_content: The Markdown string to process (without frontmatter).
            frontmatter: The dictionary representing the parsed YAML frontmatter.

        Returns:
            A TranslationMap object containing the extracted segments from both
            Markdown body and specified YAML fields.
        """
        self.logger.info("Starting translatable segment extraction...")
        tokens = self.parse(markdown_content)
        translation_map = TranslationMap()
        token_stack: List[Token] = []
        i = 0
        table_context: Optional[Dict] = None
        table_count = 0
        path_counts: Dict[str, int] = {}

        # Reset soup cache for each new extraction
        self.original_soup_cache = {} 

        # --- 1. Extract from Markdown Body --- 
        self.logger.debug("Extracting segments from Markdown body...")
        while i < len(tokens):
            token = tokens[i]
            current_token_index = i # Store index for caching key

            # --- Skip Code Blocks --- 
            if token.type in ['fence', 'code_block']: 
                self.logger.debug(f"Skipping non-translatable block: {token.type}")
                i += 1
                continue
            
            # --- Process HTML Blocks ---
            elif token.type == 'html_block':
                # Pass the current token index to store the soup object
                self._process_html_block(current_token_index, token.content, translation_map, token_stack, path_counts)
                i += 1 
                continue 
            
            # --- Manage Table Context --- 
            if token.type == 'table_open':
                table_count += 1
                table_context = {'table_index': table_count, 'row_index': 0, 'cell_index': 0, 'in_header': False}
            elif token.type == 'table_close' and table_context:
                table_context = None 
            elif table_context:
                 if token.type == 'thead_open': table_context['in_header'] = True; table_context['row_index'] = 0 
                 elif token.type == 'tbody_open': table_context['in_header'] = False; table_context['row_index'] = 0 
                 elif token.type == 'tr_open': table_context['row_index'] += 1; table_context['cell_index'] = 0 
                 elif token.type == 'th_open' or token.type == 'td_open': 
                     table_context['cell_index'] += 1
                     table_context['cell_tag'] = token.tag # Store td or th
            # --- End Table Context --- 

            # --- Manage Token Stack --- 
            if 'close' in token.type and token_stack:
                if token_stack[-1].type == token.type.replace('_close', '_open'):
                    token_stack.pop()
                else:
                    # Log potential mismatch but pop anyway for robustness?
                    self.logger.warning(f"Token stack mismatch: Expected {token_stack[-1].type.replace('_open', '_close')}, got {token.type} at level {token.level}")
                    if token_stack: token_stack.pop() # Pop cautiously
            elif 'open' in token.type:
                 token_stack.append(token)
            # --- End Token Stack --- 

            # --- Extract Text Content --- 
            if token.type == 'inline' and token.content:
                # Pass the current token index (parent of inline children) 
                # NOTE: This assumes _extract_inline_text handles caching based on its PARENT index
                self._extract_inline_text(token.children, translation_map, token_stack, path_counts, current_token_index)
                # Do NOT add segment here, _extract_inline_text does it
            
            i += 1 # Increment i for the main loop
        
        # --- 2. Extract from YAML Frontmatter --- 
        self.logger.debug("Extracting segments from YAML frontmatter...")
        yaml_fields_to_translate = self.yaml_translate_fields
        if frontmatter and yaml_fields_to_translate:
            for field_name in yaml_fields_to_translate:
                if field_name in frontmatter:
                    field_value = frontmatter[field_name]
                    # Only translate string fields for now
                    if isinstance(field_value, str) and field_value.strip():
                        # Define a path convention for YAML fields
                        yaml_path = f"yaml > {field_name}"
                        segment = TextSegment(text=field_value.strip(), type=SegmentType.YAML, path=yaml_path)
                        translation_map.add_segment(segment)
                        self.logger.debug(f"Extracted YAML segment: Path='{yaml_path}', Text='{field_value[:50]}...'")
                    elif field_value: # Log if field exists but isn't a translatable string
                        self.logger.debug(f"Skipping YAML field '{field_name}': Not a non-empty string (type: {type(field_value)})")
                else:
                     self.logger.debug(f"Skipping YAML field '{field_name}': Not found in frontmatter.")
        else:
             self.logger.debug("No YAML fields configured or no frontmatter provided.")

        self.logger.info(f"Extraction complete. Found {len(translation_map.segments)} segments.")
        return translation_map

    def _find_token_by_path(self, tokens: List[Token], path: str) -> Optional[Token]:
        """Finds a token in the AST based on its path (basic implementation)."""
        # This is a simplified version. A robust implementation needs to handle the path structure correctly.
        # The current path structure might need refinement.
        try:
            # Assuming path is like "paragraph_open_1.inline_2" or just "paragraph_open_1"
            # For now, we just use the index from the last part of the path.
            # This will break if paths become more complex.
            parts = path.split('.')
            last_part = parts[-1]
            # Extract index if it exists after an underscore
            match = re.search(r'_(\d+)$', last_part)
            if match:
                index = int(match.group(1))
                if 0 <= index < len(tokens):
                    # The path points to the opening token. We need the corresponding inline token.
                    opening_token = tokens[index]
                    if opening_token.type.endswith('_open') and index + 1 < len(tokens) and tokens[index+1].type == 'inline':
                        return tokens[index+1] # Return the inline token
                    elif opening_token.type == 'inline': # Maybe path points directly to inline? Needs review.
                         return opening_token
            self.logger.warning(f"Could not parse index from path part: {last_part} in path {path}")
            return None
        except (IndexError, ValueError, TypeError) as e:
            self.logger.error(f"Error finding token by path '{path}': {e}")
            return None
        
    def replace_node_content(self, token: Token, translated_text: str) -> bool:
        """
        Replace the content of the first text child within an inline token.

        Args:
            token: The inline token containing text to replace
            translated_text: The new text content

        Returns:
            bool: True if replacement succeeded, False otherwise
        """
        # WARNING: This is a simplified replacement that loses inline formatting.
        # See Task 10.4 for implementation of formatting preservation.
        if token.type != 'inline':
            self.logger.error(f"Expected inline token, got {token.type}")
            return False

        children = token.children
        if children is None:
            self.logger.warning(f"Inline token has no children list. Path: {token.path}")
            return False
        
        if not isinstance(children, list):
             self.logger.error(f"Inline token children is not a list. Path: {token.path}")
             return False

        found_text_child = False
        for i, child_token in enumerate(children):
            # Check type using attribute access
            if isinstance(child_token, Token) and child_token.type == 'text':
                # Access content using attribute
                original_content = child_token.content 
                # Update content using attribute
                child_token.content = translated_text 
                # According to Strategy A, remove subsequent children 
                # This simplifies reassembly but loses internal formatting.
                token.children = children[:i+1]
                # Access path using attribute if it exists (needs adding?)
                log_path = getattr(token, 'path', '[Unknown Path]')
                self.logger.debug(f"Replaced content in token path {log_path}: '{original_content[:30]}...' -> '{translated_text[:30]}...'")
                found_text_child = True
                break # Only replace the first text child
        
        if not found_text_child:
            # Access path using attribute if it exists
            log_path = getattr(token, 'path', '[Unknown Path]')
            self.logger.warning(f"No text child found in inline token to replace content. Path: {log_path}")
            return False
            
        return True

    def _parse_segment_path(self, path: str) -> Tuple[Optional[int], List[str]]:
        """
        Parses a segment path to extract the approximate token index and HTML path parts.
        Example Path: 'paragraph_open_1 > inline_0 > html_inline_0 > div_0 > p_1 > text_0'
        Another Example: 'list_item_open_3 > inline_0 > text_0' (Markdown only)
        Another Example: 'html_block_5 > div_0 > img_0 > attr_alt' 
        
        Returns:
            Tuple (token_index, html_path_parts) or (None, []) on error.
            token_index is the index of the html_block or inline token containing html_inline.
        """
        try:
            parts = path.split(' > ')
            token_index = None
            html_path_start_index = -1

            # Find the index of the source html_block or html_inline token
            for i, part in enumerate(parts):
                if part.startswith('html_block_'):
                    match = re.search(r'_(\d+)$', part)
                    if match:
                         token_index = int(match.group(1))
                         html_path_start_index = i + 1
                         break
                elif part.startswith('html_inline_'): # Part of an inline token
                     # Find the parent inline token's index
                     if i > 0 and parts[i-1].startswith('inline_'):
                         inline_part = parts[i-1]
                         parent_block_part = parts[i-2] # Assume inline is preceded by parent block
                         # Crude way to guess parent block token index
                         match_block = re.search(r'_(\d+)$', parent_block_part)
                         if match_block:
                             # The index refers to the *inline* token within the parent block
                             # which is usually parent_index + 1
                             token_index = int(match_block.group(1)) + 1 
                             html_path_start_index = i + 1 # Start HTML path after html_inline*
                             break
                     else: # html_inline might be directly under root? Unlikely.
                          self.logger.warning(f"Could not determine parent index for html_inline in path: {path}")
                          # Fallback: Try getting index from html_inline itself? Risky.
                          match_inline = re.search(r'_(\d+)$', part)
                          if match_inline: 
                              # This index isn't the main token list index
                              pass # Can't reliably get token index here

            if token_index is None:
                 # Path does not seem to contain HTML context, assume markdown only
                 return None, []
                
            html_path_parts = parts[html_path_start_index:] if html_path_start_index != -1 else []
            return token_index, html_path_parts

        except Exception as e:
            self.logger.error(f"Error parsing segment path '{path}': {e}", exc_info=True)
            return None, []

    def _find_html_node_by_path(self, soup: BeautifulSoup | Tag, html_path_parts: List[str]) -> Optional[Tag | NavigableString | tuple[Tag, str]]:
        """
        Finds a node or attribute within a BeautifulSoup object using path parts.
        Path parts like: ['div_0', 'p_1', 'text_0'] or ['p_0', 'img_0', 'attr_alt']
        Returns the target Tag, NavigableString, or a tuple (Tag, attribute_name) for attributes.
        """
        current_node: Optional[Tag | NavigableString | BeautifulSoup] = soup
        try:
            for i, part in enumerate(html_path_parts):
                if not current_node or isinstance(current_node, NavigableString):
                    self.logger.warning(f"HTML node search stopped prematurely at '{part}' in path {html_path_parts}")
                    return None
                    
                if part.startswith('attr_') or part.startswith('text_'):
                    if not isinstance(current_node, Tag):
                         self.logger.error(f"Cannot process '{part}' because current node is not a Tag (type: {type(current_node)}) in path {html_path_parts}")
                         return None
                    if part.startswith('attr_'): 
                        if i == len(html_path_parts) - 1: 
                            attr_name = part.split('_', 1)[1]
                            if current_node.has_attr(attr_name):
                                return (current_node, attr_name) 
                            else:
                                self.logger.warning(f"Target attribute '{attr_name}' not found on tag <{current_node.name}> for path {html_path_parts}")
                                return None
                        else:
                            self.logger.error(f"Invalid path: '{part}' must be the last part in {html_path_parts}")
                            return None
                    elif part.startswith('text_'):
                        if i == len(html_path_parts) - 1:
                            try:
                                text_node_index = int(part.split('_')[1])
                                text_nodes = [c for c in current_node.contents if isinstance(c, NavigableString) and str(c).strip()]
                                if 0 <= text_node_index < len(text_nodes):
                                    return text_nodes[text_node_index]
                                else:
                                    self.logger.warning(f"Text node index {text_node_index} out of range ({len(text_nodes)} found) for path {html_path_parts} in parent <{current_node.name}>")
                                    return None
                            except (ValueError, IndexError):
                                self.logger.warning(f"Could not parse or use text index from '{part}' in path {html_path_parts}")
                                return None
                        else:
                             self.logger.error(f"Invalid path: '{part}' must be the last part in {html_path_parts}")
                             return None

                else: # Target is a tag
                    match = re.match(r"([a-zA-Z0-9]+)_(\d+)", part)
                    if not match:
                        self.logger.warning(f"Could not parse tag/index from part '{part}' in path {html_path_parts}")
                        return None
                    tag_name = match.group(1)
                    index = int(match.group(2))
                    
                    # --- Revised find logic V5 --- 
                    target_tag: Optional[Tag] = None
                    if i == 0 and isinstance(current_node, BeautifulSoup):
                         # Use find_all for the *first* tag search to handle potential implicit wrappers
                         # recursive=False ensures we only search direct children
                         top_level_tags = current_node.find_all(tag_name, recursive=False)
                         if 0 <= index < len(top_level_tags):
                              target_tag = top_level_tags[index]
                         else:
                              found_tags = [t.name for t in current_node.find_all(recursive=False) if isinstance(t, Tag)]
                              self.logger.warning(f"Initial tag <{tag_name}> index {index} out of range ({len(top_level_tags)} found matching '{tag_name}' among top-level tags: {found_tags}) for path {html_path_parts}")
                              return None
                    elif isinstance(current_node, Tag):
                        # For subsequent tags, search within the *contents* only
                        children_of_type = [child for child in current_node.contents if isinstance(child, Tag) and child.name == tag_name]
                        if 0 <= index < len(children_of_type):
                            target_tag = children_of_type[index]
                        else:
                            parent_name = current_node.name
                            found_tags = [child.name for child in current_node.contents if isinstance(child, Tag)]
                            self.logger.warning(f"Tag <{tag_name}> index {index} out of range ({len(children_of_type)} found matching '{tag_name}' among children: {found_tags}) for path {html_path_parts} in parent <{parent_name}>)") 
                            return None
                    else: 
                         self.logger.error(f"HTML path search reached unexpected node type {type(current_node)} for tag part '{part}' in path {html_path_parts}")
                         return None
                    
                    current_node = target_tag # Update current node for the next iteration
                    # --- End Revised find logic V5 --- 
                         
            return current_node # Return the final found node (Tag or NavigableString via text_ part)

        except Exception as e:
            self.logger.error(f"Error finding HTML node for path {html_path_parts}: {e}", exc_info=True)
            return None

    def reassemble_markdown(
        self, 
        original_md_string: str, 
        original_tokens: List[Token], 
        translation_map: TranslationMap
    ) -> str:
        """
        Reassemble markdown with translations applied to appropriate tokens.

        Args:
            original_md_string: The source Markdown text string (for reference/fallback).
            original_tokens: The pre-parsed list of tokens from the original Markdown.
            translation_map: TranslationMap containing original segments and their translations.

        Returns:
            str: Reassembled markdown with translations.
        """
        # WARNING: Current reassembly might not perfectly preserve complex inline formatting.
        
        # Work on a deep copy of the original tokens to avoid side effects
        modified_tokens = copy.deepcopy(original_tokens)

        processed_paths = set()
        errors = 0
        # Keep track of modified soup objects, keyed by original token index
        modified_soup_cache: Dict[int, BeautifulSoup] = {}

        # --- Pass 1: Process HTML segments and modify soup objects --- 
        self.logger.info("Reassembly Pass 1: Processing HTML segments...")
        for segment in translation_map.segments: 
            path = segment.path
            translation = segment.translated_text 
            if not translation:
                 self.logger.debug(f"Skipping segment with no translation: Path {path}")
                 continue

            if segment.type not in [SegmentType.HTML_CONTENT, SegmentType.HTML_ATTRIBUTE]:
                continue 
               
            if path in processed_paths:
                 self.logger.warning(f"Skipping duplicate path for HTML segment: {path}")
                 continue

            token_index, html_path_parts = self._parse_segment_path(path)

            if token_index is None or not html_path_parts:
                self.logger.error(f"Could not parse token index or HTML path from HTML segment path: {path}")
                errors += 1
                continue

            # Retrieve original soup, copy if not yet modified
            original_soup = self.original_soup_cache.get(token_index)
            if not original_soup:
                self.logger.error(f"Original soup not found in cache for token index {token_index} (Path: {path})")
                errors += 1
                continue

            # Work on a copy of the soup for this specific token index
            if token_index not in modified_soup_cache:
                modified_soup_cache[token_index] = copy.deepcopy(original_soup)
                self.logger.debug(f"Created initial copy of soup for token index {token_index}")
               
            current_modified_soup = modified_soup_cache[token_index]

            # Find the target node/attribute in the *modified* soup
            target = self._find_html_node_by_path(current_modified_soup, html_path_parts)

            if target is None:
                self.logger.error(f"Could not find target HTML node/attribute for path {path}")
                errors += 1
                continue

            try:
                # Modify the node/attribute based on segment type
                if segment.type == SegmentType.HTML_ATTRIBUTE:
                    if isinstance(target, tuple) and len(target) == 2:
                        tag_node, attr_name = target # Unpack tuple
                        if isinstance(tag_node, Tag):
                             tag_node[attr_name] = translation
                             self.logger.debug(f"Replaced HTML Attr '{attr_name}' on <{tag_node.name}> at path {path}")
                             processed_paths.add(path) # Mark path as processed
                        else:
                            self.logger.error(f"Target for HTML_ATTRIBUTE was not a Tag node at path {path}")
                            errors += 1
                    else:
                        self.logger.error(f"_find_html_node_by_path did not return expected (Tag, attr_name) tuple for HTML_ATTRIBUTE at path {path}")
                        errors += 1
                elif segment.type == SegmentType.HTML_CONTENT:
                    if isinstance(target, NavigableString):
                        target.replace_with(translation)
                        self.logger.debug(f"Replaced HTML text node at path {path}")
                        processed_paths.add(path) # Mark path as processed
                    else:
                         # Maybe target is a tag like <p> and we replace its direct text? Risky.
                         self.logger.warning(f"Target for HTML_CONTENT at path {path} was not a text node ({type(target)}). Replacement skipped.")
                         errors += 1
                         # Do not add to processed_paths if skipped
            except Exception as e:
                self.logger.error(f"Error modifying HTML soup for path {path}: {e}", exc_info=True)
                errors += 1

        # --- Pass 2: Update Markdown tokens with modified HTML strings --- 
        self.logger.info("Reassembly Pass 2: Updating Markdown tokens with modified HTML strings...")
        for token_index, modified_soup in modified_soup_cache.items():
            if 0 <= token_index < len(modified_tokens):
                token_to_update = modified_tokens[token_index]
                if token_to_update.type in ['html_block', 'html_inline']:
                     # Render the modified soup back to string
                     # Use prettify for potentially better formatting, or just str()
                     rendered_html = str(modified_soup) 
                     token_to_update.content = rendered_html
                     self.logger.debug(f"Updated token {token_index} ({token_to_update.type}) with modified HTML.")
                else:
                     self.logger.warning(f"Token at index {token_index} intended for HTML update was not html_block/html_inline (type: {token_to_update.type})")
            else:
                 self.logger.error(f"Token index {token_index} from soup cache is out of bounds for modified_tokens (len: {len(modified_tokens)})")

        # --- Pass 3: Process non-HTML segments (existing logic using find_token_by_path?) ---
        # This needs rethinking. We already modified the tokens in place (for HTML)
        # Applying non-HTML translations requires finding the correct *original* token
        # in the *modified_tokens* list and applying changes. The simple path finding
        # might not be sufficient if structure changed.
        # For now, let's skip non-HTML reassembly if HTML was involved, 
        # as the current logic is likely broken by the deepcopy/token modification.
        # TODO: Implement robust non-HTML reassembly after HTML changes.
        self.logger.info("Reassembly Pass 3: Skipping non-HTML segment processing for now.")

        # Error Reporting
        if errors > 0:
             self.logger.error(f"Reassembly completed with {errors} errors.")
        else:
             self.logger.info("Reassembly completed successfully.")

        # Render the modified token list back to markdown string
        try:
            # Use the renderer associated with the processor's md instance
            reassembled_content = self.md.renderer.render(modified_tokens, self.md.options, {}) 
        except Exception as e:
             self.logger.exception(f"Error rendering modified tokens: {e}")
             # Fallback: return original string or raise error?
             return original_md_string # Fallback to original string

        # Frontmatter reassembly logic needs the *original full string* 
        # to extract original frontmatter if re-adding is desired.
        # This part might need rethinking depending on the overall workflow.
        # For now, just return the reassembled content.
        # TODO: Re-evaluate frontmatter handling during reassembly.
        return reassembled_content

    # --- Validation --- 

    def _compare_ast_structure(self, original_nodes: List[Token], translated_nodes: List[Token]) -> bool:
        """
        Recursively compares the structure of two AST lists (token streams).
        Only compares node types and hierarchy, not content or attributes.

        Args:
            original_nodes: The first AST list (token stream).
            translated_nodes: The second AST list (token stream).

        Returns:
            bool: True if structures match, False otherwise.
        """
        if len(original_nodes) != len(translated_nodes):
            self.logger.debug(f"_compare_ast_structure: Length mismatch ({len(original_nodes)} vs {len(translated_nodes)})")
            return False

        for i in range(len(original_nodes)):
            orig_node = original_nodes[i]
            trans_node = translated_nodes[i]

            # Compare node type
            if orig_node.type != trans_node.type:
                self.logger.debug(f"_compare_ast_structure: Type mismatch at index {i}: {orig_node.type} vs {trans_node.type}")
                return False

            # Compare nesting level
            if orig_node.level != trans_node.level:
                self.logger.debug(f"_compare_ast_structure: Level mismatch at index {i}: {orig_node.level} vs {trans_node.level}")
                return False

            # Compare presence of children (handle None or empty list)
            orig_children = orig_node.children
            trans_children = trans_node.children
            orig_has_children = bool(orig_children)
            trans_has_children = bool(trans_children)

            if orig_has_children != trans_has_children:
                self.logger.debug(f"_compare_ast_structure: Children presence mismatch at index {i}")
                return False

            # Recursively compare children if they exist
            if orig_has_children: # Both must have children if one does due to check above
                if not self._compare_ast_structure(orig_children, trans_children): # Recursive call on children lists
                    return False

        return True

    def validateReconstructedMarkdown(self, original_markdown: str, reconstructed_markdown: str) -> bool:
        """
        Validates the structural integrity of translated Markdown compared to original.

        Args:
            original_markdown: The source Markdown text.
            reconstructed_markdown: The translated Markdown text.

        Returns:
            bool: True if validation passes, False otherwise.
        """
        try:
            # Basic validation: ensure the translated content can be parsed
            self.logger.debug("Validating reconstructed markdown: Parsing...")
            translated_ast = self.parse(reconstructed_markdown)
            original_ast = self.parse(original_markdown)
            self.logger.debug("Validating reconstructed markdown: Comparing structure...")

            # Compare structure (not content) of both ASTs
            if not self._compare_ast_structure(original_ast, translated_ast):
                self.logger.warning("Structural mismatch between original and translated Markdown ASTs.")
                return False

            self.logger.debug("Reconstructed Markdown validation passed.")
            return True
        except Exception as e:
            self.logger.error(f"Validation failed during parsing or comparison: {str(e)}", exc_info=True)
            return False

    # --- Main Orchestration --- 

    def translateMarkdown(self, markdown_text: str, translations: Dict[str, str]) -> str:
        """
        Orchestrates the entire Markdown translation process.
        Args:
            markdown_text: Original Markdown content (including frontmatter).
            translations: Dictionary mapping original segment paths to translated text.
        Returns:
            str: Translated Markdown content.
        Raises:
            MarkdownProcessingError: If the translation process fails.
        """
        try:
            # 1. Extract Frontmatter and Content
            self.logger.info("Extracting frontmatter...")
            frontmatter_dict, content_to_process = self.extract_frontmatter(markdown_text)
            
            # 2. Extract Segments (which parses content and populates soup cache)
            self.logger.info("Extracting translatable segments...")
            # This generates the paths needed for the translations dict
            translation_map = self.extract_translatable_segments(content_to_process, frontmatter_dict)
            
            # Apply the provided translations to the map object
            # Check if translations is a dict, otherwise log warning or error
            if isinstance(translations, dict):
                applied_count = 0
                for segment in translation_map.segments:
                    if segment.path in translations:
                        segment.translated_text = translations[segment.path]
                        applied_count += 1
                    else:
                        # Decide what to do if translation is missing - use original?
                        segment.translated_text = segment.text # Default to original if no translation provided
                        self.logger.debug(f"No translation found for path {segment.path}, using original text.")
                self.logger.info(f"Applied {applied_count} translations to {len(translation_map.segments)} segments.")
            else:
                self.logger.warning("Translations provided is not a dictionary. Cannot apply translations.")
                # Set original text as translated_text for all segments
                for segment in translation_map.segments:
                     segment.translated_text = segment.text

            # 3. Parse original content *once* for reassembly reference
            self.logger.info("Parsing original Markdown content for reassembly...")
            original_content_tokens = self.parse(content_to_process)

            # 4. Reassemble using original string, tokens, and translated map
            self.logger.info(f"Reassembling Markdown with {len(translation_map.segments)} segments...")
            reassembled_content = self.reassemble_markdown(
                original_md_string=content_to_process, 
                original_tokens=original_content_tokens, 
                translation_map=translation_map
            )

            # 5. Validate the result (Optional but recommended)
            self.logger.info("Validating reconstructed Markdown structure...")
            if not self.validateReconstructedMarkdown(content_to_process, reassembled_content):
                # Log warning but proceed - maybe structure change is acceptable?
                self.logger.warning("Structural validation warnings occurred during reconstruction.")
            else:
                self.logger.info("Structural validation passed.")

            # 6. Re-add frontmatter if it existed
            if frontmatter_dict:
                # Use ruamel.yaml if available for better formatting preservation
                try:
                    from ruamel.yaml import YAML
                    from io import StringIO
                    yaml_parser = YAML()
                    yaml_parser.indent(mapping=2, sequence=4, offset=2)
                    string_stream = StringIO()
                    yaml_parser.dump(frontmatter_dict, string_stream)
                    frontmatter_yaml = string_stream.getvalue()
                except ImportError:
                     # Fallback to pyyaml
                    frontmatter_yaml = yaml.dump(frontmatter_dict, default_flow_style=False, sort_keys=False)
                
                return f"---\
{frontmatter_yaml}---\
\n{reassembled_content}"
            else:
                final_output = reassembled_content

            self.logger.info("Markdown translation process completed successfully.")
            return final_output

        except MarkdownProcessingError as mpe:
             self.logger.error(f"Markdown processing error during translation: {mpe}", exc_info=True)
             raise # Re-raise specific processing errors
        except Exception as e:
            error_msg = f"Unexpected error during Markdown translation: {str(e)}"
            self.logger.error(error_msg, exc_info=True) # Log with stack trace
            raise MarkdownProcessingError(error_msg) from e

# Example Usage (for illustration)
if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)
    processor = MarkdownProcessor()
    # ... add example markdown and translations ...
    # try:
    #     translated_md = processor.translateMarkdown(example_md, example_translations)
    #     print("Translated Markdown:\n", translated_md)
    # except MarkdownProcessingError as e:
    #     print(f"Error: {e}") 