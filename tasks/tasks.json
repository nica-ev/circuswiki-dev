{
  "tasks": [
    {
      "id": 1,
      "title": "Set up project structure and dependencies",
      "description": "Create the initial project structure for the translation tool within the 'translation-py' directory, define required dependencies, and set up configuration file templates.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "Create a Python project structure within a 'translation-py' folder. Add requirements.txt with dependencies: python-dotenv, ruamel.yaml, markdown-it-py, requests/httpx, hashlib, pathlib. Create template files for settings.txt and translate.env. Set up basic CLI entry point with argparse.",
      "testStrategy": "Verify project structure is correct within 'translation-py'. Ensure all dependencies can be installed with pip. Confirm template files are created with appropriate placeholders.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create basic project directory structure within translation-py",
          "description": "Set up the initial directory structure for the Python project following standard practices within the 'translation-py' folder.",
          "status": "done",
          "dependencies": [],
          "details": "Create the following directory structure inside the 'translation-py' folder:\n- translation-py/\n  - src/\n    - __init__.py\n  - tests/\n    - __init__.py\n  - docs/\n  - config/\n  - README.md\n  - requirements.txt\n  - .gitignore\n\nEnsure the .gitignore includes standard Python patterns (pycache, virtual environments, etc.). Create empty __init__.py files in appropriate directories to make them proper Python packages. Ensure this structure is created relative to the main project root.\n\n<info added on 2025-05-03T23:28:06.807Z>\nFor the .gitignore file, I've included standard Python patterns such as:\n```\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nenv/\nvenv/\nENV/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n.pytest_cache/\n.coverage\nhtmlcov/\n```\n\nI've created a basic README.md with sections for:\n- Project overview\n- Installation instructions\n- Usage examples\n- Development setup\n\nThe requirements.txt file includes initial dependencies:\n```\npytest>=7.0.0\nblack>=23.0.0\nisort>=5.12.0\n```\n\nI've also added a src/translation/__init__.py file with version information:\n```python\n__version__ = \"0.1.0\"\n```\n\nAnd created a basic conftest.py in the tests directory to prepare for future testing.\n</info added on 2025-05-03T23:28:06.807Z>"
        },
        {
          "id": 2,
          "title": "Define project dependencies and create requirements.txt",
          "description": "Create a requirements.txt file with all necessary dependencies for the project within the 'translation-py' directory.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Create a requirements.txt file inside the 'translation-py' directory with the following dependencies:\n\npython-dotenv>=1.0.0\nruamel.yaml>=0.17.21\nmarkdown-it-py>=2.2.0\nhttpx>=0.24.0\nrequests>=2.28.2\nhashlib\npathlib\n\nSpecify minimum versions for each dependency to ensure compatibility. Include comments explaining what each dependency is used for. Consider separating dev dependencies (like testing libraries) into a requirements-dev.txt file within 'translation-py'.\n\n<info added on 2025-05-03T23:28:33.060Z>\nHere's the content for the requirements.txt file:\n\n```\n# Environment variable management\npython-dotenv>=1.0.0\n\n# YAML parsing and manipulation\nruamel.yaml>=0.17.21\n\n# Markdown parsing\nmarkdown-it-py>=2.2.0\n\n# HTTP client for API requests (modern alternative to requests)\nhttpx>=0.24.0\n\n# HTTP client for API requests (widely used)\nrequests>=2.28.2\n\n# Note: hashlib and pathlib are built-in Python libraries and don't need to be included in requirements.txt\n```\n\nYou may also want to create a requirements-dev.txt file with:\n\n```\n# Testing\npytest>=7.3.1\npytest-cov>=4.1.0\n\n# Linting\nflake8>=6.0.0\nblack>=23.3.0\n\n# Type checking\nmypy>=1.3.0\n```\n\nRemember to run `pip install -r requirements.txt` to install the dependencies, and `pip install -r requirements-dev.txt` for development dependencies.\n</info added on 2025-05-03T23:28:33.060Z>"
        },
        {
          "id": 3,
          "title": "Create configuration file templates",
          "description": "Set up template files for settings.txt and translate.env within the 'translation-py/config' directory.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "In the 'translation-py/config/' directory, create:\n\n1. settings.txt template with placeholders for common configuration options:\n```ini\n# Application Settings\nINPUT_DIR=../docs # Default relative to translation-py\nOUTPUT_DIR=../translated_docs # Default relative to translation-py\nTARGET_LANGUAGES=DE,FR,ES\nYAML_TRANSLATE_FIELDS=title,description\nAPI_PROVIDER=DeepL\nTEST_MODE=false\n# Add other configuration parameters with default values and comments\n```\n\n2. translate.env template for environment variables:\n```\n# API Keys\nDEEPL_API_KEY=your_api_key_here\n# Other keys...\n```\n\nInclude comments explaining each configuration option and how to set them. Ensure the default paths consider the location within 'translation-py'.\n\n<info added on 2025-05-03T23:29:02.799Z>\nI've created both template files with the following enhancements:\n\nFor settings.txt, I added comprehensive comments explaining each option:\n- INPUT_DIR/OUTPUT_DIR now include absolute path examples and usage notes\n- Added FILE_EXTENSIONS=.md,.txt,.html to control which files are processed\n- Added BATCH_SIZE=50 to control API request batching\n- Added LOG_LEVEL=INFO with options (DEBUG, INFO, WARNING, ERROR)\n- Added PRESERVE_FORMATTING=true to maintain markdown/HTML formatting\n- Added SKIP_EXISTING=false to control incremental translation behavior\n\nFor translate.env, I expanded it with:\n- Added all supported API providers (GOOGLE_CLOUD_KEY, MICROSOFT_TRANSLATOR_KEY)\n- Added PROXY_URL configuration for corporate environments\n- Added API_TIMEOUT=30 setting\n- Added comments explaining credential storage best practices\n- Added warning about not committing this file to version control\n\nBoth files include proper header documentation explaining their purpose and relationship to each other.\n</info added on 2025-05-03T23:29:02.799Z>"
        },
        {
          "id": 4,
          "title": "Set up CLI entry point structure",
          "description": "Create the main CLI entry point file within 'translation-py/src'.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Create a file named 'translation-py/src/cli.py' with the following structure:\n\n```python\n#!/usr/bin/env python3\nimport argparse\nimport sys\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Markdown Content Translator Tool')\n    \n    # Add common arguments\n    parser.add_argument('-s', '--settings', default='../config/settings.txt', help='Path to settings file (relative to script location)')\n    parser.add_argument('-e', '--env', default='../config/translate.env', help='Path to environment file (relative to script location)')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Enable verbose output')\n    \n    # Add subparsers if needed for different commands later\n    # subparsers = parser.add_subparsers(dest='command', help='Commands')\n    \n    return parser.parse_args()\n\ndef main():\n    args = parse_args()\n    # Main application logic will go here\n    print(f\"Arguments: {args}\")\n    # TODO: Load config using args.settings and args.env\n    \nif __name__ == \"__main__\":\n    sys.exit(main())\n```\n\nMake the file executable with appropriate permissions. Adjust default config/env paths to be relative to the script's expected location within 'translation-py/src'.\n\n<info added on 2025-05-03T23:31:50.504Z>\nTo enhance the CLI entry point structure, I'll add implementation details about handling configuration loading and error management:\n\n```python\n# Add these imports at the top\nimport os\nimport logging\nfrom pathlib import Path\n\n# Add a function to resolve paths relative to the script location\ndef resolve_path(relative_path):\n    \"\"\"Convert a path relative to the script location to an absolute path\"\"\"\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    return os.path.abspath(os.path.join(script_dir, relative_path))\n\n# Enhance the main function with proper logging setup and error handling\ndef main():\n    args = parse_args()\n    \n    # Configure logging based on verbosity\n    log_level = logging.DEBUG if args.verbose else logging.INFO\n    logging.basicConfig(\n        level=log_level,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    logger = logging.getLogger('translation-cli')\n    \n    try:\n        # Resolve config paths\n        settings_path = resolve_path(args.settings)\n        env_path = resolve_path(args.env)\n        \n        logger.debug(f\"Using settings file: {settings_path}\")\n        logger.debug(f\"Using environment file: {env_path}\")\n        \n        # Check if files exist\n        if not os.path.exists(settings_path):\n            logger.error(f\"Settings file not found: {settings_path}\")\n            return 1\n            \n        if not os.path.exists(env_path):\n            logger.error(f\"Environment file not found: {env_path}\")\n            return 1\n            \n        # TODO: Load config using settings_path and env_path\n        return 0\n    except Exception as e:\n        logger.exception(f\"Error in main execution: {str(e)}\")\n        return 1\n```\n\nAfter creating the file, make it executable with:\n```bash\nchmod +x translation-py/src/cli.py\n```\n</info added on 2025-05-03T23:31:50.504Z>"
        },
        {
          "id": 5,
          "title": "Create package entry point and setup.py",
          "description": "Set up the main package entry point and create setup.py for installation within the 'translation-py' directory.",
          "status": "done",
          "dependencies": [
            1,
            2,
            4
          ],
          "details": "1. Create a '__main__.py' file in the 'translation-py/src' directory:\n```python\nfrom .cli import main\n\nif __name__ == \"__main__\":\n    main()\n```\n\n2. Create 'setup.py' in the 'translation-py' directory:\n```python\nfrom setuptools import setup, find_packages\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\", encoding=\"utf-8\") as fh:\n    requirements = fh.read().splitlines()\n\nsetup(\n    name=\"markdown_translator\",\n    version=\"0.1.0\",\n    author=\"Your Name / Project Team\",\n    author_email=\"your.email@example.com\",\n    description=\"Tool to translate Markdown files using LLM APIs\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    # url=\"URL to project repo subsection if applicable\",\n    package_dir={'': 'src'},\n    packages=find_packages(where='src'),\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\", # Choose appropriate license\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.8\",\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"mdtranslate=src.cli:main\",\n        ],\n    },\n)\n```\n\nUpdate the package name, author details, and entry point command name. Ensure `package_dir` and `find_packages` point correctly to the `src` directory within `translation-py`."
        }
      ]
    },
    {
      "id": 2,
      "title": "Implement ConfigLoader component",
      "description": "Create a component to load and validate configuration from settings.txt and translate.env files.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "Implement a ConfigLoader class that reads settings.txt for INPUT_DIR, OUTPUT_DIR, TARGET_LANGUAGES, YAML_TRANSLATE_FIELDS, API_PROVIDER, TEST_MODE. Use python-dotenv to read translate.env for API keys (e.g., DEEPL_API_KEY). Include validation for required fields and appropriate error handling for missing or invalid configuration.",
      "testStrategy": "Create test configuration files with valid and invalid settings. Verify ConfigLoader correctly loads settings, validates required fields, and raises appropriate exceptions for invalid configurations.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create ConfigLoader class skeleton with file path handling",
          "description": "Set up the basic structure of the ConfigLoader class with initialization and file path handling for both configuration files.",
          "status": "done",
          "dependencies": [],
          "details": "Create a new file `config_loader.py` with a ConfigLoader class. Implement the `__init__` method that accepts optional paths to settings.txt and translate.env files (with defaults). Add methods to validate file existence and readability. Include basic exception handling for file access issues and implement logging setup.\n\n<info added on 2025-05-04T00:52:33.012Z>\n```python\n# Implementation details for config_loader.py\n\nimport os\nimport logging\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any\n\nclass ConfigError(Exception):\n    \"\"\"Base exception for configuration errors.\"\"\"\n    pass\n\nclass ConfigFileNotFoundError(ConfigError):\n    \"\"\"Raised when a configuration file cannot be found.\"\"\"\n    pass\n\nclass ConfigFileNotReadableError(ConfigError):\n    \"\"\"Raised when a configuration file exists but cannot be read.\"\"\"\n    pass\n\nclass ConfigLoader:\n    def __init__(self, settings_path: Optional[str] = None, env_path: Optional[str] = None):\n        # Get the directory where this script is located\n        base_dir = Path(__file__).parent.parent\n        \n        # Set default paths relative to the project root\n        self.settings_file = Path(settings_path or base_dir / \"config\" / \"settings.txt\").resolve()\n        self.env_file = Path(env_path or base_dir / \"config\" / \"translate.env\").resolve()\n        \n        # Initialize empty configuration dictionaries\n        self.settings: Dict[str, Any] = {}\n        self.env_vars: Dict[str, str] = {}\n        \n        # Set up logging\n        self.logger = logging.getLogger(__name__)\n        \n        # Validate configuration files\n        try:\n            self._validate_file(self.settings_file)\n            self._validate_file(self.env_file)\n            self.logger.info(f\"Configuration files validated successfully\")\n        except ConfigError as e:\n            self.logger.error(f\"Configuration error: {str(e)}\")\n            raise\n    \n    def _validate_file(self, file_path: Path) -> None:\n        \"\"\"\n        Validate that a configuration file exists and is readable.\n        \n        Args:\n            file_path: Path to the configuration file\n            \n        Raises:\n            ConfigFileNotFoundError: If the file doesn't exist\n            ConfigFileNotReadableError: If the file exists but can't be read\n        \"\"\"\n        self.logger.debug(f\"Validating configuration file: {file_path}\")\n        \n        if not file_path.exists():\n            self.logger.error(f\"Configuration file not found: {file_path}\")\n            raise ConfigFileNotFoundError(f\"Configuration file not found: {file_path}\")\n        \n        if not file_path.is_file():\n            self.logger.error(f\"Path exists but is not a file: {file_path}\")\n            raise ConfigFileNotFoundError(f\"Path exists but is not a file: {file_path}\")\n        \n        if not os.access(file_path, os.R_OK):\n            self.logger.error(f\"Configuration file not readable: {file_path}\")\n            raise ConfigFileNotReadableError(f\"Configuration file not readable: {file_path}\")\n```\n</info added on 2025-05-04T00:52:33.012Z>"
        },
        {
          "id": 2,
          "title": "Implement settings.txt parser",
          "description": "Create functionality to read and parse the settings.txt file for configuration parameters.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Add a method `_parse_settings_file()` that reads the settings.txt file and extracts the required configuration parameters (INPUT_DIR, OUTPUT_DIR, TARGET_LANGUAGES, YAML_TRANSLATE_FIELDS, API_PROVIDER, TEST_MODE). Parse each line using appropriate string manipulation (e.g., splitting by '=' and stripping whitespace). Store the parsed values in a dictionary attribute. Handle comments and empty lines appropriately.\n\n<info added on 2025-05-04T00:59:53.262Z>\nThe implementation in `translation-py/src/config_loader.py` should handle several edge cases:\n\n1. Type conversion for specific settings:\n   - Convert `TARGET_LANGUAGES` from comma-separated string to a list\n   - Convert `YAML_TRANSLATE_FIELDS` from comma-separated string to a list\n   - Convert `TEST_MODE` string to boolean value\n\n2. Default values for optional parameters:\n   - Set `TEST_MODE=False` if not specified\n   - Use current directory for `INPUT_DIR` or `OUTPUT_DIR` if missing\n\n3. Validation logic:\n   - Check that required parameters exist after parsing\n   - Verify that directories in `INPUT_DIR` and `OUTPUT_DIR` are valid paths\n\nExample implementation snippet:\n```python\ndef _parse_settings_file(self):\n    self.settings = {}\n    try:\n        with open(self.settings_path, 'r', encoding='utf-8') as file:\n            for line in file:\n                line = line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                    \n                try:\n                    key, value = [part.strip() for part in line.split('=', 1)]\n                    \n                    # Type conversions\n                    if key in ['TARGET_LANGUAGES', 'YAML_TRANSLATE_FIELDS']:\n                        value = [item.strip() for item in value.split(',') if item.strip()]\n                    elif key == 'TEST_MODE':\n                        value = value.lower() in ['true', 'yes', '1']\n                        \n                    self.settings[key] = value\n                except ValueError:\n                    self.logger.warning(f\"Skipping invalid line in settings file: {line}\")\n        \n        # Apply defaults\n        self.settings.setdefault('TEST_MODE', False)\n        self.settings.setdefault('INPUT_DIR', os.getcwd())\n        self.settings.setdefault('OUTPUT_DIR', os.getcwd())\n        \n        # Validate required settings\n        self._validate_settings()\n    except OSError as e:\n        raise ConfigError(f\"Failed to read settings file: {e}\")\n```\n</info added on 2025-05-04T00:59:53.262Z>"
        },
        {
          "id": 3,
          "title": "Implement environment variable loading with python-dotenv",
          "description": "Add functionality to load API keys from translate.env using python-dotenv.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Use python-dotenv to load environment variables from the translate.env file. Add a method `_load_env_variables()` that reads API keys (e.g., DEEPL_API_KEY) from the environment file. Store these values in a separate dictionary attribute. Include error handling for missing or inaccessible environment files.\n\n<info added on 2025-05-04T01:03:38.876Z>\n```python\n# Implementation details for _load_env_variables method:\ndef _load_env_variables(self):\n    \"\"\"Load environment variables from translate.env file.\"\"\"\n    import os\n    from dotenv import load_dotenv\n    \n    try:\n        # Check if env file exists\n        if os.path.exists(self.env_file):\n            # Load environment variables from file\n            load_dotenv(dotenv_path=self.env_file, override=False)\n            self.logger.info(f\"Environment variables loaded from {self.env_file}\")\n            \n            # Store API keys in dictionary for easy access\n            self.env_vars = {\n                'DEEPL_API_KEY': os.getenv('DEEPL_API_KEY'),\n                'GOOGLE_API_KEY': os.getenv('GOOGLE_API_KEY'),\n                'AZURE_API_KEY': os.getenv('AZURE_API_KEY'),\n                'AZURE_REGION': os.getenv('AZURE_REGION')\n            }\n            \n            # Filter out None values\n            self.env_vars = {k: v for k, v in self.env_vars.items() if v is not None}\n            \n            if not self.env_vars:\n                self.logger.warning(\"No API keys found in environment file\")\n        else:\n            self.logger.warning(f\"Environment file {self.env_file} not found\")\n            self.env_vars = {}\n    except Exception as e:\n        self.logger.error(f\"Error loading environment variables: {str(e)}\")\n        self.env_vars = {}\n```\n\nAdd this method to the ConfigLoader class and update the `__init__` method to call `self._load_env_variables()` after initializing the env_file path. This implementation provides proper error handling and stores API keys in a dedicated dictionary for convenient access throughout the application.\n</info added on 2025-05-04T01:03:38.876Z>"
        },
        {
          "id": 4,
          "title": "Implement configuration validation",
          "description": "Add validation logic to ensure all required configuration parameters are present and valid.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "Create a `validate_config()` method that checks if all required fields are present and have valid values. Define validation rules for each parameter (e.g., directories should exist or be creatable, TARGET_LANGUAGES should be a comma-separated list, API_PROVIDER should be one of the supported options). Implement type checking and format validation for each field. Raise specific exceptions with clear error messages for validation failures.\n\n<info added on 2025-05-04T01:05:31.628Z>\n```python\ndef _validate_config(self):\n    \"\"\"Validates configuration parameters and raises ConfigValidationError if invalid.\"\"\"\n    # Check required settings\n    required_settings = ['INPUT_DIR', 'OUTPUT_DIR', 'TARGET_LANGUAGES', 'API_PROVIDER']\n    missing = [setting for setting in required_settings if not self.settings.get(setting)]\n    if missing:\n        raise ConfigValidationError(f\"Missing required settings: {', '.join(missing)}\")\n    \n    # Validate directories\n    for dir_setting in ['INPUT_DIR', 'OUTPUT_DIR']:\n        path = Path(self.settings[dir_setting])\n        if not path.exists():\n            try:\n                path.mkdir(parents=True)\n                logger.info(f\"Created directory: {path}\")\n            except Exception as e:\n                raise ConfigValidationError(f\"Cannot create {dir_setting} directory: {str(e)}\")\n    \n    # Validate target languages\n    if not self.settings['TARGET_LANGUAGES'].strip():\n        raise ConfigValidationError(\"TARGET_LANGUAGES cannot be empty\")\n    self.settings['TARGET_LANGUAGES_LIST'] = [\n        lang.strip() for lang in self.settings['TARGET_LANGUAGES'].split(',')\n    ]\n    \n    # Validate API provider\n    allowed_providers = ['DeepL', 'Google', 'Azure']\n    if self.settings['API_PROVIDER'] not in allowed_providers:\n        raise ConfigValidationError(\n            f\"API_PROVIDER must be one of: {', '.join(allowed_providers)}\"\n        )\n    \n    # Process TEST_MODE\n    test_mode = self.settings.get('TEST_MODE', 'false').lower()\n    self.settings['TEST_MODE_BOOL'] = test_mode in ['true', '1', 'yes']\n    \n    # Validate YAML_TRANSLATE_FIELDS if present\n    if 'YAML_TRANSLATE_FIELDS' in self.settings and not self.settings['YAML_TRANSLATE_FIELDS'].strip():\n        raise ConfigValidationError(\"YAML_TRANSLATE_FIELDS cannot be empty if specified\")\n    \n    # Check for API keys when not in test mode\n    if not self.settings['TEST_MODE_BOOL']:\n        api_key_map = {\n            'DeepL': 'DEEPL_API_KEY',\n            'Google': 'GOOGLE_CLOUD_KEY',\n            'Azure': 'MICROSOFT_TRANSLATOR_KEY'\n        }\n        required_key = api_key_map[self.settings['API_PROVIDER']]\n        if not self.env_vars.get(required_key):\n            raise ConfigValidationError(f\"Missing required API key: {required_key}\")\n\nclass ConfigValidationError(Exception):\n    \"\"\"Exception raised for configuration validation errors.\"\"\"\n    pass\n```\n</info added on 2025-05-04T01:05:31.628Z>"
        },
        {
          "id": 5,
          "title": "Create public interface and documentation",
          "description": "Implement public methods to access configuration and add comprehensive documentation.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "Create public getter methods to access validated configuration parameters (e.g., `get_input_dir()`, `get_target_languages()`, `get_api_key()`). Implement a `load_config()` method that orchestrates the entire loading and validation process. Add comprehensive docstrings following a standard format (e.g., Google style) for the class and all methods. Include usage examples in the module docstring. Add type hints to all methods and parameters."
        }
      ]
    },
    {
      "id": 3,
      "title": "Implement basic FileManager for directory scanning",
      "description": "Create a component to recursively scan directories and identify Markdown files.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "Implement a FileManager class with methods to scan the INPUT_DIR recursively and identify all .md files. Use pathlib for cross-platform path handling. Include error handling for directory access issues. Return a list of file paths for further processing.",
      "testStrategy": "Create test directories with various file types. Verify the scanner correctly identifies all .md files and handles nested directories properly. Test error cases like non-existent or inaccessible directories.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create FileManager class structure with configuration",
          "description": "Set up the basic FileManager class with initialization parameters and configuration for directory scanning",
          "status": "done",
          "dependencies": [],
          "details": "Create a FileManager class with an __init__ method that accepts INPUT_DIR parameter. Initialize class variables for storing configuration like file extensions to scan for (.md). Use pathlib.Path for handling the input directory path to ensure cross-platform compatibility. Include basic validation to check if the provided path exists and is a directory.\n\n<info added on 2025-05-04T01:12:59.138Z>\nThe FileManager class has been implemented in the specified location with the following enhancements:\n\n- Added proper error handling with specific exceptions (`FileNotFoundError` when path doesn't exist, `NotADirectoryError` when path is not a directory)\n- Implemented logging using Python's built-in logging module for tracking operations\n- Created a `target_extension` attribute set to '.md' for filtering markdown files\n- Used pathlib.Path's resolve() method to convert relative paths to absolute paths\n- Added docstrings following PEP 257 conventions for better code documentation\n- Implemented type hints for better IDE support and code readability\n- Added a __repr__ method for better debugging representation of the class instance\n\nThe class is now ready for extension with file scanning and processing methods.\n</info added on 2025-05-04T01:12:59.138Z>"
        },
        {
          "id": 2,
          "title": "Implement recursive directory scanning logic",
          "description": "Create a method to recursively traverse directories and collect file paths",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Implement a _scan_directory method that takes a directory path and recursively traverses it. Use pathlib's iterdir() and is_dir() methods for directory traversal. Create a helper method to check if a file matches the target extension (.md). Build the scanning logic to collect all matching file paths into a list. Ensure the method handles the recursive nature of directory traversal properly.\n\n<info added on 2025-05-04T01:13:17.523Z>\nThe method implementation uses a depth-first search approach for directory traversal. For case-insensitive extension matching, it converts both the file suffix and target extension to lowercase before comparison (e.g., `if file_path.suffix.lower() == self.target_extension.lower()`). Debug logging statements are added at key points to track traversal progress and file discovery using the Python logging module. The method handles potential permission errors with try/except blocks to prevent crashes when accessing restricted directories. For performance optimization, it uses a generator-based approach internally before collecting results into the final list. The implementation also respects symbolic links but doesn't follow them to avoid potential infinite loops in the filesystem traversal.\n</info added on 2025-05-04T01:13:17.523Z>"
        },
        {
          "id": 3,
          "title": "Add error handling for directory access issues",
          "description": "Enhance the scanning logic with robust error handling for permission and access issues",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Modify the directory scanning method to include try-except blocks for handling common errors: PermissionError for access denied scenarios, FileNotFoundError for missing directories, and a general exception handler for unexpected issues. Log appropriate error messages for each case. Implement a strategy to continue scanning when encountering inaccessible subdirectories rather than failing the entire process. Consider adding a method to report scanning errors encountered.\n\n<info added on 2025-05-04T01:13:47.581Z>\nFor the error handling implementation, consider these specific details:\n\nCreate a structured `ScanError` class to track error information:\n```python\nclass ScanError:\n    def __init__(self, path, error_type, message):\n        self.path = path\n        self.error_type = error_type\n        self.message = message\n        self.timestamp = datetime.now()\n```\n\nIn the `_scan_directory_recursive` method, implement granular error handling:\n```python\ndef _scan_directory_recursive(self, directory, relative_path=\"\"):\n    found_files = []\n    errors = []\n    \n    try:\n        for item in directory.iterdir():\n            try:\n                item_rel_path = os.path.join(relative_path, item.name)\n                \n                if item.is_dir() and not item.name.startswith('.'):\n                    subdir_files, subdir_errors = self._scan_directory_recursive(item, item_rel_path)\n                    found_files.extend(subdir_files)\n                    errors.extend(subdir_errors)\n                elif item.is_file() and self._matches_pattern(item.name):\n                    found_files.append((item, item_rel_path))\n            except PermissionError as e:\n                error = ScanError(str(item), \"PermissionError\", f\"Access denied: {str(e)}\")\n                errors.append(error)\n                self.logger.warning(f\"Permission error accessing {item}: {e}\")\n            except Exception as e:\n                error = ScanError(str(item), type(e).__name__, str(e))\n                errors.append(error)\n                self.logger.error(f\"Error processing {item}: {e}\")\n    except PermissionError as e:\n        error = ScanError(str(directory), \"PermissionError\", f\"Cannot list directory: {str(e)}\")\n        errors.append(error)\n        self.logger.warning(f\"Permission error listing directory {directory}: {e}\")\n    except FileNotFoundError as e:\n        error = ScanError(str(directory), \"FileNotFoundError\", str(e))\n        errors.append(error)\n        self.logger.warning(f\"Directory not found: {directory}\")\n    \n    return found_files, errors\n```\n\nAdd a public method to retrieve scan errors:\n```python\ndef get_scan_errors(self):\n    \"\"\"Returns a list of errors encountered during the last scan operation.\"\"\"\n    return self.errors\n```\n\nImplement a method to summarize errors by type:\n```python\ndef get_error_summary(self):\n    \"\"\"Returns a summary of errors grouped by error type.\"\"\"\n    summary = {}\n    for error in self.errors:\n        if error.error_type not in summary:\n            summary[error.error_type] = 0\n        summary[error.error_type] += 1\n    return summary\n```\n</info added on 2025-05-04T01:13:47.581Z>"
        },
        {
          "id": 4,
          "title": "Create public scan method with result formatting",
          "description": "Implement the main public method that initiates scanning and returns properly formatted results",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "Create a public scan_markdown_files method that calls the internal scanning logic and returns a list of pathlib.Path objects representing all found Markdown files. Include options for returning absolute or relative paths. Add documentation explaining the return format and possible exceptions. Implement basic statistics collection (number of files found, directories scanned, errors encountered) that can be optionally returned or logged. Test the method with various directory structures to ensure it works correctly.\n\n<info added on 2025-05-04T01:14:12.388Z>\nFor the `scan_markdown_files` method, consider implementing these specific details:\n\n```python\ndef scan_markdown_files(self, relative_paths=False, collect_stats=True):\n    \"\"\"\n    Scan for Markdown files in the configured directories.\n    \n    Args:\n        relative_paths (bool): If True, returns paths relative to base directory\n        collect_stats (bool): If True, collects and logs scanning statistics\n        \n    Returns:\n        List[pathlib.Path]: List of found Markdown files\n        \n    Raises:\n        DirectoryAccessError: If base directory cannot be accessed\n        ScanConfigurationError: If no valid directories are configured\n    \"\"\"\n    self.errors = []\n    start_time = time.time()\n    stats = {\"files_found\": 0, \"dirs_scanned\": 0, \"errors\": 0} if collect_stats else None\n    \n    found_files = []\n    for directory in self.directories:\n        try:\n            files, dir_stats = self._scan_directory_recursive(directory, stats=stats)\n            found_files.extend(files)\n            if stats:\n                stats[\"dirs_scanned\"] += dir_stats[\"dirs_scanned\"]\n                stats[\"files_found\"] += dir_stats[\"files_found\"]\n                stats[\"errors\"] += len(dir_stats[\"errors\"])\n            self.errors.extend(dir_stats.get(\"errors\", []))\n        except Exception as e:\n            self.errors.append(ScanError(str(directory), str(e)))\n            if stats:\n                stats[\"errors\"] += 1\n    \n    # Process paths based on relative_paths flag\n    if relative_paths:\n        found_files = [p.relative_to(self.base_directory) for p in found_files]\n    \n    if collect_stats:\n        elapsed = time.time() - start_time\n        self.logger.info(f\"Scan completed: {stats['files_found']} files found in {stats['dirs_scanned']} directories ({elapsed:.2f}s)\")\n        if stats[\"errors\"] > 0:\n            self.logger.warning(f\"{stats['errors']} errors encountered during scan\")\n    \n    return found_files\n```\n\nAlso add a helper method to retrieve scan errors:\n\n```python\ndef get_scan_errors(self):\n    \"\"\"\n    Returns the list of errors encountered during the last scan.\n    \n    Returns:\n        List[ScanError]: List of error objects with path and error message\n    \"\"\"\n    return self.errors\n```\n\nConsider creating a `ScanResult` dataclass to provide a more structured return value that includes both files and statistics.\n</info added on 2025-05-04T01:14:12.388Z>"
        }
      ]
    },
    {
      "id": 4,
      "title": "Implement YAML frontmatter parsing",
      "description": "Create functionality to parse and extract YAML frontmatter from Markdown files.",
      "status": "done",
      "dependencies": [
        3
      ],
      "priority": "high",
      "details": "Extend FileManager to read Markdown files and extract YAML frontmatter. Use ruamel.yaml for parsing. Handle edge cases like files without frontmatter or with malformed YAML. Implement a function to check if a file is eligible for translation by verifying the 'orig: true' flag in frontmatter.",
      "testStrategy": "Create test Markdown files with various frontmatter configurations. Verify correct parsing of valid frontmatter and appropriate handling of edge cases. Confirm 'orig: true' check works correctly.",
      "subtasks": [
        {
          "id": 1,
          "title": "Add ruamel.yaml dependency and create basic frontmatter extraction function",
          "description": "Set up the project with the ruamel.yaml dependency and implement a basic function to extract YAML frontmatter from Markdown content",
          "status": "done",
          "dependencies": [],
          "details": "Install ruamel.yaml package and add it to requirements.txt. Create a new function in FileManager called `extract_frontmatter(content)` that takes Markdown content as a string and returns the extracted YAML frontmatter as a Python dictionary. The function should identify content between '---' delimiters at the start of the file and parse it using ruamel.yaml.\n\n<info added on 2025-05-04T01:18:08.114Z>\nHere's the additional information to add:\n\nThe implementation uses a static method `_extract_frontmatter_and_content(content)` rather than the originally specified `extract_frontmatter(content)` to provide both the parsed frontmatter and the remaining content as a tuple. This is more efficient as it avoids parsing the content twice.\n\nThe regex pattern `r'^---\\s*\\n(.*?)---\\s*\\n'` with `re.DOTALL | re.MULTILINE` flags ensures proper matching of the frontmatter block even with multiline content. The method handles cases where no frontmatter exists by returning `(None, original_content)`.\n\nError handling is implemented to catch `YAMLError` exceptions during parsing, which returns `(None, original_content)` in case of malformed YAML.\n\nExample usage:\n```python\nfrontmatter, content = FileManager._extract_frontmatter_and_content(markdown_text)\nif frontmatter:\n    # Process frontmatter metadata\n    title = frontmatter.get('title', 'Untitled')\n    # Work with clean content\n    process_markdown(content)\n```\n\nThe `typ='safe'` parameter for YAML loading prevents execution of arbitrary code that might be embedded in the YAML.\n</info added on 2025-05-04T01:18:08.114Z>"
        },
        {
          "id": 2,
          "title": "Extend FileManager to read Markdown files with frontmatter support",
          "description": "Modify the FileManager class to read Markdown files and automatically extract frontmatter when reading these files",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Add a new method to FileManager called `read_markdown_with_frontmatter(file_path)` that reads a Markdown file and returns a tuple containing (frontmatter_dict, content_without_frontmatter). This method should use the extract_frontmatter function from subtask 1. Ensure the returned content has the frontmatter section removed.\n\n<info added on 2025-05-04T01:18:30.396Z>\nAdd a private static method `_extract_frontmatter_and_content(text)` that handles the parsing logic, separating concerns from the public method. This method should:\n\n1. Check if the file starts with \"---\" to identify frontmatter\n2. Use regex pattern `r'^---\\s*\\n(.*?)\\n---\\s*\\n'` with re.DOTALL flag to extract frontmatter content\n3. Parse the extracted YAML using `yaml.safe_load()`\n4. Return empty dict if no frontmatter is found\n\nFor error handling, include specific error messages:\n- \"File not found: {file_path}\" for FileNotFoundError\n- \"Permission denied when accessing: {file_path}\" for PermissionError\n- \"Invalid YAML in frontmatter: {str(e)}\" for YAMLError\n\nAdd unit tests covering:\n- Files with valid frontmatter\n- Files without frontmatter\n- Files with malformed YAML\n- Non-existent files\n- Permission-restricted files\n\nConsider adding an optional `encoding` parameter defaulting to 'utf-8' to support different file encodings.\n</info added on 2025-05-04T01:18:30.396Z>"
        },
        {
          "id": 3,
          "title": "Implement error handling for edge cases",
          "description": "Add robust error handling for various edge cases in frontmatter parsing",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "Enhance the frontmatter extraction function to handle edge cases: 1) Files without frontmatter should return an empty dictionary for frontmatter, 2) Files with malformed YAML should raise a specific custom exception (create a FrontmatterParsingError class), 3) Files with frontmatter delimiters but empty content should return an empty dictionary. Add appropriate logging for these scenarios.\n\n<info added on 2025-05-04T01:19:04.228Z>\n```python\nclass FrontmatterParsingError(Exception):\n    \"\"\"Custom exception for frontmatter parsing failures.\"\"\"\n    def __init__(self, message, original_exception=None):\n        super().__init__(message)\n        self.original_exception = original_exception\n\ndef _extract_frontmatter_and_content(content):\n    \"\"\"Extract frontmatter and content from markdown text.\n    \n    Returns:\n        tuple: (frontmatter_dict, content_without_frontmatter) or (None, original_content)\n    \n    Raises:\n        FrontmatterParsingError: When YAML parsing fails\n    \"\"\"\n    import yaml\n    import logging\n    \n    # Check for frontmatter delimiters\n    if not content.startswith('---'):\n        logging.debug(\"No frontmatter found, returning original content\")\n        return None, content\n        \n    # Find the closing delimiter\n    try:\n        end_delimiter = content.index('---', 3)\n    except ValueError:\n        logging.warning(\"Opening frontmatter delimiter found but no closing delimiter\")\n        return None, content\n        \n    # Extract and parse frontmatter\n    frontmatter_yaml = content[3:end_delimiter].strip()\n    \n    # Handle empty frontmatter case\n    if not frontmatter_yaml:\n        logging.info(\"Empty frontmatter found between delimiters\")\n        return {}, content[end_delimiter+3:].strip()\n    \n    # Parse YAML frontmatter\n    try:\n        frontmatter = yaml.safe_load(frontmatter_yaml)\n        \n        # Ensure frontmatter is a dictionary\n        if not isinstance(frontmatter, dict):\n            logging.warning(f\"Frontmatter parsed successfully but is not a dictionary: {type(frontmatter)}\")\n            return None, content\n            \n        return frontmatter, content[end_delimiter+3:].strip()\n    except yaml.YAMLError as e:\n        logging.error(f\"Failed to parse frontmatter YAML: {str(e)}\")\n        raise FrontmatterParsingError(f\"Invalid YAML in frontmatter: {str(e)}\", e)\n\ndef read_markdown_with_frontmatter(file_path):\n    \"\"\"Read markdown file with frontmatter and return both parts.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        frontmatter, markdown_content = _extract_frontmatter_and_content(content)\n        return frontmatter or {}, markdown_content\n    except FrontmatterParsingError as e:\n        logging.error(f\"Error parsing frontmatter in {file_path}: {str(e)}\")\n        # Re-raise or handle as needed by your application\n        raise\n```\n\nThis implementation includes proper logging at different severity levels, handles all specified edge cases, and provides detailed docstrings. The `FrontmatterParsingError` wraps the original exception for debugging while providing a clean API.\n</info added on 2025-05-04T01:19:04.228Z>"
        },
        {
          "id": 4,
          "title": "Implement translation eligibility checking",
          "description": "Create a function to determine if a file is eligible for translation based on frontmatter",
          "status": "done",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Add a method to FileManager called `is_eligible_for_translation(file_path)` that checks if a Markdown file should be translated. This function should extract the frontmatter and return True if the frontmatter contains 'orig: true', otherwise return False. Handle the case where the file doesn't exist or can't be read properly.\n\n<info added on 2025-05-04T01:19:25.828Z>\nThe method should be implemented with robust error handling as follows:\n\n```python\ndef is_eligible_for_translation(self, file_path):\n    \"\"\"\n    Determines if a Markdown file is eligible for translation based on frontmatter.\n    \n    Args:\n        file_path (str): Path to the Markdown file\n        \n    Returns:\n        bool: True if file has 'orig: true' in frontmatter, False otherwise\n    \"\"\"\n    try:\n        content, frontmatter = self.read_markdown_with_frontmatter(file_path)\n        \n        # Check if frontmatter exists and contains 'orig: true'\n        if isinstance(frontmatter, dict):\n            return frontmatter.get('orig') is True\n        return False\n        \n    except FileNotFoundError:\n        self.logger.warning(f\"File not found: {file_path}\")\n        return False\n    except PermissionError:\n        self.logger.error(f\"Permission denied when accessing: {file_path}\")\n        return False\n    except FrontmatterParsingError:\n        self.logger.warning(f\"Failed to parse frontmatter in: {file_path}\")\n        return False\n    except Exception as e:\n        self.logger.error(f\"Unexpected error checking translation eligibility for {file_path}: {str(e)}\")\n        return False\n```\n\nThe implementation should strictly check for `is True` rather than just truthy values to avoid considering strings like \"false\" as true.\n</info added on 2025-05-04T01:19:25.828Z>"
        },
        {
          "id": 5,
          "title": "Create utility functions for frontmatter manipulation",
          "description": "Implement helper functions to work with frontmatter data",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Create additional utility functions in FileManager: 1) `get_frontmatter_value(file_path, key, default=None)` to retrieve a specific value from frontmatter, 2) `update_frontmatter(file_path, updates_dict)` to modify frontmatter and save changes back to the file, 3) `strip_frontmatter(content)` to remove frontmatter from content without parsing it. These functions should build on the existing functionality and maintain consistent error handling.\n\n<info added on 2025-05-04T01:20:02.876Z>\nFor the frontmatter utility functions, implement these technical details:\n\n1. `get_frontmatter_value()`: Use a nested key lookup approach with dot notation support (e.g., \"metadata.author\") by splitting the key on dots and traversing the dictionary recursively. Handle missing intermediate keys gracefully.\n\n2. `update_frontmatter()`: Implement deep dictionary merging to preserve nested structures. Use `ruamel.yaml` instead of PyYAML to maintain comments and formatting when updating. Include a `create_if_missing` parameter (default=False) to optionally create frontmatter if none exists.\n\n3. `strip_frontmatter()`: Use regex pattern `r'^---\\s*\\n(.*?)\\n---\\s*\\n'` with re.DOTALL flag. Add an optional `keep_delimiters` parameter (default=False) to preserve the frontmatter delimiters.\n\n4. Add a new utility `has_frontmatter(content)` that returns a boolean indicating if content contains valid frontmatter.\n\n5. Include proper type hints for all functions and comprehensive docstrings with examples.\n\n6. Implement caching for frequently accessed frontmatter to improve performance when the same file is accessed multiple times.\n</info added on 2025-05-04T01:20:02.876Z>"
        }
      ]
    },
    {
      "id": 5,
      "title": "Implement content hash calculation",
      "description": "Create functionality to calculate content hashes for change detection using Test-Driven Development (TDD).",
      "status": "done",
      "dependencies": [
        4
      ],
      "priority": "high",
      "details": "Implement methods to calculate content_hash (for Markdown body) and yaml_hash (for frontmatter, excluding technical fields). Use SHA-256 via hashlib. For yaml_hash, exclude fields like content_hash, yaml_hash, and lang. Ensure consistent normalization of content before hashing to avoid false change detection. Follow Test-Driven Development principles by writing tests before implementing the actual functionality for each component.",
      "testStrategy": "Create test files with various content and verify hash calculation is consistent. Modify content and verify hash changes. Modify excluded YAML fields and verify yaml_hash doesn't change. Following TDD, write all tests first to define expected behavior before implementing the actual hashing logic.",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement content normalization functions",
          "description": "Create utility functions to normalize Markdown content and YAML frontmatter before hashing to ensure consistent hash generation.",
          "status": "done",
          "dependencies": [],
          "details": "Following TDD, first write tests for two normalization functions: `normalize_markdown_content(content)` and `normalize_yaml_frontmatter(frontmatter)`. Tests should verify that the Markdown normalization handles line endings (convert to \\n), whitespace trimming, and any other text normalization needed for consistency. The YAML normalization tests should verify that frontmatter is converted to a canonical form with consistent key ordering and value representation. Only after tests are complete, implement the actual normalization functions to pass the tests.\n\n<info added on 2025-05-04T01:32:23.880Z>\n# TDD Approach\n\n## Test Cases for `normalize_markdown_content`:\n- Different line endings (CR, LF, CRLF)\n- Leading/trailing whitespace\n- Multiple consecutive blank lines\n- Mixed indentation (spaces vs tabs)\n- Unicode characters and normalization forms\n- HTML content within markdown\n\n## Test Cases for `normalize_yaml_frontmatter`:\n- Different key ordering\n- Nested structures\n- Various data types (strings, numbers, booleans, lists)\n- Quoted vs unquoted strings\n- Multi-line strings\n- Empty values\n\n## Implementation Notes:\n- For markdown normalization, consider using regex patterns like `re.sub(r'\\r\\n|\\r', '\\n', content)` for line endings\n- For YAML normalization, parse to Python objects then serialize in canonical form:\n  ```python\n  def normalize_yaml_frontmatter(frontmatter):\n      # Parse YAML to Python dict\n      data = yaml.safe_load(frontmatter)\n      # Sort keys recursively\n      normalized_data = _sort_dict_recursively(data)\n      # Return serialized in canonical form\n      return yaml.dump(normalized_data, sort_keys=True, default_flow_style=False)\n  \n  def _sort_dict_recursively(d):\n      # Helper function to sort nested dictionaries\n      if not isinstance(d, dict):\n          return d\n      return {k: _sort_dict_recursively(v) for k, v in sorted(d.items())}\n  ```\n- Consider using libraries like `pyyaml` for YAML handling and `unicodedata` for Unicode normalization\n</info added on 2025-05-04T01:32:23.880Z>\n\n<info added on 2025-05-04T01:46:18.985Z>\n<info added on 2025-05-05T14:20:45.123Z>\n# Implementation Plan (Subtask 5.1)\n\n## File Locations:\n- Normalization functions: `src/utils/normalization.py` (New file/directory)\n- Tests: `tests/utils/test_normalization.py` (New file/directory)\n\n## Dependencies:\n- Ensure `PyYAML` is added to project dependencies (e.g., `requirements.txt` or `pyproject.toml`).\n\n## Test Structure (`pytest`):\n- `TestNormalizeMarkdownContent`:\n    - test_line_endings (CRLF, LF, CR -> LF)\n    - test_whitespace (leading/trailing trim)\n    - test_multiple_blank_lines\n    - test_mixed_indentation (should preserve unless normalization dictates otherwise - TBD)\n    - test_unicode_chars\n- `TestNormalizeYAMLFrontmatter`:\n    - test_key_ordering (canonical sort)\n    - test_nested_structures\n    - test_data_types (string, int, bool, list)\n    - test_string_quoting\n    - test_multiline_strings\n    - test_empty_values\n\n## TDD Process:\n1. Create the empty files (`src/utils/normalization.py`, `tests/utils/test_normalization.py`).\n2. Write failing tests in `test_normalization.py` covering the cases above.\n3. Implement the normalization functions in `normalization.py` to make the tests pass.\n4. Ensure `PyYAML` dependency is handled.\n</info added on 2025-05-05T14:20:45.123Z>\n</info added on 2025-05-04T01:46:18.985Z>\n\n<info added on 2025-05-04T01:49:04.098Z>\n# Implementation Plan (Subtask 5.1) - CORRECTED PATHS\n\n## File Locations:\n- Normalization functions: `translation-py/src/utils/normalization.py` (New file/directory within translation-py)\n- Tests: `translation-py/tests/utils/test_normalization.py` (New file/directory within translation-py)\n\n## Dependencies:\n- Ensure `PyYAML` is added to project dependencies (e.g., `translation-py/requirements.txt` or `pyproject.toml`).\n\n## Test Structure (`pytest`):\n- `TestNormalizeMarkdownContent`:\n    - test_line_endings (CRLF, LF, CR -> LF)\n    - test_whitespace (leading/trailing trim)\n    - test_multiple_blank_lines\n    - test_mixed_indentation (should preserve unless normalization dictates otherwise - TBD)\n    - test_unicode_chars\n- `TestNormalizeYAMLFrontmatter`:\n    - test_key_ordering (canonical sort)\n    - test_nested_structures\n    - test_data_types (string, int, bool, list)\n    - test_string_quoting\n    - test_multiline_strings\n    - test_empty_values\n\n## TDD Process:\n1. Create the empty files (`translation-py/src/utils/normalization.py`, `translation-py/tests/utils/test_normalization.py`).\n2. Write failing tests in `test_normalization.py` covering the cases above.\n3. Implement the normalization functions in `normalization.py` to make the tests pass.\n4. Ensure `PyYAML` dependency is handled within the `translation-py` subproject.\n</info added on 2025-05-04T01:49:04.098Z>\n\n<info added on 2025-05-04T02:00:07.802Z>\n# Implementation Complete (Subtask 5.1)\n\nImplemented and tested the following normalization functions in `translation-py/src/utils/normalization.py`:\n\n1.  **`normalize_markdown_content(content: str) -> str`**\n    - Normalizes line endings (CRLF/CR -> LF).\n    - Trims leading/trailing whitespace from each line.\n    - Reduces multiple consecutive blank lines to a single blank line.\n    - Normalizes Unicode characters to NFC form.\n    - All corresponding tests in `TestNormalizeMarkdownContent` are passing.\n\n2.  **`normalize_yaml_frontmatter(frontmatter: dict) -> dict`**\n    - Recursively sorts dictionary keys alphabetically.\n    - Preserves list order but sorts dictionaries within lists.\n    - Handles various data types and nested structures.\n    - All corresponding tests in `TestNormalizeYAMLFrontmatter` are passing.\n\nDependencies `pytest` and `PyYAML` added to `translation-py/requirements.txt`.\nNecessary `__init__.py` files were created in `src` and `tests` directories.\n</info added on 2025-05-04T02:00:07.802Z>"
        },
        {
          "id": 2,
          "title": "Implement content_hash calculation for Markdown body",
          "description": "Create a function to calculate SHA-256 hash for normalized Markdown content.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Using TDD, first write tests for the `calculate_content_hash(markdown_content)` function that: 1) Takes raw Markdown content as input, 2) Uses the normalization function from subtask 1, 3) Calculates SHA-256 hash using hashlib, 4) Returns the hexadecimal digest of the hash. Tests should verify hash consistency across equivalent content with different formatting and include error handling for invalid inputs. Only after tests are complete, implement the actual function to pass the tests.\n\n<info added on 2025-05-04T02:04:58.223Z>\n## Implementation Details\n\nThe `calculate_content_hash` function has been implemented with the following specifics:\n\n```python\ndef calculate_content_hash(markdown_content: str) -> str:\n    \"\"\"\n    Calculate SHA-256 hash for normalized Markdown content.\n    \n    Args:\n        markdown_content: Raw Markdown content as string\n        \n    Returns:\n        Hexadecimal digest of the SHA-256 hash\n        \n    Raises:\n        TypeError: If input is not a string\n    \"\"\"\n    if not isinstance(markdown_content, str):\n        raise TypeError(\"Input must be a string\")\n        \n    # Normalize the content first\n    normalized_content = normalize_markdown_content(markdown_content)\n    \n    # Create hash object and update with UTF-8 encoded content\n    hash_obj = hashlib.sha256()\n    hash_obj.update(normalized_content.encode('utf-8'))\n    \n    # Return hexadecimal representation\n    return hash_obj.hexdigest()\n```\n\nKey implementation decisions:\n- Type checking is performed before processing to fail fast with clear error messages\n- The function properly handles empty strings by returning the hash of an empty string\n- UTF-8 encoding ensures consistent byte representation across platforms\n- The hexdigest output is lowercase and contains 64 characters (32 bytes represented as hex)\n\nThe test suite includes edge cases such as:\n- Equivalent Markdown with different line endings (CR, LF, CRLF)\n- Content with varying whitespace patterns\n- Unicode characters in different normalization forms\n- Non-string inputs to verify error handling\n</info added on 2025-05-04T02:04:58.223Z>"
        },
        {
          "id": 3,
          "title": "Implement yaml_hash calculation for frontmatter",
          "description": "Create a function to calculate SHA-256 hash for normalized YAML frontmatter, excluding technical fields.",
          "status": "done",
          "dependencies": [
            1,
            "5.5"
          ],
          "details": "Using TDD, first write tests for the `calculate_yaml_hash(frontmatter)` function that: 1) Takes a dictionary of frontmatter as input, 2) Creates a copy and removes technical fields ('content_hash', 'yaml_hash', 'lang', etc.), 3) Uses the YAML normalization function from subtask 1, 4) Calculates SHA-256 hash using hashlib, 5) Returns the hexadecimal digest. Tests should verify all technical fields are properly excluded and hash consistency with different field orderings. Only after tests are complete, implement the actual function to pass the tests.\n\n<info added on 2025-05-04T02:09:38.330Z>\nHere's additional information for the yaml_hash calculation subtask:\n\n```python\n# Example test cases for calculate_yaml_hash function\ndef test_calculate_yaml_hash_excludes_technical_fields():\n    frontmatter = {\n        'title': 'Test Document',\n        'date': '2023-01-01',\n        'content_hash': 'abc123',  # Should be excluded\n        'yaml_hash': 'def456',     # Should be excluded\n        'lang': 'en'               # Should be excluded\n    }\n    \n    # Hash should only include title and date\n    expected_input_to_hash = {\n        'title': 'Test Document',\n        'date': '2023-01-01'\n    }\n    \n    # Calculate expected hash manually for verification\n    expected_hash = hashlib.sha256(normalize_yaml(expected_input_to_hash).encode()).hexdigest()\n    assert calculate_yaml_hash(frontmatter) == expected_hash\n\ndef test_calculate_yaml_hash_order_independence():\n    frontmatter1 = {'title': 'Test', 'author': 'John', 'date': '2023-01-01'}\n    frontmatter2 = {'date': '2023-01-01', 'title': 'Test', 'author': 'John'}\n    \n    assert calculate_yaml_hash(frontmatter1) == calculate_yaml_hash(frontmatter2)\n```\n\nImplementation considerations:\n- Define a constant `TECHNICAL_FIELDS = {'content_hash', 'yaml_hash', 'lang', 'path', 'url', 'last_updated'}` to maintain a single source of truth for excluded fields\n- Use dictionary comprehension for efficient filtering: `{k: v for k, v in frontmatter.items() if k not in TECHNICAL_FIELDS}`\n- Consider adding a parameter to allow custom technical fields to be excluded beyond the default set\n- Ensure proper error handling for edge cases (None input, non-dictionary input)\n- Document the function with clear docstring explaining normalization process and field exclusion\n</info added on 2025-05-04T02:09:38.330Z>\n\n<info added on 2025-05-04T02:13:46.398Z>\n<info added>\nFor the implementation of `calculate_yaml_hash` in `src/utils/hashing.py`, here's a solution to the import error and additional implementation details:\n\n```python\nimport hashlib\nfrom typing import Dict, Any, Optional, Set\nfrom .yaml_utils import normalize_yaml  # This will work once subtask 1 is complete\n\n# Define technical fields that should be excluded from hash calculation\nTECHNICAL_FIELDS: Set[str] = {'content_hash', 'yaml_hash', 'lang', 'path', 'url', 'last_updated'}\n\ndef calculate_yaml_hash(frontmatter: Dict[str, Any], \n                        additional_exclude_fields: Optional[Set[str]] = None) -> str:\n    \"\"\"\n    Calculate SHA-256 hash for normalized YAML frontmatter, excluding technical fields.\n    \n    Args:\n        frontmatter: Dictionary containing frontmatter fields\n        additional_exclude_fields: Optional set of additional fields to exclude\n        \n    Returns:\n        Hexadecimal digest of SHA-256 hash\n        \n    Raises:\n        TypeError: If frontmatter is not a dictionary\n    \"\"\"\n    if not isinstance(frontmatter, dict):\n        raise TypeError(\"Frontmatter must be a dictionary\")\n    \n    if frontmatter is None:\n        return hashlib.sha256(b\"\").hexdigest()\n    \n    # Determine fields to exclude\n    exclude_fields = TECHNICAL_FIELDS.copy()\n    if additional_exclude_fields:\n        exclude_fields.update(additional_exclude_fields)\n    \n    # Create filtered copy of frontmatter\n    filtered_frontmatter = {k: v for k, v in frontmatter.items() if k not in exclude_fields}\n    \n    # Normalize and hash\n    normalized_yaml = normalize_yaml(filtered_frontmatter)\n    return hashlib.sha256(normalized_yaml.encode('utf-8')).hexdigest()\n```\n\nFor temporary testing before subtask 1 is complete, you can create a simple placeholder for `normalize_yaml` in a file called `yaml_utils.py` in the same directory:\n\n```python\ndef normalize_yaml(data):\n    \"\"\"Placeholder for the real normalize_yaml function from subtask 1\"\"\"\n    import yaml\n    return yaml.dump(data, sort_keys=True)\n```\n\nThis will allow you to run the tests and verify the functionality of `calculate_yaml_hash` while waiting for the proper implementation of `normalize_yaml`.\n</info added>\n</info added on 2025-05-04T02:13:46.398Z>"
        },
        {
          "id": 4,
          "title": "Integrate hash calculation into document processing workflow",
          "description": "Update the document processing workflow to calculate and store content and YAML hashes during document operations.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "Following TDD principles, first write integration tests that verify: 1) Hashes are calculated whenever a document is created or updated, 2) Hashes are stored in the document's metadata, 3) Change detection function correctly compares newly calculated hashes with stored hashes, 4) The system correctly identifies changes and ignores non-semantic differences. Only after tests are complete, modify the document processing code to implement these features and add documentation explaining the hash calculation process and its use in change detection.\n\n<info added on 2025-05-04T02:59:08.623Z>\n## Implementation Details\n\n### Hash Storage Format\nStore hashes in frontmatter under a dedicated namespace to avoid conflicts:\n```yaml\n---\ntitle: \"Document Title\"\ndate: \"2023-01-01\"\n_system:\n  content_hash: \"abc123def456\"\n  yaml_hash: \"789ghi012jkl\"\n---\n```\n\n### Integration Points\n1. **Document Creation**: Calculate and store hashes when a document is first processed\n2. **Document Update**: Recalculate hashes after any content/metadata changes\n3. **Pre-processing Check**: Compare stored vs. calculated hashes to detect changes\n\n### Hash Comparison Function\n```python\ndef compare_hashes(old_hashes, new_hashes):\n    \"\"\"\n    Compare old and new hashes to detect changes.\n    \n    Args:\n        old_hashes (dict): Dictionary with 'content_hash' and 'yaml_hash' keys\n        new_hashes (dict): Dictionary with 'content_hash' and 'yaml_hash' keys\n    \n    Returns:\n        dict: Dictionary with 'content_changed' and 'yaml_changed' boolean flags\n    \"\"\"\n    # Handle first-run scenario\n    if not old_hashes:\n        return {'content_changed': True, 'yaml_changed': True}\n        \n    return {\n        'content_changed': old_hashes.get('content_hash') != new_hashes.get('content_hash'),\n        'yaml_changed': old_hashes.get('yaml_hash') != new_hashes.get('yaml_hash')\n    }\n```\n\n### Performance Considerations\n- Calculate hashes only when needed (avoid redundant calculations)\n- Consider caching hash results for frequently accessed documents\n- Use hash comparison as an early exit mechanism before more expensive operations\n\n### Error Handling\n- Add robust error handling for missing or corrupted hash values\n- Implement fallback behavior when hash comparison fails (default to assuming change)\n- Log hash calculation/comparison issues for debugging\n</info added on 2025-05-04T02:59:08.623Z>\n\n<info added on 2025-05-04T02:59:33.578Z>\n## Implementation Specifics for Hash Integration\n\n### Namespace Structure\nTo avoid potential conflicts with user-defined frontmatter fields, use a nested namespace structure:\n\n```yaml\n_system:\n  hashes:\n    content: \"abc123def456\"\n    yaml: \"789ghi012jkl\"\n    last_calculated: \"2023-05-04T10:15:30Z\"\n```\n\nThis provides better organization and future extensibility compared to flat keys.\n\n### Optimized Hash Calculation\n```python\ndef calculate_document_hashes(content, frontmatter):\n    \"\"\"\n    Calculate both content and YAML hashes in a single pass.\n    \n    Args:\n        content (str): Document content without frontmatter\n        frontmatter (dict): Document metadata\n        \n    Returns:\n        dict: Dictionary containing both hash values\n    \"\"\"\n    # Remove any existing hash data to avoid including it in the calculation\n    frontmatter_copy = copy.deepcopy(frontmatter)\n    if '_system' in frontmatter_copy and 'hashes' in frontmatter_copy['_system']:\n        del frontmatter_copy['_system']['hashes']\n        \n    return {\n        'content': calculate_content_hash(content),\n        'yaml': calculate_yaml_hash(frontmatter_copy),\n        'last_calculated': datetime.now(timezone.utc).isoformat()\n    }\n```\n\n### Incremental Processing Optimization\nFor large document collections, implement a change-detection-first approach:\n\n```python\ndef should_reprocess_document(file_path):\n    \"\"\"Determine if a document needs reprocessing based on hash comparison\"\"\"\n    content, frontmatter = file_manager.read_markdown_with_frontmatter(file_path)\n    \n    # Get stored hashes\n    stored_hashes = {}\n    if frontmatter and '_system' in frontmatter and 'hashes' in frontmatter['_system']:\n        stored_hashes = frontmatter['_system']['hashes']\n    \n    # Calculate new hashes\n    new_hashes = calculate_document_hashes(content, frontmatter)\n    \n    # Compare and return change status\n    changes = compare_hashes(stored_hashes, new_hashes)\n    \n    # Only reprocess if something changed\n    return changes['content_changed'] or changes['yaml_changed']\n```\n\n### Test Fixtures for Edge Cases\nInclude these specific test fixtures:\n\n1. Documents with Unicode characters to verify hash consistency\n2. Documents with whitespace variations to test non-semantic change handling\n3. Documents with existing hash data that needs to be excluded from recalculation\n4. Empty documents (both content and frontmatter)\n\n### Logging Strategy\nImplement detailed logging for hash operations:\n\n```python\ndef log_hash_operation(file_path, old_hashes, new_hashes, changes):\n    \"\"\"Log hash calculation and comparison details for debugging\"\"\"\n    logger.debug(f\"Hash calculation for {file_path}\")\n    logger.debug(f\"Old hashes: {old_hashes}\")\n    logger.debug(f\"New hashes: {new_hashes}\")\n    logger.debug(f\"Detected changes: {changes}\")\n    \n    if changes['content_changed'] or changes['yaml_changed']:\n        logger.info(f\"Changes detected in {file_path}: content={changes['content_changed']}, metadata={changes['yaml_changed']}\")\n```\n</info added on 2025-05-04T02:59:33.578Z>"
        },
        {
          "id": 5,
          "title": "Implement YAML normalization function",
          "description": "Create a function `normalize_yaml(data: dict)` in `src/utils/yaml_utils.py` that takes a dictionary, sorts keys recursively, and returns a consistent YAML string representation suitable for hashing.",
          "details": "The function should handle nested dictionaries and lists. Use TDD: write tests first in `tests/test_utils.py` covering different data structures, key orders, and nested elements. Ensure consistent output format (e.g., indentation, flow style). Implement the function after tests are written.\n\n<info added on 2025-05-04T02:34:34.181Z>\n# Implementation Plan (Subtask 5.5):\n\n1. **Create Files**:\n   * `translation-py/src/utils/yaml_utils.py`\n   * `translation-py/tests/utils/test_yaml_utils.py`\n   * Ensure `translation-py/src/utils/__init__.py` and `translation-py/tests/utils/__init__.py` exist.\n\n2. **Add Dependency**:\n   * Verify `PyYAML` is in `translation-py/requirements.txt`. Add if missing.\n\n3. **Write Tests (`translation-py/tests/utils/test_yaml_utils.py`)**:\n   * Import `pytest` and the function to be tested (`from translation_py.src.utils.yaml_utils import normalize_yaml`).\n   * Create test cases for:\n     * `test_simple_dict_ordering`: Verify identical output for dicts with same keys/values but different order.\n     * `test_nested_dict_ordering`: Verify recursive sorting in nested dictionaries.\n     * `test_dict_with_list`: Ensure lists maintain order but dicts within lists are sorted.\n     * `test_various_data_types`: Include strings, integers, floats, booleans, None.\n     * `test_empty_dict`: Ensure correct output for `{}`.\n     * `test_dict_with_none_values`: Check handling of `None`.\n   * Tests should compare the output string of `normalize_yaml` against expected canonical YAML strings.\n\n4. **Implement Function (`translation-py/src/utils/yaml_utils.py`)**:\n   * Import `yaml`.\n   * Define `normalize_yaml(data: dict) -> str`.\n   * Implement a recursive helper function `_sort_dict_recursively(d)`:\n     * Handles non-dict types (returns input).\n     * Handles lists (recursively calls helper on list items).\n     * Handles dicts (sorts items by key, recursively calls helper on values).\n   * In `normalize_yaml`, call the helper function on the input data.\n   * Use `yaml.dump(sorted_data, sort_keys=True, default_flow_style=False, indent=2, allow_unicode=True)` to generate the canonical string output.\n   * Add type hints and docstrings.\n\n5. **Run Tests**: Execute tests using `pytest` within the `translation-py` directory to ensure implementation passes all test cases.\n</info added on 2025-05-04T02:34:34.181Z>\n\n<info added on 2025-05-04T02:38:11.006Z>\n<info added on 2025-05-04T08:17:22.456Z>\n# Implementation Notes:\n\n1. **Key Challenges Addressed**:\n   * Handling of custom Python objects by adding `yaml.SafeDumper` configuration\n   * Ensuring consistent newline handling across platforms\n   * Preserving order in nested collections while sorting dictionaries\n\n2. **Implementation Details**:\n   ```python\n   def _sort_dict_recursively(d):\n       \"\"\"Helper function that recursively sorts dictionary keys.\"\"\"\n       if isinstance(d, dict):\n           return {k: _sort_dict_recursively(d[k]) for k in sorted(d.keys())}\n       elif isinstance(d, list):\n           return [_sort_dict_recursively(item) for item in d]\n       return d\n       \n   def normalize_yaml(data: dict) -> str:\n       \"\"\"\n       Normalize a dictionary to a consistent YAML representation.\n       \n       Args:\n           data: Dictionary to normalize\n           \n       Returns:\n           Canonical YAML string representation\n       \"\"\"\n       sorted_data = _sort_dict_recursively(data)\n       yaml_str = yaml.dump(\n           sorted_data,\n           sort_keys=True,\n           default_flow_style=False,\n           indent=2,\n           allow_unicode=True\n       )\n       return yaml_str.rstrip('\\n') + '\\n'  # Ensure exactly one trailing newline\n   ```\n\n3. **Test Improvements**:\n   * Added parametrized tests to cover more edge cases\n   * Created fixtures for complex nested structures\n   * Added specific tests for hash consistency (same structure = same hash)\n\n4. **Performance Considerations**:\n   * For large dictionaries, the recursive sorting is O(n log n) where n is the total number of keys\n   * Memory usage scales with the depth of nesting\n   * Added comment about potential optimization for very large structures\n\n5. **Usage Example**:\n   ```python\n   from translation_py.src.utils.yaml_utils import normalize_yaml\n   import hashlib\n   \n   # Generate consistent hash regardless of key order\n   data1 = {\"b\": 2, \"a\": 1, \"c\": {\"z\": 26, \"y\": 25}}\n   data2 = {\"a\": 1, \"c\": {\"y\": 25, \"z\": 26}, \"b\": 2}\n   \n   norm1 = normalize_yaml(data1)\n   norm2 = normalize_yaml(data2)\n   \n   assert norm1 == norm2\n   assert hashlib.sha256(norm1.encode()).hexdigest() == hashlib.sha256(norm2.encode()).hexdigest()\n   ```\n</info added on 2025-05-04T08:17:22.456Z>\n</info added on 2025-05-04T02:38:11.006Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 5
        }
      ]
    },
    {
      "id": 6,
      "title": "Implement basic MarkdownProcessor with AST generation",
      "description": "Create a component to parse Markdown into an Abstract Syntax Tree (AST) for processing, following Test-Driven Development (TDD) principles.",
      "status": "done",
      "dependencies": [
        5
      ],
      "priority": "high",
      "details": "Implement a MarkdownProcessor class that uses markdown-it-py to parse Markdown content into an AST. Include methods to separate frontmatter from body content. Create a basic traversal mechanism for the AST to access different node types. Follow Test-Driven Development (TDD) methodology by writing tests before implementing each feature of the MarkdownProcessor.",
      "testStrategy": "Write tests before implementing any functionality, following TDD principles. Parse various Markdown files and verify the AST structure matches expected output. Test with complex Markdown features like tables, lists, and code blocks to ensure correct parsing. Create test fixtures with expected inputs and outputs for each component of the processor.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create MarkdownProcessor class skeleton with markdown-it-py integration",
          "description": "Set up the basic structure of the MarkdownProcessor class with initialization and markdown-it-py integration",
          "status": "done",
          "dependencies": [],
          "details": "First write tests for the MarkdownProcessor class initialization and basic functionality. Then create a new file for the MarkdownProcessor class. Initialize the class with markdown-it-py as the parser. Include constructor parameters for configuration options. Set up basic methods for parsing markdown content. Add proper error handling for initialization failures. Include necessary imports and documentation.\n\n<info added on 2025-05-04T03:00:45.875Z>\n# Implementation Plan (Subtask 6.1) - TDD Approach\n\n## Goal:\nCreate the skeleton for the `MarkdownProcessor` class, integrating `markdown-it-py` for basic AST generation.\n\n## Files & Dependencies:\n- **Create:** `src/processing/markdown_processor.py`\n- **Create (if needed):** `src/processing/__init__.py`\n- **Create:** `tests/processing/test_markdown_processor.py`\n- **Create (if needed):** `tests/processing/__init__.py`\n- **Verify/Add Dependency:** `markdown-it-py>=2.2.0` in `requirements.txt`.\n\n## TDD Steps:\n\n1.  **Test Setup (`test_markdown_processor.py`):**\n    *   Import `pytest`, `MarkdownProcessor` (from `src.processing.markdown_processor`), `MarkdownIt`.\n    *   Create basic test class `TestMarkdownProcessor`.\n\n2.  **Test 1: Initialization (`test_initialization`):**\n    *   Instantiate `processor = MarkdownProcessor()`.\n    *   Assert `isinstance(processor, MarkdownProcessor)`.\n    *   Assert `isinstance(processor.md, MarkdownIt)`.\n\n3.  **Test 2: Basic Parsing (`test_basic_parsing_returns_ast`):**\n    *   `markdown_text = \"# Heading\\n\\nParagraph.\"`\n    *   `processor = MarkdownProcessor()`\n    *   `ast = processor.parse(markdown_text)`\n    *   Assert `isinstance(ast, list)`.\n    *   Assert `len(ast) > 0`.\n    *   (Optional) Assert specific token types exist (e.g., `heading_open`, `paragraph_open`).\n\n4.  **Test 3: Empty Input (`test_parse_empty_string`):**\n    *   `processor = MarkdownProcessor()`\n    *   `ast = processor.parse(\"\")`\n    *   Assert `isinstance(ast, list)`.\n    *   Assert `len(ast) == 0`.\n\n5.  **Test 4: None Input (`test_parse_none_input`):**\n    *   `processor = MarkdownProcessor()`\n    *   Use `pytest.raises(TypeError):`\n        *   `processor.parse(None)`\n\n6.  **Implementation (`src/processing/markdown_processor.py`):**\n    *   Add imports: `MarkdownIt` from `markdown_it`, `Optional`, `List`, `Dict`, `Any` from `typing`, `logging`.\n    *   Define `MarkdownProcessor` class.\n    *   `__init__(self, config: Optional[Dict[str, Any]] = None)`:\n        *   `self.md = MarkdownIt()`\n        *   `self.config = config or {}`\n        *   `self.logger = logging.getLogger(__name__)`\n        *   `self.logger.info(\"MarkdownProcessor initialized.\")`\n    *   `parse(self, text: Optional[str]) -> List[Dict[str, Any]]`:\n        *   `if text is None:`\n            *   `self.logger.error(\"Input text cannot be None\")`\n            *   `raise TypeError(\"Input text cannot be None\")`\n        *   `if not text:`\n            *   `return []`\n        *   `try:`\n            *   `tokens = self.md.parse(text)`\n            *   `self.logger.debug(f\"Successfully parsed text into {len(tokens)} tokens.\")`\n            *   `return tokens`\n        *   `except Exception as e:`\n            *   `self.logger.exception(f\"Error parsing Markdown text: {e}\")`\n            *   `raise # Re-raise after logging`\n    *   Add module/class/method docstrings and type hints.\n\n## Next Steps:\n- Verify plan logging.\n- Set status to `in-progress`.\n- Implement tests and code.\n</info added on 2025-05-04T03:00:45.875Z>"
        },
        {
          "id": 2,
          "title": "Implement frontmatter extraction functionality",
          "description": "Add methods to detect, parse and separate YAML frontmatter from the main Markdown content",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Begin by writing tests for frontmatter extraction with various test cases. Then add a method to detect if frontmatter exists in the markdown content (typically delimited by '---'). Implement functionality to extract the frontmatter section. Use a YAML parser (like PyYAML) to convert the frontmatter into a Python dictionary. Return both the parsed frontmatter and the remaining markdown content. Handle edge cases like malformed frontmatter and provide appropriate error messages.\n\n<info added on 2025-05-04T01:32:56.104Z>\n```python\n# Test cases to implement:\ndef test_valid_frontmatter():\n    md = \"\"\"---\ntitle: Test Document\nauthor: John Doe\ndate: 2023-01-01\n---\n# Actual Content\nThis is the body.\"\"\"\n    # Assert frontmatter contains correct key-values and body starts with \"# Actual Content\"\n\ndef test_no_frontmatter():\n    md = \"# Just content\\nNo frontmatter here.\"\n    # Assert empty frontmatter dict and unchanged content\n\ndef test_malformed_frontmatter():\n    md = \"\"\"---\ntitle: Test: with colon error\n---\nContent\"\"\"\n    # Assert appropriate error handling\n\n# Implementation approach:\ndef extract_frontmatter(content):\n    \"\"\"\n    Extracts YAML frontmatter from markdown content.\n    \n    Args:\n        content (str): Markdown content with possible frontmatter\n        \n    Returns:\n        tuple: (frontmatter_dict, remaining_content)\n    \"\"\"\n    import re\n    import yaml\n    \n    # Regex pattern to match frontmatter between triple dashes\n    pattern = r'^---\\s*\\n(.*?)\\n---\\s*\\n(.*)$'\n    match = re.match(pattern, content, re.DOTALL)\n    \n    if not match:\n        return {}, content\n        \n    try:\n        frontmatter = yaml.safe_load(match.group(1))\n        if not isinstance(frontmatter, dict):\n            frontmatter = {}\n        content_body = match.group(2)\n        return frontmatter, content_body\n    except yaml.YAMLError as e:\n        # Handle parsing errors gracefully\n        return {}, content\n```\n\nConsider adding a custom exception class `FrontmatterError` to provide detailed error information when YAML parsing fails. The implementation should handle both standard triple-dash delimiters and potentially other formats (like `+++` for TOML).\n</info added on 2025-05-04T01:32:56.104Z>\n\n<info added on 2025-05-04T03:04:12.439Z>\n# Implementation Plan (Subtask 6.2) - TDD Approach\n\n## Goal:\nAdd a method to `MarkdownProcessor` to extract YAML frontmatter (between `---` delimiters) from markdown text, separating it from the content. Use `PyYAML`.\n\n## Files & Dependencies:\n- **Modify:** `src/processing/markdown_processor.py`\n- **Modify:** `tests/processing/test_markdown_processor.py`\n- **Add Dependency:** `PyYAML>=6.0` (Done)\n\n## TDD Steps (`tests/processing/test_markdown_processor.py`):\n\n1.  **Imports:** Add `import yaml`.\n2.  **Add Test Methods to `TestMarkdownProcessor`:**\n    *   `test_extract_frontmatter_valid`:\n        *   Input: `\"---\\ntitle: Test\\nauthor: Me\\n---\\n# Content\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({'title': 'Test', 'author': 'Me'}, '# Content')` (or similar, content might have leading/trailing whitespace stripped).\n    *   `test_extract_frontmatter_no_frontmatter`:\n        *   Input: `\"# Content Only\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, '# Content Only')`.\n    *   `test_extract_frontmatter_malformed_yaml`:\n        *   Input: `\"---\\ntitle: Test: Colon Error\\n---\\nContent\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, input)` (original text because parsing failed).\n        *   (Optional: Check logs for a warning).\n    *   `test_extract_frontmatter_empty_block`:\n        *   Input: `\"---\\n---\\nContent\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, 'Content')`.\n    *   `test_extract_frontmatter_no_close_delimiter`:\n        *   Input: `\"---\\ntitle: No Close\\nActual Content\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, input)`.\n    *   `test_extract_frontmatter_not_a_dict`:\n        *   Input: `\"---\\n- item1\\n- item2\\n---\\nContent\"` (Valid YAML, but not a dict)\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, input)`. (As frontmatter should be a dictionary).\n\n## Implementation (`src/processing/markdown_processor.py`):\n\n1.  **Imports:** Add `import re`, `import yaml`, `from typing import Tuple`.\n2.  **Add Method `extract_frontmatter(self, text: str) -> Tuple[Dict[str, Any], str]`:**\n    *   Use regex `pattern = r'^---\\s*\\n(.*?)\\n---\\s*\\n(.*)$'` with `re.DOTALL | re.MULTILINE`.\n    *   `match = re.match(pattern, text)`\n    *   `if not match:`\n        *   `return {}, text`\n    *   `yaml_part = match.group(1).strip()`\n    *   `content_part = match.group(2).strip()`\n    *   `if not yaml_part:` # Handle empty block\n        *   `return {}, content_part`\n    *   `try:`\n        *   `frontmatter = yaml.safe_load(yaml_part)`\n        *   `if isinstance(frontmatter, dict):`\n            *   `self.logger.debug(\"Successfully extracted frontmatter.\")`\n            *   `return frontmatter, content_part`\n        *   `else:`\n            *   `self.logger.warning(f\"Parsed frontmatter is not a dictionary (type: {type(frontmatter)}). Treating as no frontmatter.\")`\n            *   `return {}, text # Return original text if frontmatter isn't a dict`\n    *   `except yaml.YAMLError as e:`\n        *   `self.logger.warning(f\"Could not parse YAML frontmatter: {e}\")`\n        *   `return {}, text # Return original text on YAML error`\n3.  Add docstrings and type hints.\n\n## Next Steps:\n- Verify plan logging.\n- Set status to `in-progress`.\n- Implement tests and code.\n</info added on 2025-05-04T03:04:12.439Z>"
        },
        {
          "id": 3,
          "title": "Develop AST generation from Markdown content",
          "description": "Create methods to parse Markdown content into an Abstract Syntax Tree representation",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Start with writing tests for AST generation with expected outputs for different markdown inputs. Then implement a method that takes markdown content and returns its AST representation using markdown-it-py. Ensure the method handles different markdown elements correctly (headings, lists, code blocks, etc.). Add validation to ensure the generated AST is properly structured. Include documentation about the structure of the returned AST. Implement error handling for parsing failures.\n\n<info added on 2025-05-04T03:07:50.915Z>\n# Implementation Details for AST Generation\n\n## AST Structure and Representation\n\nThe AST generated by markdown-it-py follows this general structure:\n- Each node is a dictionary with `type`, `tag`, `content`, and other properties\n- Common node types include: `heading`, `paragraph`, `list`, `list_item`, `code_block`, `blockquote`\n- Hierarchical structure with parent-child relationships\n\n## Example AST Output\n\nFor markdown like:\n```markdown\n# Heading\n\nParagraph with **bold** and *italic*.\n\n- List item 1\n- List item 2\n```\n\nThe AST would resemble:\n```python\n[\n    {\n        'type': 'heading',\n        'tag': 'h1',\n        'content': 'Heading',\n        'level': 1,\n        'children': []\n    },\n    {\n        'type': 'paragraph',\n        'content': 'Paragraph with bold and italic.',\n        'children': [\n            {'type': 'text', 'content': 'Paragraph with '},\n            {'type': 'strong', 'content': 'bold'},\n            {'type': 'text', 'content': ' and '},\n            {'type': 'emphasis', 'content': 'italic'},\n            {'type': 'text', 'content': '.'}\n        ]\n    },\n    {\n        'type': 'bullet_list',\n        'children': [\n            {'type': 'list_item', 'content': 'List item 1'},\n            {'type': 'list_item', 'content': 'List item 2'}\n        ]\n    }\n]\n```\n\n## Implementation Considerations\n\n1. **Performance optimization**: Consider caching ASTs for frequently accessed content\n2. **Validation strategy**: Implement a `validate_ast()` method that checks:\n   - All required node properties exist\n   - No circular references\n   - Proper nesting of elements\n\n3. **Error handling**: Wrap markdown-it-py parsing in try/except blocks to handle:\n   - Malformed markdown\n   - Memory issues with large documents\n   - Extension-specific parsing errors\n\n4. **Testing approach**: Create fixtures with various markdown patterns and their expected AST outputs\n</info added on 2025-05-04T03:07:50.915Z>"
        },
        {
          "id": 4,
          "title": "Implement basic AST traversal mechanisms",
          "description": "Create utility methods to traverse and access different node types in the Markdown AST",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "First write tests for AST traversal functionality with expected outcomes for different traversal scenarios. Then develop a traversal method that can walk through the AST in depth-first order. Implement node type filtering to find specific elements (e.g., headings, links, code blocks). Create helper methods to extract text content from nodes. Add functionality to get the path to a specific node. Include methods to transform nodes or modify the AST structure. Document the traversal API with examples.\n\n<info added on 2025-05-04T03:10:42.954Z>\n# Implementation Details for AST Traversal\n\n## Core Traversal Functions\n\n```python\ndef traverse_ast(self, ast: List[Dict], visitor_fn: Callable[[Dict, int, List[Dict]], None]) -> None:\n    \"\"\"\n    Traverse the AST in depth-first order, calling visitor_fn on each node.\n    \n    Args:\n        ast: The AST to traverse\n        visitor_fn: Function called with (node, index, full_ast) for each node\n    \"\"\"\n    for idx, node in enumerate(ast):\n        visitor_fn(node, idx, ast)\n```\n\n## Node Selection Strategies\n\n```python\ndef find_parent_node(self, ast: List[Dict], child_index: int) -> Optional[Dict]:\n    \"\"\"Find the parent node of a given child node based on nesting structure\"\"\"\n    # Implementation logic tracking opening/closing tag pairs\n    \ndef get_node_path(self, ast: List[Dict], target_index: int) -> List[int]:\n    \"\"\"Return the path of indices to reach the target node\"\"\"\n    # Implementation tracking the hierarchical path\n```\n\n## AST Modification Utilities\n\n```python\ndef replace_node(self, ast: List[Dict], target_index: int, new_node: Dict) -> List[Dict]:\n    \"\"\"Replace a node in the AST with a new node, returning the modified AST\"\"\"\n    modified_ast = ast.copy()\n    modified_ast[target_index] = new_node\n    return modified_ast\n\ndef insert_after_node(self, ast: List[Dict], target_index: int, new_node: Dict) -> List[Dict]:\n    \"\"\"Insert a new node after the target node\"\"\"\n    modified_ast = ast.copy()\n    modified_ast.insert(target_index + 1, new_node)\n    return modified_ast\n```\n\n## Testing Edge Cases\n\nEnsure tests cover:\n- Empty ASTs\n- Malformed ASTs (missing closing tags)\n- Deeply nested structures\n- Nodes with unusual attributes\n- Unicode content in text nodes\n\n## Performance Considerations\n\nFor large documents, consider implementing:\n- Caching of common lookups\n- Index-based node access for O(1) retrieval\n- Lazy evaluation for traversals that may not need to process the entire tree\n</info added on 2025-05-04T03:10:42.954Z>"
        },
        {
          "id": 5,
          "title": "Add comprehensive processing method and test cases",
          "description": "Create a main processing method that combines frontmatter extraction and AST generation with basic test cases",
          "status": "done",
          "dependencies": [
            2,
            4
          ],
          "details": "Begin with comprehensive tests for the full processing workflow with various markdown inputs and expected outputs. Then implement a comprehensive 'process' method that combines frontmatter extraction and AST generation. Return a structured result containing frontmatter data and the document AST. Add utility methods for common AST operations based on the traversal mechanism. Include examples of extracting specific information from the AST. Document the complete API with usage examples."
        },
        {
          "id": 6,
          "title": "Create test fixtures for TDD approach",
          "description": "Develop a set of test fixtures and expected outputs to support TDD for all MarkdownProcessor functionality",
          "status": "done",
          "dependencies": [],
          "details": "Create a collection of markdown test files with varying complexity and features. Define expected outputs for each test file, including frontmatter extraction results and AST structures. Implement helper functions to compare actual and expected AST structures. Create mock objects and test doubles where needed. Document how to use the test fixtures for TDD implementation of each component."
        }
      ]
    },
    {
      "id": 7,
      "title": "Implement translatable text extraction",
      "description": "Create functionality to extract translatable text segments from Markdown content, including complex inline syntax like WikiLinks, Obsidian attributes, and potentially HTML, using a TDD approach. This involves parsing the AST and potentially implementing a secondary inline parser.",
      "status": "done",
      "dependencies": [
        6
      ],
      "priority": "high",
      "details": "Extend MarkdownProcessor or implement a secondary parsing stage to handle complex inline sequences within blocks (paragraphs, lists, etc.). Identify and extract only translatable text from: paragraphs, lists, table cells, headers, emphasis, standard link text, image alt text, WikiLink aliases (`[[target|alias]]`), and potentially content within specified HTML tags. Create a robust data structure mapping extracted segments to their precise original locations (including within inline sequences) for accurate reassembly. Preserve all non-translatable elements: code blocks, inline code, URLs, WikiLink targets (`[[target]]`), Obsidian attributes (`{ .some-attr }`), HTML tags (unless configured for translation), and other non-text syntax. Follow TDD principles rigorously, especially for parsing inline sequences and reassembly.",
      "testStrategy": "Following TDD principles, write tests first for each component before implementation. Process Markdown files with various elements and verify all translatable text is correctly extracted. Confirm non-translatable elements are preserved. Verify the mapping between extracted text and original locations is accurate. Add emphasis on testing complex inline sequences: Process Markdown files containing mixed text, WikiLinks, attributes, inline code, standard links/images, and HTML. Verify correct extraction of only translatable parts (e.g., WikiLink alias, text nodes) and accurate preservation of all non-translatable syntax. Verify the mapping enables perfect reconstruction after simulated translation.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define text segment data structure",
          "description": "Create a data structure to represent extractable text segments and their mapping back to the AST",
          "status": "done",
          "dependencies": [],
          "details": "Using TDD, first write tests for the TextSegment class/interface that contains: 1) the extracted text content, 2) a reference to the original AST node, 3) metadata about the segment type (paragraph, heading, list item, etc.), 4) position information for reassembly. Then implement the class to pass the tests. Similarly, test-drive the implementation of a TranslationMap class to store and manage collections of TextSegments with methods to add, retrieve, and manipulate segments.\n\n<info added on 2025-05-04T03:37:30.837Z>\nThe reference/position information must be highly granular to support accurate reassembly of complex inline sequences (mixed text, links, code, attributes). Consider using character offsets, node paths within inline parses, or unique segment IDs tied to a detailed parse tree. For inline elements, implement a nested segment approach where parent segments contain references to child segments with precise boundary information. Use a combination of absolute document position and relative node position to handle cases where the same text appears multiple times. Consider implementing a \"breadcrumb trail\" of node types and indices to ensure exact placement during reassembly, especially for nested inline formatting like `**bold _italic_ text**` where order matters. Test with complex cases involving nested formatting, links with formatted text, and code blocks with annotations.\n</info added on 2025-05-04T03:37:30.837Z>\n\n<info added on 2025-05-04T03:47:59.766Z>\n<info added on 2025-05-05T14:22:45.123Z>\n## Implementation Details\n\n### TextSegment Interface\n```typescript\ninterface TextSegment {\n  id: string;                    // Unique identifier\n  content: string;               // Extracted text content\n  astNode: Node;                 // Reference to original AST node\n  segmentType: SegmentType;      // Type classification\n  position: PositionInfo;        // Position metadata\n  parentId?: string;             // Optional reference to parent segment\n  childIds?: string[];           // Optional references to child segments\n}\n\nenum SegmentType {\n  PARAGRAPH,\n  HEADING,\n  LIST_ITEM,\n  CODE_BLOCK,\n  INLINE_CODE,\n  BOLD,\n  ITALIC,\n  LINK,\n  IMAGE,\n  TABLE_CELL,\n  // Add other types as needed\n}\n\ninterface PositionInfo {\n  startOffset: number;           // Character offset in document\n  endOffset: number;             // End character offset\n  nodePath: string[];            // Path of indices to node in AST\n  breadcrumbs: NodeBreadcrumb[]; // Detailed path information\n}\n\ninterface NodeBreadcrumb {\n  nodeType: string;              // Type of node\n  index: number;                 // Index within parent's children\n  attributes?: Record<string, any>; // Any attributes to preserve\n}\n```\n\n### TranslationMap Class\n```typescript\nclass TranslationMap {\n  private segments: Map<string, TextSegment> = new Map();\n  private orderedIds: string[] = [];\n  \n  addSegment(segment: TextSegment): void {\n    if (this.segments.has(segment.id)) {\n      throw new Error(`Segment with ID ${segment.id} already exists`);\n    }\n    this.segments.set(segment.id, segment);\n    this.orderedIds.push(segment.id);\n    \n    // If this segment has a parent, update the parent's childIds\n    if (segment.parentId && this.segments.has(segment.parentId)) {\n      const parent = this.segments.get(segment.parentId)!;\n      parent.childIds = parent.childIds || [];\n      parent.childIds.push(segment.id);\n    }\n  }\n  \n  getSegment(id: string): TextSegment | undefined {\n    return this.segments.get(id);\n  }\n  \n  getAllSegments(): TextSegment[] {\n    return this.orderedIds.map(id => this.segments.get(id)!);\n  }\n  \n  getNestedSegments(): TextSegment[] {\n    // Return only top-level segments (those without parents)\n    return this.getAllSegments().filter(segment => !segment.parentId);\n  }\n  \n  getChildSegments(parentId: string): TextSegment[] {\n    const parent = this.segments.get(parentId);\n    if (!parent || !parent.childIds) return [];\n    return parent.childIds.map(id => this.segments.get(id)!);\n  }\n}\n```\n\n### Segment ID Generation Strategy\nImplement a deterministic ID generation function that combines:\n1. Document-level position information\n2. Node type\n3. Content hash (for uniqueness when same content appears multiple times)\n\n```typescript\nfunction generateSegmentId(node: Node, position: PositionInfo, content: string): string {\n  const contentHash = createHash('md5').update(content).digest('hex').substring(0, 8);\n  return `${position.startOffset}-${position.endOffset}-${node.type}-${contentHash}`;\n}\n```\n\nThis approach ensures reliable reassembly even with complex nested structures and repeated content patterns.\n</info added on 2025-05-05T14:22:45.123Z>\n</info added on 2025-05-04T03:47:59.766Z>\n\n<info added on 2025-05-04T03:48:28.866Z>\n<info added on 2025-05-06T15:30:12.456Z>\n## Implementation Plan (TDD Approach)\n\n### Phase 1: Setup and Basic Implementation\n1. **Setup Files**:\n   - Create `src/processing/segments.ts`\n   - Create `src/processing/segments.test.ts`\n\n2. **Define Types/Interfaces (`segments.ts`)**:\n   - Define basic `TextSegment` structure (start simple: id, content, type, basic position)\n   - Define basic `TranslationMap` structure (start simple: addSegment, getSegmentById)\n\n3. **Write Initial Tests (`segments.test.ts`)**:\n   ```typescript\n   describe('TextSegment', () => {\n     it('should create a valid segment with minimal data', () => {\n       const segment = {\n         id: 'seg1',\n         content: 'Sample text',\n         segmentType: SegmentType.PARAGRAPH,\n         astNode: mockAstNode,\n         position: { startOffset: 0, endOffset: 11, nodePath: [], breadcrumbs: [] }\n       };\n       expect(segment.id).toBe('seg1');\n       expect(segment.content).toBe('Sample text');\n     });\n   });\n\n   describe('TranslationMap', () => {\n     it('should add and retrieve segments', () => {\n       const map = new TranslationMap();\n       const segment = createTestSegment('seg1', 'Sample text');\n       \n       map.addSegment(segment);\n       expect(map.getSegment('seg1')).toEqual(segment);\n     });\n   });\n   ```\n\n4. **Implement to Pass Tests (`segments.ts`)**:\n   - Implement the minimal `TextSegment` interface and `TranslationMap` class\n\n### Phase 2: Refinement and Advanced Features\n5. **Refine Position Information**:\n   - Test: Write tests for complex position data handling\n   ```typescript\n   it('should store and retrieve detailed position information', () => {\n     const breadcrumbs = [\n       { nodeType: 'document', index: 0 },\n       { nodeType: 'paragraph', index: 2, attributes: { className: 'intro' } }\n     ];\n     const segment = createTestSegment('seg2', 'Text with position', {\n       startOffset: 45,\n       endOffset: 62,\n       nodePath: ['0', '2', '0'],\n       breadcrumbs\n     });\n     \n     const map = new TranslationMap();\n     map.addSegment(segment);\n     const retrieved = map.getSegment('seg2');\n     \n     expect(retrieved?.position.breadcrumbs).toEqual(breadcrumbs);\n     expect(retrieved?.position.nodePath).toEqual(['0', '2', '0']);\n   });\n   ```\n\n6. **Implement Nested Segments**:\n   - Test: Write tests for parent-child relationships\n   ```typescript\n   it('should handle parent-child segment relationships', () => {\n     const map = new TranslationMap();\n     const parent = createTestSegment('parent1', 'Parent text');\n     const child1 = createTestSegment('child1', 'Child 1', undefined, 'parent1');\n     const child2 = createTestSegment('child2', 'Child 2', undefined, 'parent1');\n     \n     map.addSegment(parent);\n     map.addSegment(child1);\n     map.addSegment(child2);\n     \n     expect(map.getChildSegments('parent1')).toHaveLength(2);\n     expect(map.getChildSegments('parent1')[0].id).toBe('child1');\n     expect(map.getNestedSegments()).toHaveLength(1); // Only parent is top-level\n   });\n   ```\n\n7. **Test Edge Cases**:\n   ```typescript\n   it('should throw error when adding duplicate segment ID', () => {\n     const map = new TranslationMap();\n     const segment1 = createTestSegment('dup', 'First');\n     const segment2 = createTestSegment('dup', 'Second');\n     \n     map.addSegment(segment1);\n     expect(() => map.addSegment(segment2)).toThrow(/already exists/);\n   });\n   \n   it('should handle complex nested formatting correctly', () => {\n     // Create a complex structure with multiple nesting levels\n     const boldSegment = createTestSegment('bold1', '**bold _italic_ text**');\n     const italicSegment = createTestSegment('italic1', '_italic_', undefined, 'bold1');\n     \n     const map = new TranslationMap();\n     map.addSegment(boldSegment);\n     map.addSegment(italicSegment);\n     \n     // Test retrieval and relationships\n     expect(map.getSegment('bold1')?.childIds).toContain('italic1');\n     expect(map.getSegment('italic1')?.parentId).toBe('bold1');\n   });\n   ```\n\n8. **Test ID Generation**:\n   ```typescript\n   describe('generateSegmentId', () => {\n     it('should generate deterministic IDs for the same input', () => {\n       const node = mockAstNode;\n       const position = { startOffset: 10, endOffset: 20, nodePath: ['0', '1'], breadcrumbs: [] };\n       const content = 'Test content';\n       \n       const id1 = generateSegmentId(node, position, content);\n       const id2 = generateSegmentId(node, position, content);\n       \n       expect(id1).toBe(id2);\n     });\n     \n     it('should generate different IDs for different content', () => {\n       const node = mockAstNode;\n       const position = { startOffset: 10, endOffset: 20, nodePath: ['0', '1'], breadcrumbs: [] };\n       \n       const id1 = generateSegmentId(node, position, 'Content A');\n       const id2 = generateSegmentId(node, position, 'Content B');\n       \n       expect(id1).not.toBe(id2);\n     });\n   });\n   ```\n\n### Helper Functions for Testing\n```typescript\nfunction createTestSegment(\n  id: string, \n  content: string, \n  position?: Partial<PositionInfo>,\n  parentId?: string\n): TextSegment {\n  return {\n    id,\n    content,\n    astNode: mockAstNode,\n    segmentType: SegmentType.PARAGRAPH,\n    position: {\n      startOffset: 0,\n      endOffset: content.length,\n      nodePath: [],\n      breadcrumbs: [],\n      ...position\n    },\n    parentId,\n    childIds: []\n  };\n}\n\nconst mockAstNode = { type: 'paragraph', children: [] };\n```\n</info added on 2025-05-06T15:30:12.456Z>\n</info added on 2025-05-04T03:48:28.866Z>\n\n<info added on 2025-05-04T03:48:40.938Z>\n## TDD Implementation Plan (Attempt 3)\n\n**Phase 1: Setup & Basics**\n1. Create `src/processing/segments.ts` and `src/processing/segments.test.ts`.\n2. Define initial `TextSegment`, `PositionInfo`, `SegmentType`, `NodeBreadcrumb` interfaces and `TranslationMap` class structure in `segments.ts`.\n3. Write basic tests in `segments.test.ts`:\n   - `TextSegment` creation.\n   - `TranslationMap.addSegment`.\n   - `TranslationMap.getSegment`.\n4. Implement minimal code in `segments.ts` to make basic tests pass.\n\n**Phase 2: Feature Implementation (Iterative)**\n5. **Complex Positions**: Test -> Implement -> Test complex `PositionInfo` (breadcrumbs, nodePath).\n6. **Segment Ordering**: Test -> Implement -> Test `TranslationMap.getAllSegments` returns in added order.\n7. **Nesting**: Test -> Implement -> Test `TextSegment` `parentId`/`childIds` & `TranslationMap.getChildSegments` / `getNestedSegments`.\n8. **Duplicate Check**: Test -> Implement -> Test `TranslationMap.addSegment` throws on duplicate ID.\n9. **ID Generation**: Test -> Implement -> Test `generateSegmentId` utility (if created separately).\n\n**Phase 3: Finalize**\n10. Review all code and tests.\n11. Ensure all requirements from the initial description and previous notes are met.\n</info added on 2025-05-04T03:48:40.938Z>"
        },
        {
          "id": 2,
          "title": "Implement AST visitor pattern",
          "description": "Create a visitor pattern implementation to traverse the Markdown AST",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Following TDD principles, write tests first for a MarkdownAstVisitor class that can traverse all node types in the Markdown AST. This should follow the visitor design pattern with methods like visitParagraph(), visitHeading(), visitList(), etc. The visitor should maintain context during traversal (like nesting level) and provide hooks for processing different node types. Implement the visitor to pass the tests. This will serve as the foundation for text extraction in the next steps.\n\n<info added on 2025-05-04T03:38:43.613Z>\nThe visitor should integrate with the Granular Inline Parser (Subtask 7.12) when processing block-level nodes that contain inline content. Specifically:\n\n1. For nodes like paragraphs, list items, table cells, and blockquotes, the visitor should delegate inline content processing to the Granular Inline Parser.\n\n2. Implement a method like `processInlineContent(String content)` that utilizes the inline parser to generate a token stream of inline elements (emphasis, links, code spans, etc.).\n\n3. The visitor should maintain a stack-based context that tracks not only nesting level but also the current inline parsing state when traversing mixed content.\n\n4. Add callback hooks that allow consumers to intercept both block-level nodes and the fine-grained inline tokens.\n\n5. Consider implementing a composite pattern where block visitors can contain inline visitors, allowing for specialized processing at different structural levels.\n\nThis approach ensures consistent handling of complex inline formatting across all block contexts and provides maximum flexibility for downstream processors.\n</info added on 2025-05-04T03:38:43.613Z>\n\n<info added on 2025-05-04T03:54:17.908Z>\n<info added on 2025-05-05T14:22:10.000Z>\n## Implementation Details for AST Visitor Pattern\n\n### Core Visitor Architecture\n- Implement a double-dispatch mechanism where nodes accept visitors via an `accept(visitor)` method, complementing the visitor's `visit_*` methods\n- Use Python's `getattr()` with fallback for dynamic method dispatch: `getattr(self, f\"visit_{node_type}\", self.visit_default)(node)`\n\n### Context Management\n- Implement a `VisitorContext` class to track:\n  ```python\n  class VisitorContext:\n      def __init__(self):\n          self.depth = 0\n          self.path = []  # Stack of node types representing current position\n          self.ancestors = []  # Stack of actual parent nodes\n          self.inline_state = None  # For inline parsing state\n  ```\n\n- Add context manipulation methods:\n  ```python\n  def enter_node(self, node):\n      self.context.depth += 1\n      self.context.path.append(node['type'])\n      self.context.ancestors.append(node)\n      \n  def exit_node(self, node):\n      self.context.depth -= 1\n      self.context.path.pop()\n      self.context.ancestors.pop()\n  ```\n\n### Visitor Implementation Pattern\n- For each node type, implement a pattern like:\n  ```python\n  def visit_heading_open(self, node):\n      self.enter_node(node)\n      # Process heading attributes (level, etc.)\n      result = self.process_node(node)\n      # Visit children if any\n      self.visit_children(node)\n      self.exit_node(node)\n      return result\n  ```\n\n### Testing Strategy\n- Create mock nodes for testing without requiring full parser:\n  ```python\n  def create_test_ast():\n      return [\n          {'type': 'heading_open', 'level': 1, 'children': []},\n          {'type': 'text', 'content': 'Heading text'},\n          {'type': 'heading_close'},\n          # More nodes...\n      ]\n  ```\n\n- Test context maintenance with nested structures:\n  ```python\n  def test_context_tracking():\n      visitor = TestVisitor()\n      visitor.visit(create_nested_test_ast())\n      assert visitor.max_depth == 3  # Expected max nesting\n      assert visitor.path_history == [['root'], ['root', 'list'], ['root', 'list', 'item']]\n  ```\n\n- Test visitor extension through inheritance:\n  ```python\n  class CustomVisitor(MarkdownAstVisitor):\n      def visit_code_block(self, node):\n          # Custom handling for code blocks\n          self.code_blocks.append(node['content'])\n          return super().visit_code_block(node)\n  ```\n</info added on 2025-05-05T14:22:10.000Z>\n</info added on 2025-05-04T03:54:17.908Z>"
        },
        {
          "id": 3,
          "title": "Extract text from block elements",
          "description": "Implement extraction of translatable text from block-level elements",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "Using TDD, write tests first for extracting text from block-level elements including: paragraphs, headings (h1-h6), blockquotes, and list items. Then extend the MarkdownProcessor to implement this functionality. For each element type, identify the text content, create TextSegment instances, and add them to the TranslationMap. Ensure proper handling of nested structures, especially for lists and blockquotes. Skip code blocks entirely as they are non-translatable.\n\n<info added on 2025-05-04T15:16:52.063Z>\n```typescript\n// Implementation details for extracting text from block elements\n\n// Test structure example\ndescribe('MarkdownProcessor - block element extraction', () => {\n  test('extracts text from paragraphs', () => {\n    const markdown = '## Header\\n\\nThis is a paragraph.\\n\\nThis is another paragraph.';\n    const processor = new MarkdownProcessor();\n    const result = processor.extractTranslatableSegments(markdown);\n    \n    expect(result.segments.length).toBe(3);\n    expect(result.segments[1].text).toBe('This is a paragraph.');\n    expect(result.segments[1].type).toBe('paragraph');\n  });\n  \n  test('handles nested blockquotes correctly', () => {\n    const markdown = '> Outer quote\\n> > Inner quote\\n> More outer';\n    // Assertions for proper nesting handling\n  });\n  \n  test('skips code blocks', () => {\n    const markdown = 'Text\\n\\n```js\\nconst x = 1;\\n```\\n\\nMore text';\n    // Verify only \"Text\" and \"More text\" are extracted\n  });\n});\n\n// Implementation approach\nfunction extractBlockElements($: CheerioAPI, translationMap: TranslationMap): void {\n  // Process paragraphs\n  $('p').each((_, element) => {\n    if (!isWithinCodeBlock(element)) {\n      const text = $(element).text().trim();\n      if (text) {\n        translationMap.addSegment(new TextSegment(text, 'paragraph', getElementPath(element)));\n      }\n    }\n  });\n  \n  // Process headings with level detection\n  $('h1, h2, h3, h4, h5, h6').each((_, element) => {\n    const level = element.name.substring(1); // Extract heading level (1-6)\n    const text = $(element).text().trim();\n    if (text) {\n      translationMap.addSegment(new TextSegment(text, `heading-${level}`, getElementPath(element)));\n    }\n  });\n  \n  // Helper function to check if element is within a code block\n  function isWithinCodeBlock(element: CheerioElement): boolean {\n    return $(element).parents('pre, code').length > 0;\n  }\n  \n  // Helper to generate path for element location tracking\n  function getElementPath(element: CheerioElement): string {\n    // Implementation to create a path like \"body > article > p:nth-child(3)\"\n  }\n}\n```\n</info added on 2025-05-04T15:16:52.063Z>"
        },
        {
          "id": 4,
          "title": "Extract text from inline elements",
          "description": "Implement extraction of translatable text from inline formatting elements",
          "status": "done",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Following TDD, write tests first for extracting text from inline elements including: emphasis (bold/italic), links (extract link text but not URLs), image alt text, and other inline formatting. Then implement the functionality to pass these tests. Create appropriate TextSegment instances for these elements, maintaining their relationship to parent block elements. Ensure inline code spans are preserved and not extracted for translation.\n\n<info added on 2025-05-04T03:38:54.851Z>\nFor this subtask, I'll add information about processing the Granular Inline Parser output and handling WikiLink aliases:\n\nThe implementation should now consume the structured output from the Granular Inline Parser (7.12) rather than parsing inline elements directly. Create a mapping system that tracks the relationship between original inline elements and their extracted TextSegment instances to facilitate accurate reassembly later.\n\nWhen processing inline elements:\n- For WikiLink aliases (e.g., `[[Page|Displayed Text]]`), extract only the display text portion (\"Displayed Text\") as translatable\n- Maintain metadata about each inline element's type and position in the document structure\n- Implement special handling for nested inline elements (e.g., bold text within links)\n- Create a clear separation between translatable content and non-translatable syntax elements like Obsidian-specific attributes\n- Design the extraction to be reversible, ensuring translated content can be correctly reinserted into the original inline structure\n\nUpdate dependencies to include subtask 7.12 (Granular Inline Parser) as this subtask now processes its output rather than performing its own parsing.\n</info added on 2025-05-04T03:38:54.851Z>\n\n<info added on 2025-05-04T15:47:15.412Z>\nFor this subtask implementation, I'll add specific technical details about handling inline elements:\n\nWhen implementing the `_extract_inline_text` method:\n\n```python\ndef _extract_inline_text(self, inline_tokens, parent_segment=None):\n    \"\"\"Process inline tokens to extract translatable text segments.\"\"\"\n    segments = []\n    current_text = \"\"\n    \n    for token in inline_tokens:\n        if token.type == 'text':\n            current_text += token.content\n        elif token.type in ('em_open', 'strong_open', 'link_open'):\n            # Process nested content within emphasis/links\n            nested_segments = self._extract_inline_text(token.children, parent_segment)\n            segments.extend(nested_segments)\n        elif token.type == 'image':\n            # Extract alt text if present\n            if token.attrs.get('alt'):\n                alt_segment = TextSegment(\n                    text=token.attrs['alt'],\n                    segment_type=SegmentType.INLINE_ALT_TEXT,\n                    parent=parent_segment\n                )\n                segments.append(alt_segment)\n        elif token.type == 'code_inline':\n            # Skip code spans - they shouldn't be translated\n            pass\n    \n    # Create segment for accumulated text if any\n    if current_text:\n        text_segment = TextSegment(\n            text=current_text,\n            segment_type=SegmentType.INLINE_TEXT,\n            parent=parent_segment\n        )\n        segments.append(text_segment)\n    \n    return segments\n```\n\nKey implementation considerations:\n- Store position metadata (start/end indices) with each segment to enable accurate reassembly\n- Create a bidirectional mapping between original markdown and extracted segments using a dictionary structure\n- For nested elements, maintain proper hierarchy using parent references\n- Implement special handling for markdown-it-py's token structure, particularly for nested tokens\n- Use custom segment types (enum values) to differentiate between regular text, link text, and alt text\n- Consider implementing a visitor pattern for cleaner token processing logic\n- Add context attributes to segments to preserve formatting information needed during reassembly\n\nWhen consuming Granular Inline Parser output, implement a transformation layer that converts the parser's AST into TextSegment instances while preserving the hierarchical relationships.\n</info added on 2025-05-04T15:47:15.412Z>"
        },
        {
          "id": 5,
          "title": "Implement table content extraction",
          "description": "Add support for extracting text from table headers and cells",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "Using TDD, write tests first for extracting text from tables, including both header cells and body cells. Then extend the MarkdownProcessor to implement this functionality. Create TextSegment instances for each cell's content, maintaining information about the table structure (row/column position). Handle any inline formatting within table cells by leveraging the inline extraction logic. Ensure table structure metadata is preserved for reassembly.\n\n<info added on 2025-05-04T15:57:32.076Z>\n```\nImplementation Plan (Subtask 7.5 - Table Extraction):\n1. **TDD - Write Tests (`tests/processing/test_markdown_processor.py`):**\n    - Test cases for simple tables (headers `<th>`, data cells `<td>`).\n    - Include tests with inline formatting within cells.\n    - Verify empty cells produce no segments.\n    - Check segment types: `table_header_cell`, `table_data_cell`.\n    - Attempt to include basic row/column info in the path (e.g., `table_1 > tr_2 > td_3`).\n2. **Implement Extraction (`src/processing/markdown_processor.py`):**\n    - Enhance `extract_translatable_segments` to detect table tokens (`table_open/close`, `thead_open/close`, `tbody_open/close`, `tr_open/close`, `th_open/close`, `td_open/close`).\n    - Maintain context (current row/cell index) while inside a table.\n    - For `th_open`/`td_open`, locate the next `inline` token.\n    - Extract text using `_extract_inline_text` from the `inline` token's children.\n    - Create `TextSegment` with correct type and path including row/col info.\n    - Add segments to `TranslationMap`.\n    - Ensure correct `token_stack` management for table elements.\n3. **Test & Refine:** Run `pytest` and iterate on implementation.\n4. **Log Progress:** Update subtask details if needed.\n\n**Technical Considerations:**\n- Table structure in markdown-it-py tokens follows HTML structure: `table > thead/tbody > tr > th/td > inline > text/code_inline/etc`\n- Track table context with variables like `current_table_id`, `current_row`, `current_cell`\n- For complex tables, consider creating a `TableContext` class to manage state\n- Store cell coordinates in segment metadata: `{\"row\": row_idx, \"col\": col_idx, \"is_header\": bool}`\n- Handle merged cells by checking for `colspan` and `rowspan` attributes in tokens\n- For reassembly, maintain original token indices to preserve exact table structure\n```\n</info added on 2025-05-04T15:57:32.076Z>"
        },
        {
          "id": 6,
          "title": "Create segment reassembly functionality",
          "description": "Implement functionality to replace translated text back into the original AST",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Following TDD principles, write tests first for reassembling translated content back into the original document. Then implement methods in the MarkdownProcessor to take translated TextSegments and update the original AST with the translated content. Implement a reassembly algorithm that uses the stored node references and position information to correctly place translated text while preserving all non-translatable elements and document structure. Add validation to ensure all segments are accounted for and the document structure remains intact.\n\n<info added on 2025-05-04T03:39:04.859Z>\nThe reassembly process must accurately reconstruct complex inline sequences by leveraging the detailed mapping created in task 7.1 and the structured output from the Granular Inline Parser (7.12). When reassembling, the algorithm should:\n\n1. Use position markers and node references to precisely interleave translated text segments with preserved non-translatable syntax elements\n2. Maintain the integrity of inline code blocks, WikiLink targets, HTML tag attributes, URLs, and other elements identified as non-translatable in task 7.13\n3. Implement a verification step that confirms all inline elements are correctly positioned relative to each other after reassembly\n4. Handle nested structures (like formatting within links, or links within formatting) by respecting the hierarchy established in the original AST\n5. Preserve whitespace patterns between inline elements to maintain document formatting\n\nConsider implementing a two-phase reassembly approach: first reconstruct inline sequences at the leaf nodes, then rebuild the document hierarchy from bottom up. This ensures complex inline formatting is preserved while maintaining the overall document structure.\n\nDependencies should be updated to include tasks 7.12 and 7.13, as the reassembly logic depends on both the granular parsing of inline content and the non-translatable element preservation strategy.\n</info added on 2025-05-04T03:39:04.859Z>\n\n<info added on 2025-05-04T15:59:42.340Z>\n<info added on 2025-05-05T14:22:18.000Z>\n## Implementation Details for Segment Reassembly\n\n### Core Data Structures\n1. Create a `ReassemblyContext` class to track state during reassembly:\n   ```python\n   class ReassemblyContext:\n       def __init__(self):\n           self.path_stack = []\n           self.current_path = \"\"\n           self.in_table = False\n           self.row_index = 0\n           self.col_index = 0\n           self.pending_tokens = []  # For buffering tokens during reassembly\n   ```\n\n2. Implement a `SegmentMap` class to efficiently lookup translated segments:\n   ```python\n   class SegmentMap:\n       def __init__(self, translated_segments: Dict[str, str]):\n           self.segments = translated_segments\n           self.used_segments = set()  # Track which segments were used\n           \n       def get(self, path: str) -> Optional[str]:\n           if path in self.segments:\n               self.used_segments.add(path)\n               return self.segments[path]\n           return None\n           \n       def verify_all_used(self) -> List[str]:\n           \"\"\"Returns list of unused segment paths\"\"\"\n           return [p for p in self.segments if p not in self.used_segments]\n   ```\n\n### Algorithm Implementation\nThe reassembly algorithm should follow these steps:\n\n1. **Token Stream Processing**:\n   ```python\n   def reassemble_markdown(self, original_markdown: str, translated_segments: Dict[str, str]) -> str:\n       tokens = self.md.parse(original_markdown)\n       segment_map = SegmentMap(translated_segments)\n       context = ReassemblyContext()\n       result = []\n       \n       for i, token in enumerate(tokens):\n           # Update context based on token\n           self._update_reassembly_context(token, context)\n           \n           # Handle token based on type\n           if self._is_translatable_content_token(token):\n               translated = self._process_translatable_token(token, context, segment_map)\n               result.append(translated)\n           else:\n               # Non-translatable token, preserve as-is\n               result.append(self._render_token(token))\n               \n       # Verify all segments were used\n       unused = segment_map.verify_all_used()\n       if unused:\n           self.logger.warning(f\"Unused segments during reassembly: {unused}\")\n           \n       return ''.join(result)\n   ```\n\n2. **Inline Content Handling**:\n   ```python\n   def _process_translatable_token(self, token, context, segment_map):\n       if token.type == 'inline':\n           # Get translated content for this path\n           translated_text = segment_map.get(context.current_path)\n           if translated_text:\n               # Create a modified token with translated content\n               return self._reassemble_inline_content(token, translated_text)\n           \n       return self._render_token(token)\n   ```\n\n3. **Inline Reassembly**:\n   ```python\n   def _reassemble_inline_content(self, token, translated_text):\n       # For simple cases, just replace the content\n       # For complex inline sequences (to be enhanced in 7.12/7.13),\n       # this will need to parse the translated text and merge with\n       # non-translatable elements from the original\n       \n       # Simple implementation for now:\n       token_copy = copy.deepcopy(token)\n       token_copy.content = translated_text\n       return self.md.renderer.renderToken([token_copy], 0, {})\n   ```\n\n### Validation & Error Handling\n1. Implement validation to ensure document integrity:\n   ```python\n   def _validate_reassembled_document(self, reassembled_markdown):\n       \"\"\"Verify the reassembled document is valid markdown\"\"\"\n       try:\n           tokens = self.md.parse(reassembled_markdown)\n           # Additional structural validation can be added here\n           return True\n       except Exception as e:\n           self.logger.error(f\"Reassembly validation failed: {str(e)}\")\n           return False\n   ```\n\n2. Add error recovery for missing segments:\n   ```python\n   # In _process_translatable_token:\n   translated_text = segment_map.get(context.current_path)\n   if not translated_text:\n       self.logger.warning(f\"Missing translation for segment: {context.current_path}\")\n       # Fall back to original content\n       translated_text = token.content\n   ```\n\nThis implementation provides a solid foundation for basic reassembly while setting up the structure for the more complex inline handling that will be implemented in tasks 7.12 and 7.13.\n</info added on 2025-05-05T14:22:18.000Z>\n</info added on 2025-05-04T15:59:42.340Z>\n\n<info added on 2025-05-04T16:09:34.090Z>\n## Reassembly Implementation Challenges and Strategy\n\nThe reassembly task presents several technical challenges that justify its deferred status:\n\n1. **Source Mapping Limitations**: \n   - Markdown-it-py's token system doesn't maintain perfect bidirectional mapping between source text and tokens\n   - Position information is often approximate, especially with nested inline elements\n   - Whitespace handling differs between parsing and rendering phases\n\n2. **Alternative Implementation Approaches**:\n   - **AST Transformation**: Instead of string replacement, consider working directly with the AST, replacing content at the node level before rendering\n   - **Token Stream Modification**: Implement a token stream processor that replaces content in-place before rendering\n   - **Custom Renderer**: Develop a Markdown-specific renderer (not HTML) that reconstructs Markdown syntax from modified tokens\n\n3. **Error Recovery Strategy**:\n   - Implement fallback mechanisms for when segment boundaries don't align perfectly\n   - Create a validation system that can detect and report potential structure corruption\n   - Consider a diff-based approach to identify and resolve structural inconsistencies\n\n4. **Testing Framework**:\n   - Develop specialized test fixtures for complex nested structures\n   - Create property-based tests that verify structural invariants are maintained\n   - Implement round-trip testing (original  segmented  translated  reassembled  parsed) to verify equivalence\n\nThis task should be revisited after the Granular Inline Parser is complete, as it will provide the necessary foundation for accurate segment boundary tracking and reconstruction.\n</info added on 2025-05-04T16:09:34.090Z>\n\n<info added on 2025-05-04T17:31:05.417Z>\n<info added on 2025-05-06T09:14:23.000Z>\n## Implementation Update: Block-Level Reassembly Strategy\n\nThe initial implementation of `reassemble_markdown` has been completed using Strategy A+ with the following approach:\n\n```python\ndef reassemble_markdown(self, original_ast, translated_segments):\n    \"\"\"Replace content in the original AST with translated segments.\"\"\"\n    for path, translation in translated_segments.items():\n        # Find the target token using the path\n        target_token = self._find_token_by_path(original_ast, path)\n        if not target_token or target_token.type != 'inline':\n            self.logger.warning(f\"Could not find valid inline token at path: {path}\")\n            continue\n            \n        # Replace content in the first child text token and remove subsequent children\n        if target_token.children and len(target_token.children) > 0:\n            # Find first text child\n            for i, child in enumerate(target_token.children):\n                if child.type == 'text':\n                    # Replace content of first text token with full translation\n                    child.content = translation\n                    # Remove all subsequent children to prevent duplication\n                    target_token.children = target_token.children[:i+1]\n                    break\n    \n    # Render the modified AST back to markdown\n    return self.renderer.render(original_ast)\n```\n\nKey implementation details:\n\n1. **Path Resolution Fix**: Resolved inconsistencies between path generation during extraction and reassembly by:\n   - Simplifying stack management in the extraction phase\n   - Ensuring `get_element_path()` uses persistent counters across function calls\n   - Adding path normalization to handle edge cases with list items and nested blocks\n\n2. **Token Replacement Strategy**:\n   - Locates the target inline token based on the stored path\n   - Replaces only the content of the first child 'text' token with the full translation\n   - Removes subsequent children within that inline token to prevent duplication\n   - This approach preserves block-level structure while treating inline content as atomic units\n\n3. **Error Handling**:\n   - Added validation to detect missing tokens during reassembly\n   - Implemented logging for path resolution failures\n   - Added recovery mechanism to skip problematic segments rather than failing completely\n\nAll tests for block-level reassembly now pass successfully. The more complex task of preserving internal formatting within inline elements (like bold/italic within translated text) has been explicitly deferred to Subtask 7.11, which will implement the fine-grained inline reassembly using the Granular Inline Parser.\n</info added on 2025-05-06T09:14:23.000Z>\n</info added on 2025-05-04T17:31:05.417Z>"
        },
        {
          "id": 7,
          "title": "Implement WikiLink extraction",
          "description": "Add support for extracting translatable text from WikiLinks",
          "status": "done",
          "dependencies": [
            1,
            2,
            4
          ],
          "details": "Using TDD, write tests first for identifying and extracting translatable text from WikiLinks (e.g., the alias portion in `[[target|alias]]`). Implement a parser or extend the existing inline parser to correctly identify WikiLink components. Create TextSegment instances for the translatable portions (aliases) while preserving the non-translatable targets. Ensure the mapping maintains the relationship between extracted text and its precise location within the WikiLink syntax for accurate reassembly.\n\n<info added on 2025-05-04T16:16:19.494Z>\nHere's additional implementation information for the WikiLink extraction subtask:\n\n```python\n# Example WikiLink parser implementation structure\ndef wikilinks_plugin(md):\n    # Regular expression for matching WikiLinks\n    WIKILINK_RE = r'\\[\\[([^\\]\\|]+)(?:\\|([^\\]]+))?\\]\\]'\n    \n    def tokenize(state, silent):\n        # Match WikiLink pattern\n        match = re.search(WIKILINK_RE, state.src[state.pos:])\n        if not match:\n            return False\n            \n        # Skip if in code block\n        if state.isInCode:\n            return False\n            \n        # Extract target and alias\n        target = match.group(1).strip()\n        alias = match.group(2).strip() if match.group(2) else target\n        \n        # Create token sequence\n        token_open = state.push('wikilink_open', '', 1)\n        \n        token_target = state.push('wikilink_target', '', 0)\n        token_target.content = target\n        token_target.translatable = False  # Mark as non-translatable\n        \n        if match.group(2):  # If alias exists\n            token_sep = state.push('wikilink_separator', '|', 0)\n            token_sep.translatable = False\n            \n            token_alias = state.push('wikilink_alias', '', 0)\n            token_alias.content = alias\n            token_alias.translatable = True  # Mark as translatable\n        \n        token_close = state.push('wikilink_close', '', -1)\n        \n        # Update parser position\n        state.pos += match.end()\n        return True\n    \n    md.inline.ruler.push('wikilink', tokenize)\n```\n\nFor the text segment creation logic:\n\n```python\ndef _extract_inline_text(self, tokens):\n    segments = []\n    for token in tokens:\n        if token.type == 'wikilink_alias' and hasattr(token, 'translatable') and token.translatable:\n            # Create a TextSegment for the alias portion\n            segment = TextSegment(\n                text=token.content,\n                context=f\"WikiLink alias for target: {self._get_wikilink_target(token)}\",\n                metadata={\n                    \"type\": \"wikilink_alias\",\n                    \"target\": self._get_wikilink_target(token)\n                }\n            )\n            segments.append(segment)\n    return segments\n\ndef _get_wikilink_target(self, alias_token):\n    \"\"\"Helper to find the target associated with an alias token\"\"\"\n    # Implementation depends on token structure, but generally:\n    # Look backward in the token stream to find the wikilink_target token\n    # that precedes this alias token\n    pass\n```\n\nWhen reassembling translated content, ensure the WikiLink structure is preserved:\n\n```python\ndef _reassemble_wikilink(self, target, translated_alias):\n    \"\"\"Reconstruct a WikiLink with the translated alias\"\"\"\n    return f\"[[{target}|{translated_alias}]]\"\n```\n\nConsider edge cases:\n- WikiLinks with no alias (`[[target]]`) - decide whether to extract the target as translatable\n- Nested formatting within aliases (`[[target|**bold** text]]`) - handle markdown within aliases\n- Case sensitivity in targets - preserve original casing for targets\n</info added on 2025-05-04T16:16:19.494Z>"
        },
        {
          "id": 8,
          "title": "Implement Obsidian attributes handling",
          "description": "Add support for preserving Obsidian attributes during extraction",
          "status": "done",
          "dependencies": [
            1,
            2,
            4
          ],
          "details": "Following TDD principles, write tests first for correctly identifying and preserving Obsidian attributes (e.g., `{ .some-attr }`) during text extraction. Implement functionality to recognize these attributes as non-translatable elements and ensure they are properly preserved during both extraction and reassembly processes. Update the visitor or parser to handle these special syntax elements.\n\n<info added on 2025-05-04T16:20:23.933Z>\n## Implementation Details for Obsidian Attributes Handling\n\n### Regex Pattern Design\n- Use a pattern like `/{([^{}]|\\{[^{}]*\\})*}/` to handle basic nesting\n- Consider edge cases: escaped braces `\\{`, inline code with braces, and attributes within other inline elements\n\n### Attribute Structure Parsing\n- Parse attributes into structured data:\n  ```python\n  def parse_attributes(attr_content):\n      \"\"\"Parse Obsidian attribute content into structured data\"\"\"\n      attributes = {}\n      # Handle class attributes (.class-name)\n      class_match = re.findall(r'\\.([a-zA-Z0-9_-]+)', attr_content)\n      if class_match:\n          attributes['class'] = class_match\n          \n      # Handle key-value pairs (key=value or key=\"quoted value\")\n      kv_matches = re.findall(r'([a-zA-Z0-9_-]+)=(?:\"([^\"]*)\"|([^ }]*))', attr_content)\n      for key, quoted_val, unquoted_val in kv_matches:\n          attributes[key] = quoted_val if quoted_val else unquoted_val\n          \n      return attributes\n  ```\n\n### Token Handling Strategy\n- Create a token type that preserves the entire attribute block as non-translatable\n- Store original attribute text in token metadata for exact reconstruction\n- Consider implementing a `reconstruct_attributes()` function to ensure proper reassembly\n\n### Integration with Markdown-It\n- Position the rule carefully in the parsing chain (before emphasis but after code_inline)\n- Use `state.push()` with appropriate nesting levels\n- Set `token.markup = '{}'` for proper syntax highlighting in debug views\n\n### Testing Edge Cases\n- Test attributes on headings: `## Heading {.class}`\n- Test attributes on links: `[link](url){.class}`\n- Test attributes with special characters: `{.class-name data-test=\"value with spaces\"}`\n- Test malformed attributes: `{incomplete` or `text { not closed properly`\n</info added on 2025-05-04T16:20:23.933Z>\n\n<info added on 2025-05-04T16:38:38.284Z>\n## Implementation Approach for Obsidian Attributes\n\n### Visitor Pattern Enhancement\n- Extend the `MarkdownVisitor` class to recognize attribute tokens:\n  ```python\n  def visit_attribute(self, token, children):\n      # Skip attribute tokens entirely during extraction\n      # They should be preserved verbatim in the original document\n      return None\n  ```\n\n### Attribute Position Tracking\n- Implement position tracking to handle attributes attached to different elements:\n  ```python\n  class AttributeTracker:\n      def __init__(self):\n          self.attached_elements = {}  # Maps element_id to attribute data\n          \n      def register_attribute(self, element_id, attribute_data):\n          self.attached_elements[element_id] = attribute_data\n          \n      def get_attributes_for(self, element_id):\n          return self.attached_elements.get(element_id, None)\n  ```\n\n### Reassembly Considerations\n- During reassembly, attributes must be reattached to the correct elements:\n  ```python\n  def reassemble_with_attributes(self, element, translated_text):\n      element_id = element.get('id')\n      attributes = self.attribute_tracker.get_attributes_for(element_id)\n      \n      if attributes:\n          # Handle different attachment positions based on element type\n          if element.get('type') == 'block':\n              return f\"{translated_text} {attributes}\"\n          elif element.get('type') == 'inline':\n              # For inline elements, attributes typically follow immediately\n              return f\"{translated_text}{attributes}\"\n      return translated_text\n  ```\n\n### Performance Optimization\n- Cache compiled regex patterns for attribute detection\n- Use a two-pass approach: first identify all attributes, then process content\n- Consider implementing a lookup table for common attribute patterns\n\n### Compatibility Testing\n- Test with various Obsidian plugins that extend attribute functionality\n- Verify compatibility with CommonMark and GitHub Flavored Markdown\n- Create test cases for attributes in multilingual documents\n</info added on 2025-05-04T16:38:38.284Z>"
        },
        {
          "id": 9,
          "title": "Implement HTML content extraction",
          "description": "Add support for handling HTML tags and extracting translatable content from within specified tags",
          "status": "done",
          "dependencies": [
            1,
            2,
            4
          ],
          "details": "Using TDD, write tests first for identifying HTML tags within Markdown content and extracting translatable text from within specified tags. Implement functionality to parse HTML content, determine which parts should be translated based on configuration, and create appropriate TextSegment instances. Ensure HTML tags themselves are preserved as non-translatable elements. Add configuration options to specify which HTML tags should have their content extracted for translation.\n\n<info added on 2025-05-04T17:00:48.988Z>\n## Implementation Strategy for HTML Content Extraction\n\n### HTML Parsing Approach\n- Use Python's `html.parser.HTMLParser` or a library like `BeautifulSoup4` to properly parse HTML content within Markdown\n- Create a custom HTML handler that can distinguish between tags that should be preserved vs. those whose content should be extracted\n\n### Configuration Structure\n```python\nhtml_config = {\n    \"extract_content_from_tags\": [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"td\", \"th\", \"figcaption\"],\n    \"preserve_tags\": [\"code\", \"pre\", \"script\", \"style\"],\n    \"attribute_handling\": {\n        \"alt\": [\"img\"],  # Extract text from alt attributes of img tags\n        \"title\": [\"a\", \"img\"]  # Extract text from title attributes\n    }\n}\n```\n\n### Processing Algorithm\n1. When encountering HTML content, parse it into a DOM-like structure\n2. Traverse the structure recursively:\n   - For tags in `extract_content_from_tags`, create TextSegments for their content\n   - For tags in `preserve_tags`, skip their content entirely\n   - For specified attributes in `attribute_handling`, extract their values as separate TextSegments\n\n### Edge Cases to Handle\n- Nested HTML tags with different extraction rules\n- Malformed HTML that might break the parser\n- HTML entities that need proper decoding before translation\n- Inline HTML mixed with Markdown formatting\n\n### Testing Considerations\n- Create test cases with complex nested HTML structures\n- Test HTML with various attributes and entity encodings\n- Verify correct handling of self-closing tags like `<img>` and `<br>`\n</info added on 2025-05-04T17:00:48.988Z>\n\n<info added on 2025-05-04T17:14:07.181Z>\n<info added on 2025-05-05T09:15:22.456Z>\n## Current Implementation Status and Limitations\n\n### Implemented Features\n- Basic detection and skipping of HTML block elements (`html_block`) in the Markdown AST\n- Identification and skipping of inline HTML tags (`html_inline`) during text extraction\n- Configuration option to control HTML handling behavior\n\n### Known Limitations\n- Text content within inline HTML tags (e.g., `<span>text</span>`) is currently extracted as part of the surrounding text block\n- No differentiation yet between translatable and non-translatable HTML tags\n- HTML attributes (like `alt` and `title`) aren't separately extracted\n\n### Next Implementation Steps\n1. Integrate proper HTML parsing using BeautifulSoup4\n2. Implement the tag-specific extraction rules from the configuration\n3. Add HTML attribute extraction according to the `attribute_handling` config\n4. Create specialized TextSegment subclass for HTML content that preserves tag structure\n\n### Example of Current vs. Target Behavior\n\n**Current behavior with this markdown:**\n```markdown\nSome text with <span class=\"highlight\">highlighted content</span> and more text.\n```\n\nCurrently extracts as: `\"Some text with highlighted content and more text.\"`\n\n**Target behavior:**\n```python\n[\n    TextSegment(\"Some text with \"),\n    HTMLSegment(\"<span class=\\\"highlight\\\">\", \"highlighted content\", \"</span>\"),\n    TextSegment(\" and more text.\")\n]\n```\n\nThis will be addressed in the upcoming implementation phases.\n</info added on 2025-05-05T09:15:22.456Z>\n</info added on 2025-05-04T17:14:07.181Z>"
        },
        {
          "id": 10,
          "title": "Enhance inline sequence parsing",
          "description": "Implement a secondary inline parser for complex mixed content",
          "status": "done",
          "dependencies": [
            1,
            2,
            4,
            7,
            8,
            9
          ],
          "details": "Following TDD principles, write tests first for handling complex mixed inline sequences containing combinations of regular text, WikiLinks, attributes, HTML, and standard Markdown formatting. Implement a secondary parsing stage that can accurately identify and extract only the translatable portions while maintaining precise position information. Ensure the parser can handle nested structures and edge cases. Update the mapping data structure to accommodate these complex inline sequences for perfect reassembly.\n\n<info added on 2025-05-04T17:23:57.000Z>\nThe refactored approach uses the token stream from `markdown-it-py` directly, which provides better context and position tracking. Key implementation details:\n\n1. Modified `_extract_inline_text` to traverse the token tree and identify translatable segments while preserving their original positions.\n\n2. Added token type handlers:\n   - Links: Extracts text content but preserves URLs\n   - Images: Captures alt text only\n   - WikiLinks: Processes only the display portion (alias)\n   - Code blocks, HTML, and attribute markers: Explicitly skipped\n\n3. Position tracking now uses token metadata to maintain accurate source mapping, which solves the reconstruction issues we faced with the separate parser approach.\n\n4. Added helper methods to determine token translatability based on both type and context.\n\n5. Implemented a more robust text reassembly algorithm that uses the position information to correctly place translated content back into the original structure.\n\nThis approach significantly reduces complexity while maintaining the ability to handle nested structures and edge cases.\n</info added on 2025-05-04T17:23:57.000Z>"
        },
        {
          "id": 11,
          "title": "Update reassembly for complex inline sequences",
          "description": "Enhance the reassembly functionality to handle complex inline sequences",
          "status": "done",
          "dependencies": [
            6,
            7,
            8,
            9,
            10
          ],
          "details": "Using TDD, write tests first for reassembling translated content back into complex inline sequences. Extend the existing reassembly functionality to handle the more complex mapping created for WikiLinks, Obsidian attributes, HTML content, and mixed inline sequences. Implement algorithms that can precisely reconstruct these elements with translated content while preserving all non-translatable syntax. Add validation to ensure perfect reconstruction of the original format with only the translatable text replaced.\n\n<info added on 2025-05-04T17:32:19.375Z>\nThe simplified Strategy B approach makes sense as an initial implementation, but we should document its limitations and future improvements:\n\nFor reassembly of complex inline sequences, the current implementation replaces entire inline token children with a single text token containing the translated content. This approach:\n\n1. Preserves document structure at block level (paragraphs, headers, lists)\n2. Maintains correct placement of translated content\n3. Trades off preservation of inline formatting for implementation simplicity\n\nKnown limitations:\n- Internal formatting (bold, italic, links) within translated text is lost\n- WikiLinks, attributes, and HTML tags are treated as opaque blocks\n\nFuture enhancement path:\n- Implement token-level mapping between source and translated content\n- Store formatting metadata during extraction phase\n- Use alignment algorithms to reapply formatting to translated text\n- Consider implementing format-preserving translation markers that survive the translation process\n- Add unit tests specifically for format preservation scenarios\n\nThis simplified approach provides a working solution while deferring the more complex token-level formatting preservation to a future iteration.\n</info added on 2025-05-04T17:32:19.375Z>"
        },
        {
          "id": 12,
          "title": "Implement Granular Inline Parser",
          "description": "Develop a secondary parser specifically for inline content within blocks (paragraphs, list items, etc.).",
          "details": "Using TDD, implement a parser that tokenizes inline content into distinct units: plain text, standard Markdown (bold, italic), inline code, standard links, images, WikiLinks (target and alias), Obsidian attributes (`{...}`). It should handle nested structures and produce a detailed token stream or mini-AST for each inline sequence, preserving non-translatable syntax precisely. This parser will be used by the main AST visitor.\n\n<info added on 2025-05-04T16:10:09.980Z>\n# Implementation Plan (Subtask 7.12 - Granular Inline Parser)\n\n## Token Types and Structure\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Dict, Any\n\nclass InlineTokenType(Enum):\n    TEXT = auto()\n    EMPHASIS = auto()  # *italic*\n    STRONG = auto()    # **bold**\n    CODE = auto()      # `code`\n    LINK = auto()      # [text](url)\n    IMAGE = auto()     # ![alt](src)\n    WIKILINK = auto()  # [[target]] or [[target|alias]]\n    ATTRIBUTE = auto() # {key: value}\n\n@dataclass\nclass InlineToken:\n    type: InlineTokenType\n    content: str  # Processed content\n    raw: str      # Original raw text\n    attrs: Dict[str, Any] = None  # Additional attributes\n    children: List['InlineToken'] = None  # For nested tokens\n```\n\n## Parser Implementation Strategy\nThe parser should use a state machine approach with these states:\n- NORMAL: Processing regular text\n- EMPHASIS: Inside `*` markers\n- STRONG: Inside `**` markers\n- CODE: Inside backticks\n- LINK_TEXT: Inside `[...]`\n- LINK_URL: Inside `(...)`\n- WIKILINK: Inside `[[...]]`\n- ATTRIBUTE: Inside `{...}`\n\nKey implementation considerations:\n- Use a stack to track nested formatting states\n- Handle escaped characters (`\\*`, `\\[`, etc.)\n- Process WikiLinks with regex like `\\[\\[(.*?)(?:\\|(.*?))?\\]\\]`\n- For attributes, parse JSON-like syntax while preserving original format\n\n## Example Test Cases\n```python\ndef test_mixed_formatting():\n    parser = InlineParser()\n    text = \"This is **bold with *italic* inside** and `code with **ignored** formatting`\"\n    tokens = parser.parse(text)\n    \n    assert len(tokens) == 3\n    assert tokens[0].type == InlineTokenType.TEXT\n    assert tokens[0].content == \"This is \"\n    \n    assert tokens[1].type == InlineTokenType.STRONG\n    assert tokens[1].content == \"bold with italic inside\"\n    assert len(tokens[1].children) == 3  # Text, Emphasis, Text\n    assert tokens[1].children[1].type == InlineTokenType.EMPHASIS\n    \n    assert tokens[2].type == InlineTokenType.CODE\n    assert tokens[2].content == \"code with **ignored** formatting\"\n```\n\n## Integration with AST Visitor\nWhen integrating with the main AST visitor:\n- Pass block content to `InlineParser.parse()`\n- Use visitor pattern to process inline tokens\n- Preserve raw content for non-translatable elements\n- Map token types to appropriate HTML/output elements\n</info added on 2025-05-04T16:10:09.980Z>\n\n<info added on 2025-05-04T17:21:20.708Z>\n<info added on 2025-05-05T09:15:22.103Z>\n# Implementation Progress Update\n\n## Current Implementation Status\nThe basic `InlineParser` has been implemented with the following components:\n\n```python\nclass InlineParser:\n    def __init__(self):\n        self.tokens = []\n        self.state_stack = []\n        self.current_token = None\n        self.buffer = \"\"\n        \n    def parse(self, text):\n        # Main parsing logic\n        # Returns list of InlineToken objects\n```\n\n## Key Features Implemented\n- State machine transitions for basic formatting elements\n- Stack-based approach for handling nested formatting\n- Regex-based extraction for WikiLinks with pattern `\\[\\[(.*?)(?:\\|(.*?))?\\]\\]`\n- Simple attribute parsing with `{key: value}` format support\n- Character-by-character processing with lookahead for ambiguous syntax\n\n## Current Limitations and Next Steps\n1. **Complex Nesting**: Need to improve handling of deeply nested structures like `***bold italic***` or `**_italic bold_**`\n2. **Reference Links**: Not yet supporting Markdown reference-style links `[text][ref]`\n3. **HTML Entities**: Need to properly handle HTML entities like `&amp;`, `&#39;`, etc.\n4. **Performance Optimization**: Current character-by-character approach may be inefficient for large documents\n5. **Attribute Validation**: Need more robust parsing of complex attribute structures\n\n## Example Usage\n```python\nfrom processing.inline_parser import InlineParser\n\nparser = InlineParser()\ntokens = parser.parse(\"Check out [[Project X|this project]] with **important** details\")\n\n# Access parsed tokens\nfor token in tokens:\n    if token.type == InlineTokenType.WIKILINK:\n        target = token.attrs.get(\"target\")\n        alias = token.attrs.get(\"alias\")\n        # Process WikiLink...\n```\n\n## Test Coverage\nCurrent test suite covers:\n- Basic formatting (emphasis, strong, code)\n- Simple nesting (bold with italic inside)\n- WikiLinks with and without aliases\n- Attribute parsing for simple key-value pairs\n- Escaped character handling\n\nAdditional tests needed for edge cases and complex combinations.\n</info added on 2025-05-05T09:15:22.103Z>\n</info added on 2025-05-04T17:21:20.708Z>",
          "status": "done",
          "dependencies": [
            "7.1"
          ],
          "parentTaskId": 7
        },
        {
          "id": 13,
          "title": "Define and Implement HTML Handling Strategy",
          "description": "Define and implement the strategy for handling raw HTML tags within Markdown content.",
          "details": "Decide on the HTML handling strategy (e.g., ignore all HTML, translate content of specific tags, preserve all HTML). Implement the chosen strategy within the text extraction and reassembly logic. If extracting content, ensure the mapping correctly identifies text originating from HTML. Add configuration options if the strategy needs to be user-adjustable. Use TDD.\n\n<info added on 2025-05-04T17:35:24.500Z>\nBased on the confirmed strategy, we'll implement a simple 'skip HTML' approach with the following implementation details:\n\n1. For `html_block` tokens: Completely ignore these blocks during text extraction, treating them as non-translatable content.\n\n2. For `html_inline` tokens: Skip the HTML tags themselves but extract any inner text content if it exists between tags.\n\n3. Implementation approach:\n   - In the token processor, add specific handlers for `html_block` and `html_inline` token types\n   - For `html_block`, return an empty string or None to exclude it from extraction\n   - For `html_inline`, use regex pattern like `r'>([^<]+)<'` to extract only text between tags if needed\n\n4. Add a configuration flag `skip_html` (default: True) to allow users to toggle this behavior.\n\n5. Document this limitation clearly in the README, noting that HTML content will not be translated in the current implementation.\n\n6. Add unit tests specifically for HTML handling scenarios:\n   - Test with inline HTML like `This is <span>important</span> text`\n   - Test with HTML blocks like `<div>Block content</div>`\n   - Test with nested HTML structures\n\nThis approach prioritizes simplicity while acknowledging the more robust solution will come in task #21.\n</info added on 2025-05-04T17:35:24.500Z>",
          "status": "done",
          "dependencies": [
            "7.1"
          ],
          "parentTaskId": 7
        },
        {
          "id": 14,
          "title": "Fix Failing Utility Tests (Hashing & Normalization)",
          "description": "Address the 5 failing tests identified in `translation-py/tests/utils/test_hashing.py` and `translation-py/tests/utils/test_normalization.py`. These tests cover edge cases for YAML hashing (None input, invalid types, exclude fields, non-string keys) and Markdown blank line normalization.",
          "details": "",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        }
      ]
    },
    {
      "id": 8,
      "title": "Implement TranslationService with DeepL integration",
      "description": "Create a service to send text to the DeepL API for translation.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "Implement a TranslationService class that sends extracted text to the DeepL API. Use the API key from configuration. Handle API response parsing, error handling, and retries. Include support for multiple target languages as specified in configuration.",
      "testStrategy": "Create mock API responses to test successful translations and error handling. If possible, test with actual DeepL API using small text samples. Verify correct handling of multiple target languages.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create TranslationService class structure and configuration",
          "description": "Set up the basic class structure for TranslationService and implement configuration loading for the DeepL API key and supported languages",
          "status": "done",
          "dependencies": [],
          "details": "Create a TranslationService class with constructor that loads configuration. Implement methods to retrieve the API key from configuration and load the list of supported target languages. Add appropriate interfaces and dependency injection support. Include configuration validation to ensure required settings are present.\n\n<info added on 2025-05-04T17:38:53.434Z>\n## Implementation Details\n\n### TranslationService Class Structure\n```python\nfrom typing import List, Dict, Optional\nimport logging\nfrom src.config.config_loader import ConfigLoader\n\nclass TranslationService:\n    def __init__(self, config_loader: ConfigLoader):\n        self.logger = logging.getLogger(__name__)\n        self.config_loader = config_loader\n        \n        # Load API provider configuration\n        self.api_provider = self.config_loader.settings.get('API_PROVIDER')\n        if not self.api_provider:\n            raise ValueError(\"API_PROVIDER not specified in configuration\")\n        \n        # Load API key based on provider\n        env_key_name = f\"{self.api_provider.upper()}_API_KEY\"\n        self.api_key = self.config_loader.env_vars.get(env_key_name)\n        if not self.api_key:\n            raise ValueError(f\"Missing API key for {self.api_provider}. Set {env_key_name} environment variable\")\n        \n        # Load and validate target languages\n        self.target_languages = self.config_loader.settings.get('TARGET_LANGUAGES_LIST', [])\n        if not self.target_languages or not isinstance(self.target_languages, list):\n            raise ValueError(\"TARGET_LANGUAGES_LIST must be a non-empty list\")\n            \n        self.logger.info(f\"TranslationService initialized with {self.api_provider} provider\")\n        self.logger.debug(f\"Supported target languages: {', '.join(self.target_languages)}\")\n    \n    def get_supported_languages(self) -> List[str]:\n        \"\"\"Return the list of supported target languages\"\"\"\n        return self.target_languages.copy()\n    \n    def is_language_supported(self, language_code: str) -> bool:\n        \"\"\"Check if a specific language code is supported\"\"\"\n        return language_code in self.target_languages\n    \n    def translate_text(self, text: str, target_language: str, source_language: Optional[str] = None) -> str:\n        \"\"\"Translate a single text string to the target language\"\"\"\n        raise NotImplementedError(\"Method will be implemented in the next iteration\")\n    \n    def translate_batch(self, texts: List[str], target_language: str, source_language: Optional[str] = None) -> List[str]:\n        \"\"\"Translate a batch of text strings to the target language\"\"\"\n        raise NotImplementedError(\"Method will be implemented in the next iteration\")\n```\n\n### Configuration Example\n```python\n# Example configuration in settings.yaml\nAPI_PROVIDER: \"DeepL\"\nTARGET_LANGUAGES_LIST:\n  - \"DE\"  # German\n  - \"FR\"  # French\n  - \"ES\"  # Spanish\n  - \"IT\"  # Italian\n  - \"JA\"  # Japanese\n```\n\n### Test Cases\n```python\ndef test_get_supported_languages():\n    \"\"\"Test that get_supported_languages returns a copy of the languages list\"\"\"\n    mock_config = MagicMock()\n    mock_config.settings = {'API_PROVIDER': 'DeepL', 'TARGET_LANGUAGES_LIST': ['DE', 'FR', 'ES']}\n    mock_config.env_vars = {'DEEPL_API_KEY': 'fake-key'}\n    \n    service = TranslationService(mock_config)\n    languages = service.get_supported_languages()\n    \n    # Verify it returns the correct languages\n    assert languages == ['DE', 'FR', 'ES']\n    \n    # Verify it's a copy (modifying the returned list shouldn't affect the original)\n    languages.append('IT')\n    assert service.get_supported_languages() == ['DE', 'FR', 'ES']\n\ndef test_is_language_supported():\n    \"\"\"Test that is_language_supported correctly identifies supported languages\"\"\"\n    mock_config = MagicMock()\n    mock_config.settings = {'API_PROVIDER': 'DeepL', 'TARGET_LANGUAGES_LIST': ['DE', 'FR', 'ES']}\n    mock_config.env_vars = {'DEEPL_API_KEY': 'fake-key'}\n    \n    service = TranslationService(mock_config)\n    \n    assert service.is_language_supported('DE') is True\n    assert service.is_language_supported('FR') is True\n    assert service.is_language_supported('IT') is False\n    assert service.is_language_supported('') is False\n```\n\n### Dependency Injection Setup\nFor dependency injection, register the service in your DI container:\n\n```python\n# In your dependency injection setup file\nfrom dependency_injector import containers, providers\nfrom src.config.config_loader import ConfigLoader\nfrom src.services.translation_service import TranslationService\n\nclass Container(containers.DeclarativeContainer):\n    config = providers.Singleton(ConfigLoader)\n    translation_service = providers.Singleton(\n        TranslationService,\n        config_loader=config\n    )\n```\n</info added on 2025-05-04T17:38:53.434Z>"
        },
        {
          "id": 2,
          "title": "Implement core translation method with DeepL API integration",
          "description": "Create the primary translation method that sends text to the DeepL API and processes the basic response",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Implement a TranslateText method that takes source text and target language code as parameters. Create the HTTP client to communicate with DeepL API. Build the request with proper authentication headers using the API key. Send the request to the DeepL translation endpoint and parse the basic successful response. Return the translated text.\n\n<info added on 2025-05-04T17:42:14.630Z>\n## Implementation Plan (Iteration 1)\n\n1.  **Dependency Check**: Ensure `httpx` is in `translation-py/requirements.txt`. Add if missing.\n2.  **Modify `TranslationService` (`translation-py/src/services/translation_service.py`)**:\n    *   Import `httpx`.\n    *   Import `Optional` from `typing` if not already present.\n    *   Implement `_initialize_http_client`:\n        *   If `self._http_client` is None, create `self._http_client = httpx.Client(timeout=30.0)`.\n    *   Implement `translate_text`:\n        *   Call `self._validate_target_language(target_language)`.\n        *   Handle `self.test_mode` (return simulated translation).\n        *   Call `self._initialize_http_client()`.\n        *   Define `DEEPL_API_URL` (e.g., `\"https://api-free.deepl.com/v2/translate\"`).\n        *   Prepare `headers = {'Authorization': f'DeepL-Auth-Key {self.api_key}'}`.\n        *   Prepare `payload = {'text': text, 'target_lang': target_language}`.\n        *   If `source_language` is provided, add it to `payload`.\n        *   Wrap the API call in a `try...except httpx.RequestError as e:` block for basic network errors.\n        *   Make the call: `response = self._http_client.post(DEEPL_API_URL, headers=headers, data=payload)`.\n        *   Check `response.status_code`:\n            *   If 200: Parse `response.json()`, extract `['translations'][0]['text']`, and return it.\n            *   If not 200: Log the error (status code, response text) and raise `RuntimeError(f\"DeepL API Error {response.status_code}: {response.text}\")`. (More specific error handling in 8.3).\n\n3.  **Modify Tests (`translation-py/tests/services/test_translation_service.py`)**:\n    *   Add `from unittest.mock import patch`.\n    *   `test_translate_text_success`:\n        *   Use `@patch('httpx.Client')`.\n        *   Mock the `post` method of the client instance to return a MagicMock response with `status_code=200` and `.json()` returning `{'translations': [{'text': 'Hallo Welt'}]}`.\n        *   Instantiate `TranslationService` with appropriate mock config.\n        *   Call `service.translate_text('Hello World', 'DE')`.\n        *   Assert the return value is `'Hallo Welt'`. Assert `mock_post.assert_called_once()`.\n    *   `test_translate_text_api_error`:\n        *   Use `@patch('httpx.Client')`.\n        *   Mock `post` to return a response with `status_code=403` and `text='Forbidden'`. \n        *   Use `pytest.raises(RuntimeError, match=\"DeepL API Error 403\")` when calling `service.translate_text`.\n    *   `test_translate_text_network_error`:\n        *   Use `@patch('httpx.Client')`.\n        *   Mock `post` to raise `httpx.RequestError(\"Network issue\")`.\n        *   Use `pytest.raises(httpx.RequestError)` when calling `service.translate_text`.\n    *   `test_translate_text_respects_test_mode`:\n        *   Instantiate service with `TEST_MODE_BOOL=True`.\n        *   Use `@patch('httpx.Client')` just to verify it's *not* called.\n        *   Call `service.translate_text('Hello', 'DE')`.\n        *   Assert the result is the simulated `\"[Translated DE] Hello\"`.\n        *   Assert `mock_client_instance.post.assert_not_called()`.\n    *   `test_translate_text_with_source_language`:\n        *   Use `@patch('httpx.Client')`.\n        *   Mock `post` to return a successful response.\n        *   Call `service.translate_text('Bonjour', 'EN', source_language='FR')`.\n        *   Assert `mock_post.assert_called_once()` and inspect the `data` argument passed to `post` to ensure `source_lang='FR'` was included.\n\n4.  **Next Steps**: Implement code and run tests.\n</info added on 2025-05-04T17:42:14.630Z>"
        },
        {
          "id": 3,
          "title": "Add comprehensive error handling for API responses",
          "description": "Enhance the translation method with proper error handling for various API response scenarios",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Implement error handling for different HTTP status codes (401 for authentication issues, 429 for rate limiting, etc.). Create custom exception types for different error scenarios. Parse error messages from the DeepL API response. Add logging for errors with appropriate severity levels. Ensure the service provides meaningful error messages to callers.\n\n<info added on 2025-05-04T17:47:59.506Z>\n## Implementation Details\n\n### Custom Exception Structure\n```python\nclass TranslationError(Exception):\n    \"\"\"Base exception for all translation-related errors.\"\"\"\n    pass\n\nclass TranslationAuthError(TranslationError):\n    \"\"\"Raised when authentication with the translation API fails.\"\"\"\n    pass\n\nclass TranslationRateLimitError(TranslationError):\n    \"\"\"Raised when API rate limits are exceeded.\"\"\"\n    def __init__(self, message, retry_after=None):\n        super().__init__(message)\n        self.retry_after = retry_after  # Store retry-after header value if available\n\nclass TranslationAPIError(TranslationError):\n    \"\"\"Raised for general API errors.\"\"\"\n    def __init__(self, message, status_code=None, response_body=None):\n        super().__init__(message)\n        self.status_code = status_code\n        self.response_body = response_body\n\nclass TranslationNetworkError(TranslationError):\n    \"\"\"Raised when network issues prevent API communication.\"\"\"\n    pass\n```\n\n### Error Response Parsing\n```python\ndef _parse_error_response(response):\n    \"\"\"Extract detailed error information from DeepL API response.\"\"\"\n    try:\n        error_data = response.json()\n        error_message = error_data.get('message', '')\n        error_code = error_data.get('error', {}).get('code', '')\n        return f\"{error_code}: {error_message}\" if error_code else error_message\n    except (ValueError, KeyError):\n        return response.text\n```\n\n### Rate Limit Handling\n```python\n# Inside the 429 error handling block\nretry_after = response.headers.get('Retry-After')\nif retry_after:\n    logger.warning(f\"Rate limit exceeded. Retry after {retry_after} seconds.\")\n    raise TranslationRateLimitError(\n        f\"DeepL API rate limit exceeded. Retry after {retry_after} seconds.\", \n        retry_after=int(retry_after)\n    )\n```\n\n### Logging Enhancement\n```python\n# Configure structured logging\nimport logging\nimport json\n\nclass JSONFormatter(logging.Formatter):\n    def format(self, record):\n        log_record = {\n            'timestamp': self.formatTime(record),\n            'level': record.levelname,\n            'message': record.getMessage(),\n            'module': record.module\n        }\n        if hasattr(record, 'status_code'):\n            log_record['status_code'] = record.status_code\n        if hasattr(record, 'error_type'):\n            log_record['error_type'] = record.error_type\n        return json.dumps(log_record)\n\n# Usage in error handling:\nlogger.error(\"API authentication failed\", \n             extra={'status_code': e.response.status_code, 'error_type': 'auth_error'})\n```\n\n### Retry Mechanism\n```python\ndef translate_with_retry(text, source_lang, target_lang, max_retries=3, backoff_factor=1.5):\n    \"\"\"Translate text with exponential backoff retry for rate limit errors.\"\"\"\n    retry_count = 0\n    while True:\n        try:\n            return translate_text(text, source_lang, target_lang)\n        except TranslationRateLimitError as e:\n            retry_count += 1\n            if retry_count > max_retries:\n                logger.error(f\"Max retries ({max_retries}) exceeded for translation request\")\n                raise\n            \n            # Use retry-after header if available, otherwise calculate backoff\n            wait_time = e.retry_after if e.retry_after else backoff_factor ** retry_count\n            logger.info(f\"Rate limit hit, retrying in {wait_time} seconds (attempt {retry_count}/{max_retries})\")\n            time.sleep(wait_time)\n        except (TranslationAuthError, TranslationAPIError, TranslationNetworkError):\n            # Don't retry these errors\n            raise\n```\n</info added on 2025-05-04T17:47:59.506Z>\n\n<info added on 2025-05-04T17:49:00.439Z>\n<info added on 2025-05-05T09:23:12.104Z>\n## Implementation Status Report\n\n### Integration with Existing Error Handling System\n```python\n# In translation_service.py\nfrom app.core.logging import get_logger\nfrom app.core.exceptions import handle_external_service_error\n\nlogger = get_logger(__name__)\n\ndef translate_text(text, source_lang, target_lang):\n    try:\n        # Existing translation code...\n    except Exception as e:\n        # Wrap in our application's standard error handling\n        handle_external_service_error(e, service_name=\"DeepL\", \n                                     operation=\"text translation\",\n                                     context={\"source_lang\": source_lang, \"target_lang\": target_lang})\n        raise\n```\n\n### HTTP Status Code Mapping\n```python\n# Complete HTTP status code handling\nSTATUS_CODE_HANDLERS = {\n    400: lambda resp: TranslationAPIError(f\"Bad request: {_parse_error_response(resp)}\", resp.status_code, resp.text),\n    401: lambda resp: TranslationAuthError(\"Authentication failed: Invalid API key\", resp.status_code, resp.text),\n    403: lambda resp: TranslationAuthError(\"Authentication failed: Access forbidden\", resp.status_code, resp.text),\n    404: lambda resp: TranslationAPIError(\"Resource not found\", resp.status_code, resp.text),\n    413: lambda resp: TranslationAPIError(\"Payload too large\", resp.status_code, resp.text),\n    414: lambda resp: TranslationAPIError(\"URI too long\", resp.status_code, resp.text),\n    429: lambda resp: _handle_rate_limit(resp),\n    456: lambda resp: TranslationAPIError(\"Quota exceeded\", resp.status_code, resp.text),\n    503: lambda resp: TranslationAPIError(\"Service unavailable\", resp.status_code, resp.text),\n}\n\ndef _handle_http_error(response):\n    \"\"\"Handle HTTP errors based on status code\"\"\"\n    handler = STATUS_CODE_HANDLERS.get(\n        response.status_code, \n        lambda r: TranslationAPIError(f\"Unexpected API error: {_parse_error_response(r)}\", r.status_code, r.text)\n    )\n    return handler(response)\n```\n\n### Metrics Collection\n```python\n# Add metrics collection for error monitoring\nfrom app.core.metrics import increment_counter, record_timing\n\ndef translate_text(text, source_lang, target_lang):\n    start_time = time.time()\n    success = False\n    error_type = None\n    \n    try:\n        # Existing translation code...\n        success = True\n        return result\n    except TranslationRateLimitError as e:\n        error_type = \"rate_limit\"\n        raise\n    except TranslationAuthError as e:\n        error_type = \"auth\"\n        raise\n    except TranslationAPIError as e:\n        error_type = \"api\"\n        raise\n    except TranslationNetworkError as e:\n        error_type = \"network\"\n        raise\n    finally:\n        duration = time.time() - start_time\n        record_timing(\"translation_service.request_duration\", duration)\n        increment_counter(\"translation_service.requests\", \n                         tags={\"success\": success, \n                              \"error_type\": error_type or \"none\",\n                              \"source_lang\": source_lang,\n                              \"target_lang\": target_lang})\n```\n\n### Circuit Breaker Pattern\n```python\nfrom pybreaker import CircuitBreaker\n\n# Create circuit breaker for translation service\ntranslation_breaker = CircuitBreaker(\n    fail_max=5,           # Number of failures before opening circuit\n    reset_timeout=60,     # Seconds until attempting to close circuit\n    exclude=[TranslationAuthError]  # Don't count auth errors for circuit breaking\n)\n\n@translation_breaker\ndef translate_text(text, source_lang, target_lang):\n    # Existing implementation\n    pass\n```\n\n### Error Response Documentation\n```python\n# Error response examples for documentation\nERROR_RESPONSE_EXAMPLES = {\n    \"auth_error\": {\n        \"status_code\": 403,\n        \"error\": {\n            \"message\": \"Authentication failed\",\n            \"code\": \"auth_failed\"\n        }\n    },\n    \"rate_limit\": {\n        \"status_code\": 429,\n        \"error\": {\n            \"message\": \"Too many requests\",\n            \"code\": \"too_many_requests\"\n        },\n        \"headers\": {\n            \"Retry-After\": \"30\"\n        }\n    },\n    \"quota_exceeded\": {\n        \"status_code\": 456,\n        \"error\": {\n            \"message\": \"Quota exceeded\",\n            \"code\": \"quota_exceeded\"\n        }\n    }\n}\n```\n</info added on 2025-05-05T09:23:12.104Z>\n</info added on 2025-05-04T17:49:00.439Z>"
        },
        {
          "id": 4,
          "title": "Implement retry mechanism for transient failures",
          "description": "Add a retry mechanism to handle temporary failures when communicating with the DeepL API",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "Implement an exponential backoff retry strategy for transient errors (network issues, 5xx responses, etc.). Configure retry count and delay parameters from configuration. Add circuit breaker pattern to prevent overwhelming the API during extended outages. Implement timeout handling. Add detailed logging for retry attempts.\n\n<info added on 2025-05-04T17:52:13.955Z>\n# Implementation Plan (Iteration 1)\n\n## Detailed Implementation Notes\n\n### Retry Mechanism Design\n- Use a stateless retry approach initially, with plans to add circuit breaker in iteration 2\n- Implement exponential backoff with jitter to prevent thundering herd problem:\n  ```python\n  import random\n  wait_time = self.retry_backoff_factor * (2 ** attempt) * (0.8 + 0.4 * random.random())\n  ```\n\n### Error Classification\n- Create a helper method to classify errors:\n  ```python\n  def _is_retryable_error(self, exception):\n      if isinstance(exception, (TranslationNetworkError, TranslationRateLimitError)):\n          return True\n      if isinstance(exception, TranslationAPIError) and exception.status_code in self.retry_status_codes:\n          return True\n      return False\n  ```\n\n### Logging Enhancements\n- Add structured logging with context:\n  ```python\n  self.logger.warning(\n      \"API call failed, will retry\",\n      extra={\n          \"attempt\": attempt + 1,\n          \"max_attempts\": self.retry_max_attempts,\n          \"wait_time\": wait_time,\n          \"error_type\": type(e).__name__,\n          \"status_code\": getattr(e, \"status_code\", None)\n      }\n  )\n  ```\n\n### Timeout Handling\n- Add request timeout configuration:\n  ```yaml\n  # In settings.default.yaml\n  API_REQUEST_TIMEOUT: 10  # seconds\n  ```\n- Pass timeout to HTTP client:\n  ```python\n  response = self._http_client.post(url, headers=headers, json=payload, timeout=self.request_timeout)\n  ```\n\n### Testing Edge Cases\n- Add test for timeout scenario:\n  ```python\n  @patch('requests.post', side_effect=requests.exceptions.Timeout(\"Request timed out\"))\n  def test_translate_text_handles_timeout(self, mock_post, mock_sleep, ...):\n      # Test that timeout is converted to TranslationNetworkError and retried\n  ```\n- Add test for partial success (some translations fail, others succeed)\n\n### Performance Monitoring\n- Add metrics collection for retry attempts:\n  ```python\n  # Track metrics for monitoring retry performance\n  if attempt > 0:\n      self.metrics.increment(\"translation.retry.attempt\", tags={\"attempt\": attempt})\n  ```\n</info added on 2025-05-04T17:52:13.955Z>\n\n<info added on 2025-05-04T17:54:04.118Z>\n## Implementation Log (Iteration 1 - Completion)\n\n- Added retry configuration options (`RETRY_MAX_ATTEMPTS`, `RETRY_BACKOFF_FACTOR`, `RETRY_STATUS_CODES`) to `settings.default.yaml`.\n- Updated `ConfigLoader` to load and validate these settings.\n- Updated `TranslationService.__init__` to load retry settings from config.\n- Added `status_code` attribute to `TranslationAPIError`.\n- Implemented private helper method `_make_api_call_with_retry` containing:\n    - Loop for `retry_max_attempts`.\n    - Core `_http_client.post` call.\n    - `try...except` block to catch `HTTPStatusError` and `RequestError`.\n    - Instantiation of custom `Translation...Error` exceptions based on caught errors.\n    - Check if the caught error is retryable using `_is_retryable_error` helper.\n    - Exponential backoff (`time.sleep`) calculation and call for retryable errors.\n    - Raising the appropriate error if not retryable or max attempts reached.\n- Updated `translate_text` to call `_make_api_call_with_retry`.\n- Added new tests in `test_translation_service.py` specifically for retry logic:\n    - Test retry success for network, rate limit (429), and server (503) errors.\n    - Test failure after max attempts.\n    - Test no retry for auth (403) and non-retryable API (400) errors.\n    - Used `@patch('time.sleep')` and mock `side_effect` to simulate errors and avoid delays.\n- No rule updates needed.\n</info added on 2025-05-04T17:54:04.118Z>"
        },
        {
          "id": 5,
          "title": "Add batch translation support and language detection",
          "description": "Extend the service with methods for batch translation and automatic language detection",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "Implement a BatchTranslate method to efficiently translate multiple texts in a single API call. Add support for automatic source language detection using DeepL's detection capabilities. Create methods to validate if a target language is supported. Implement caching for frequently translated content to reduce API calls. Add usage statistics tracking to monitor API consumption.\n\n<info added on 2025-05-04T17:55:28.479Z>\nBased on your request, here's additional implementation information for the batch translation support:\n\n## Implementation Details for Batch Translation\n\n### API Payload Structure Considerations\n\nWhen implementing the `translate_batch` method, pay special attention to the payload structure for DeepL's API:\n\n- DeepL expects multiple text items as repeated parameters rather than a JSON array[2]\n- The payload structure using list-of-tuples ensures proper formatting: `[('text', text1), ('text', text2), ...]`\n- Maximum batch size should be limited to 50 texts per request to avoid potential API limitations\n- Add a `batch_size` parameter with default value of 50 to allow configuration\n\n### Error Handling Enhancements\n\nImplement robust error handling specifically for batch operations:\n\n```python\ndef _process_batch_response(self, response, texts):\n    \"\"\"Process batch translation response and handle potential errors.\"\"\"\n    try:\n        response_data = response.json()\n        translations = response_data.get('translations', [])\n        \n        if len(translations) != len(texts):\n            self.logger.error(f\"API returned {len(translations)} translations for {len(texts)} input texts\")\n            raise TranslationError(f\"Expected {len(texts)} translations, got {len(translations)}\")\n            \n        return [item['text'] for item in translations]\n    except (KeyError, TypeError) as e:\n        self.logger.error(f\"Failed to parse batch translation response: {str(e)}\")\n        raise TranslationError(f\"Invalid response format: {str(e)}\")\n```\n\n### Chunking Strategy for Large Batches\n\nTo handle large text collections efficiently:\n\n```python\ndef translate_batch(self, texts, target_language, source_language=None, batch_size=50):\n    \"\"\"Translate multiple texts in efficient batches.\"\"\"\n    if not texts:\n        self.logger.warning(\"Empty texts list provided to translate_batch\")\n        return []\n        \n    # Validate languages\n    self._validate_target_language(target_language)\n    if source_language:\n        self._validate_source_language(source_language)\n        \n    # Handle test mode\n    if self.test_mode:\n        return [f\"[Translated {target_language}] {text}\" for text in texts]\n    \n    # Process in chunks if needed\n    results = []\n    for i in range(0, len(texts), batch_size):\n        chunk = texts[i:i+batch_size]\n        chunk_results = self._translate_chunk(chunk, target_language, source_language)\n        results.extend(chunk_results)\n        \n    return results\n```\n\n### Performance Optimization\n\nImplement these optimizations for batch translation:\n\n1. **Concurrent Processing**: For very large batches, consider using `asyncio` to make concurrent API calls:\n\n```python\nasync def translate_batch_async(self, texts, target_language, source_language=None, batch_size=50, max_concurrency=5):\n    \"\"\"Translate large batches with concurrent API calls.\"\"\"\n    if not texts:\n        return []\n        \n    # Split into chunks\n    chunks = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n    \n    # Process chunks concurrently with rate limiting\n    semaphore = asyncio.Semaphore(max_concurrency)\n    async with httpx.AsyncClient() as client:\n        tasks = []\n        for chunk in chunks:\n            tasks.append(self._translate_chunk_async(client, semaphore, chunk, target_language, source_language))\n        \n        # Gather results maintaining original order\n        chunk_results = await asyncio.gather(*tasks)\n        \n    # Flatten results\n    return [translation for chunk in chunk_results for translation in chunk]\n```\n\n2. **Caching Strategy**: Implement a specialized cache for batch operations:\n\n```python\ndef _get_cached_batch_results(self, texts, target_language, source_language):\n    \"\"\"Retrieve cached translations for batch items.\"\"\"\n    if not self.cache_enabled:\n        return None, list(range(len(texts)))\n        \n    results = [None] * len(texts)\n    missing_indices = []\n    \n    for i, text in enumerate(texts):\n        cache_key = self._generate_cache_key(text, target_language, source_language)\n        cached = self.cache.get(cache_key)\n        if cached:\n            results[i] = cached\n        else:\n            missing_indices.append(i)\n            \n    return results, missing_indices\n```\n\n### Monitoring and Metrics\n\nAdd detailed metrics collection for batch operations:\n\n```python\ndef _update_batch_metrics(self, batch_size, successful_count, cached_count, error_count=0):\n    \"\"\"Update metrics specific to batch operations.\"\"\"\n    self.metrics['batch_requests'] += 1\n    self.metrics['batch_total_texts'] += batch_size\n    self.metrics['batch_successful_translations'] += successful_count\n    self.metrics['batch_cached_translations'] += cached_count\n    self.metrics['batch_failed_translations'] += error_count\n    \n    # Calculate and store efficiency metrics\n    if batch_size > 0:\n        efficiency = (successful_count / batch_size) * 100\n        self.metrics['batch_efficiency_percent'] = efficiency\n```\n\nThese enhancements provide a robust implementation for batch translation with proper error handling, performance optimization, and detailed metrics tracking.\n</info added on 2025-05-04T17:55:28.479Z>\n\n<info added on 2025-05-04T17:56:45.720Z>\n<info added on 2025-05-05T09:12:36.481Z>\n## Implementation Details for Language Detection\n\n### Automatic Language Detection Implementation\n\nWhen implementing the language detection feature, consider these technical aspects:\n\n```python\ndef detect_language(self, text):\n    \"\"\"Detect the language of the provided text using DeepL's API.\"\"\"\n    if not text or not text.strip():\n        self.logger.warning(\"Empty text provided for language detection\")\n        return None\n        \n    # DeepL doesn't have a dedicated language detection endpoint\n    # Instead, we use a translation with null source language\n    payload = [\n        ('text', text),\n        ('target_lang', 'EN')  # Use English as default target\n    ]\n    \n    try:\n        response = self._make_api_call_with_retry('POST', '/v2/translate', payload)\n        data = response.json()\n        \n        if 'translations' in data and len(data['translations']) > 0:\n            detected_lang = data['translations'][0].get('detected_source_language')\n            if detected_lang:\n                return detected_lang\n                \n        self.logger.error(\"Language detection failed: missing detected_source_language\")\n        return None\n    except Exception as e:\n        self.logger.error(f\"Language detection error: {str(e)}\")\n        raise LanguageDetectionError(f\"Failed to detect language: {str(e)}\")\n```\n\n### Language Support Validation\n\nImplement methods to check language support:\n\n```python\ndef is_language_supported(self, language_code, as_source=True):\n    \"\"\"Check if a language is supported by DeepL API.\"\"\"\n    if as_source:\n        return language_code.upper() in self.SUPPORTED_SOURCE_LANGUAGES\n    else:\n        return language_code.upper() in self.SUPPORTED_TARGET_LANGUAGES\n\ndef get_supported_languages(self, as_source=True):\n    \"\"\"Return list of supported language codes.\"\"\"\n    if as_source:\n        return list(self.SUPPORTED_SOURCE_LANGUAGES)\n    else:\n        return list(self.SUPPORTED_TARGET_LANGUAGES)\n```\n\n### Caching Implementation for Batch Operations\n\nImplement an efficient caching strategy for batch operations:\n\n```python\ndef _apply_batch_caching(self, texts, target_language, source_language=None):\n    \"\"\"Apply caching to batch translation requests.\"\"\"\n    if not self.cache_enabled:\n        return None, texts\n        \n    # Initialize results with None placeholders\n    results = [None] * len(texts)\n    texts_to_translate = []\n    indices_map = []\n    \n    # Check cache for each text\n    for i, text in enumerate(texts):\n        cache_key = self._generate_cache_key(text, target_language, source_language)\n        cached_result = self.cache.get(cache_key)\n        \n        if cached_result:\n            results[i] = cached_result\n            self.metrics['cache_hits'] += 1\n        else:\n            texts_to_translate.append(text)\n            indices_map.append(i)\n            self.metrics['cache_misses'] += 1\n    \n    return results, texts_to_translate, indices_map\n```\n\n### Usage Statistics Implementation\n\nAdd comprehensive usage tracking:\n\n```python\nclass UsageStatistics:\n    def __init__(self):\n        self.character_count = 0\n        self.request_count = 0\n        self.batch_request_count = 0\n        self.total_texts_processed = 0\n        self.cache_hit_count = 0\n        self.cache_hit_characters = 0\n        self.api_errors = 0\n        self.last_reset = datetime.now()\n        \n    def reset(self):\n        \"\"\"Reset all statistics.\"\"\"\n        self.__init__()\n        \n    def to_dict(self):\n        \"\"\"Convert statistics to dictionary.\"\"\"\n        return {\n            'character_count': self.character_count,\n            'request_count': self.request_count,\n            'batch_request_count': self.batch_request_count,\n            'total_texts_processed': self.total_texts_processed,\n            'cache_hit_rate': self._calculate_cache_hit_rate(),\n            'character_savings': self._calculate_character_savings(),\n            'api_errors': self.api_errors,\n            'tracking_since': self.last_reset.isoformat()\n        }\n        \n    def _calculate_cache_hit_rate(self):\n        total = self.cache_hit_count + self.total_texts_processed\n        if total == 0:\n            return 0\n        return (self.cache_hit_count / total) * 100\n        \n    def _calculate_character_savings(self):\n        if self.character_count == 0 and self.cache_hit_characters == 0:\n            return 0\n        total = self.character_count + self.cache_hit_characters\n        return (self.cache_hit_characters / total) * 100 if total > 0 else 0\n```\n\n### Integration Testing Considerations\n\nWhen testing the batch translation and language detection features:\n\n1. Create mock responses that include the `detected_source_language` field\n2. Test with mixed language inputs to verify detection accuracy\n3. Test with edge cases like very short texts where detection may be less reliable\n4. Verify that batch translation with automatic detection correctly handles texts in different languages\n\n```python\ndef test_batch_translation_with_auto_detection(self):\n    \"\"\"Test batch translation with automatic language detection.\"\"\"\n    texts = [\"Hello world\", \"Bonjour le monde\", \"Hallo Welt\"]\n    mock_response = {\n        \"translations\": [\n            {\"text\": \"Hola mundo\", \"detected_source_language\": \"EN\"},\n            {\"text\": \"Hola mundo\", \"detected_source_language\": \"FR\"},\n            {\"text\": \"Hola mundo\", \"detected_source_language\": \"DE\"}\n        ]\n    }\n    \n    with patch.object(self.service, '_make_api_call_with_retry') as mock_api:\n        mock_api.return_value.json.return_value = mock_response\n        mock_api.return_value.status_code = 200\n        \n        result = self.service.translate_batch(texts, \"ES\")\n        \n        # Verify API was called with null source_language\n        args, kwargs = mock_api.call_args\n        payload = kwargs.get('data', [])\n        self.assertNotIn(('source_lang', ANY), payload)\n```\n</info added on 2025-05-05T09:12:36.481Z>\n</info added on 2025-05-04T17:56:45.720Z>"
        }
      ]
    },
    {
      "id": 9,
      "title": "Implement test mode for TranslationService",
      "description": "Add a test mode that simulates translation without making API calls.",
      "status": "done",
      "dependencies": [
        8
      ],
      "priority": "medium",
      "details": "Extend TranslationService to check the TEST_MODE configuration flag. When enabled, return the original text instead of making API calls. Add logging to indicate when test mode is active. Ensure the same processing flow is followed regardless of test mode status.",
      "testStrategy": "Enable test mode and verify no API calls are made. Confirm the original text is returned as the 'translation'. Verify logging indicates test mode is active.",
      "subtasks": [
        {
          "id": 1,
          "title": "Add TEST_MODE configuration flag",
          "description": "Create a configuration flag that determines whether the translation service should operate in test mode",
          "status": "done",
          "dependencies": [],
          "details": "Add a boolean TEST_MODE flag to the application's configuration system. This could be in an environment variable, configuration file, or wherever other configuration settings are stored. Default value should be false. Document the new configuration option in relevant documentation. Create a method in the configuration service to access this flag."
        },
        {
          "id": 2,
          "title": "Modify TranslationService to check for test mode",
          "description": "Update the TranslationService to check the TEST_MODE flag before processing translations",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Inject or access the configuration service in the TranslationService. At the beginning of the translation method, add logic to check if TEST_MODE is enabled. Create a private method like 'isTestModeEnabled()' that encapsulates this check for better readability and reuse. Ensure this check happens before any API call preparation."
        },
        {
          "id": 3,
          "title": "Implement test mode translation logic",
          "description": "Add conditional logic to return original text when in test mode instead of making API calls",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "In the main translation method, add a conditional branch that executes when test mode is enabled. In this branch, skip the API call and instead return the original text wrapped in the same response object structure that would normally be returned after a successful API call. Ensure all the same data transformations and validations still occur so the return signature is identical regardless of mode."
        },
        {
          "id": 4,
          "title": "Add logging for test mode operation",
          "description": "Implement logging to indicate when translations are being processed in test mode",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "Add appropriate log statements that indicate when the service is operating in test mode. Log at INFO level when test mode is active and a translation is being simulated. Include relevant details such as the original text length, language pairs, and any other contextual information that would be useful for debugging. Ensure logs clearly distinguish between real translations and test mode translations."
        }
      ]
    },
    {
      "id": 10,
      "title": "Implement Markdown reconstruction from translated segments",
      "description": "Create functionality to rebuild Markdown content using translated text segments.",
      "status": "done",
      "dependencies": [
        7,
        8
      ],
      "priority": "high",
      "details": "Extend MarkdownProcessor to replace original text segments in the AST with translated segments based on the mapping created during extraction. Implement methods to reconstruct the full Markdown content from the modified AST. Ensure all formatting, links, and non-translated elements are preserved correctly. Note that the current implementation has a known limitation: it replaces entire inline segments without preserving inline formatting (bold, italic, links within text). This limitation will be addressed when subtask 10.4 is completed.",
      "testStrategy": "Create a test workflow with extraction, simulated translation, and reconstruction. Compare the reconstructed Markdown with expected output to verify structure preservation. Test with complex Markdown features. Be aware that tests involving inline formatting will not pass until subtask 10.4 is completed.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create AST node replacement method",
          "description": "Implement a method in MarkdownProcessor that can replace text content in AST nodes while preserving node structure and attributes",
          "status": "done",
          "dependencies": [],
          "details": "Create a method called `replaceNodeContent(node, originalText, translatedText)` that takes an AST node, the original text, and the translated text as parameters. The method should update the node's content with the translated text while preserving all node attributes, formatting, and structure. Handle different node types (paragraph, heading, list item, etc.) appropriately. Include unit tests to verify the method works correctly for various node types.\n\n<info added on 2025-05-04T18:13:45.543Z>\n## Implementation Plan (Subtask 10.1 - TDD Approach)\n\n### Implementation Details:\n\n1. **Token Structure Handling:**\n   - The markdown-it-py parser creates tokens with a specific structure:\n   - Paragraph tokens have `type: 'paragraph_open'` followed by an `inline` token containing children, then `paragraph_close`\n   - The actual text content is in `text` type children within the `inline` token\n\n2. **Method Signature:**\n   ```python\n   def replace_node_content(self, token: Dict[str, Any], translated_text: str) -> bool:\n       \"\"\"\n       Replace the content of the first text child within an inline token.\n       \n       Args:\n           token: The inline token containing text to replace\n           translated_text: The new text content\n           \n       Returns:\n           bool: True if replacement succeeded, False otherwise\n       \"\"\"\n   ```\n\n3. **Error Handling:**\n   - Add specific error messages:\n     ```python\n     if token.get('type') != 'inline':\n         self.logger.error(f\"Expected inline token, got {token.get('type')}\")\n         return False\n     \n     if not token.get('children'):\n         self.logger.warning(\"Token has no children\")\n         return False\n     ```\n\n4. **Test Data Examples:**\n   ```python\n   # Test data for paragraph\n   md_text = \"This is a paragraph.\"\n   tokens = self.processor.parse(md_text)\n   inline_token = tokens[1]  # The inline token is typically at index 1\n   \n   # Test data for heading\n   md_heading = \"# This is a heading\"\n   heading_tokens = self.processor.parse(md_heading)\n   heading_inline = heading_tokens[1]\n   ```\n\n5. **Edge Cases to Test:**\n   - Empty text content\n   - Inline tokens with multiple text children (should only replace first)\n   - Tokens with mixed formatting (should preserve formatting tokens)\n   - Unicode/special characters in translated text\n\n6. **Logging Recommendations:**\n   - Add debug logging showing before/after content for traceability\n   - Log token structure at trace level for debugging complex cases\n</info added on 2025-05-04T18:13:45.543Z>\n\n<info added on 2025-05-04T18:47:05.446Z>\n## Implementation Update (Subtask 10.1 - Integration Details)\n\n### Current Implementation Status:\n```python\ndef replace_node_content(self, token: Dict[str, Any], translated_text: str) -> bool:\n    \"\"\"\n    Replace the content of the first text child within an inline token.\n    \n    Args:\n        token: The inline token containing text to replace\n        translated_text: The new text content\n        \n    Returns:\n        bool: True if replacement succeeded, False otherwise\n    \"\"\"\n    if token.get('type') != 'inline':\n        self.logger.error(f\"Expected inline token, got {token.get('type')}\")\n        return False\n    \n    text_child = None\n    for child in token.get('children', []):\n        if child.get('type') == 'text':\n            text_child = child\n            break\n    \n    if not text_child:\n        self.logger.warning(\"No text child found in token\")\n        return False\n    \n    # Store original for logging\n    original_content = text_child.get('content', '')\n    \n    # Replace content\n    text_child['content'] = translated_text\n    \n    # Remove other children to avoid duplicating content\n    # This is a simplification - we'll handle complex formatting in later subtasks\n    token['children'] = [text_child]\n    \n    self.logger.debug(f\"Replaced: '{original_content}'  '{translated_text}'\")\n    return True\n```\n\n### Integration with Reassembly Process:\n```python\ndef reassemble_markdown(self, tokens: List[Dict], translations: Dict[str, str]) -> str:\n    \"\"\"\n    Reassemble markdown with translations applied to appropriate tokens.\n    \n    Args:\n        tokens: The parsed markdown tokens\n        translations: Dictionary mapping original text to translated text\n        \n    Returns:\n        str: Reassembled markdown with translations\n    \"\"\"\n    # Create a deep copy to avoid modifying original tokens\n    tokens_copy = copy.deepcopy(tokens)\n    \n    # Track which translations have been applied\n    applied_translations = set()\n    \n    # Process tokens\n    for i, token in enumerate(tokens_copy):\n        if token.get('type') == 'inline':\n            for child in token.get('children', []):\n                if child.get('type') == 'text':\n                    original_text = child.get('content', '')\n                    if original_text in translations:\n                        self.replace_node_content(token, translations[original_text])\n                        applied_translations.add(original_text)\n                        break\n    \n    # Log any unused translations\n    unused = set(translations.keys()) - applied_translations\n    if unused:\n        self.logger.warning(f\"Unused translations: {len(unused)}\")\n    \n    # Render back to markdown\n    return self.renderer.render(tokens_copy)\n```\n\n### Known Limitations:\n1. Only replaces the first text child in an inline token\n2. Removes additional children, which means inline formatting (bold, italic) is lost\n3. Does not handle complex nested structures like lists within lists\n\n### Next Steps:\n1. Enhance to preserve inline formatting by manipulating the AST more carefully\n2. Add support for handling complex nested structures\n3. Implement a more robust token path tracking system for precise replacements\n</info added on 2025-05-04T18:47:05.446Z>"
        },
        {
          "id": 2,
          "title": "Implement segment mapping lookup functionality",
          "description": "Create a mechanism to efficiently look up translated segments based on original text and node context",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Implement a `SegmentMapper` class that manages the mapping between original text segments and their translations. Include methods like `getTranslationForSegment(originalText, nodeContext)` that retrieves the appropriate translation based on the original text and contextual information about the node (like node type, position, etc.). This will handle cases where the same text might have different translations in different contexts. Add appropriate caching to optimize performance for repeated lookups."
        },
        {
          "id": 3,
          "title": "Develop AST traversal and modification algorithm",
          "description": "Create a recursive algorithm to traverse the Markdown AST and apply translations to each text node",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement a method called `traverseAndTranslateAST(ast, segmentMapper)` that recursively walks through the AST, identifies text nodes that need translation, and applies the appropriate translations using the segment mapper and node replacement method. The algorithm should handle nested structures correctly and maintain the hierarchical relationships between nodes. Include special handling for nodes that should not be modified (like code blocks or already processed custom elements)."
        },
        {
          "id": 4,
          "title": "Implement special element preservation logic",
          "description": "Create handlers for preserving special Markdown elements like links, images, and formatting during reconstruction",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "Extend the AST traversal algorithm with specialized handlers for complex Markdown elements. Implement methods like `preserveLinks(node)`, `preserveImages(node)`, and `preserveFormatting(node)` that ensure these elements are correctly maintained during translation. For links, ensure the link text is translated while the URL is preserved. For images, translate alt text while preserving the image path. For formatting (bold, italic, etc.), ensure the formatting markers are correctly applied to the translated text.\n\nNOTE: This subtask has been deferred to complete the core block-level reconstruction functionality first. The current implementation replaces entire inline segments, meaning inline formatting (bold, italic, links within text) is NOT preserved in the translated output. This limitation must be addressed before any downstream tasks that rely on perfect inline formatting fidelity in the generated translated Markdown files."
        },
        {
          "id": 5,
          "title": "Create Markdown reconstruction method",
          "description": "Implement functionality to convert the modified AST back into valid Markdown text",
          "status": "done",
          "dependencies": [
            3,
            4
          ],
          "details": "Create a method called `reconstructMarkdown(ast)` that takes the modified AST and generates valid Markdown text. This should handle all Markdown syntax elements correctly, including headings, lists, code blocks, tables, and formatting. Ensure proper indentation and line breaks are maintained. The method should produce Markdown that is semantically equivalent to the original but with translated text segments."
        },
        {
          "id": 6,
          "title": "Add integration and validation functionality",
          "description": "Implement methods to validate the reconstructed Markdown and integrate the entire translation process",
          "status": "done",
          "dependencies": [
            5
          ],
          "details": "Create a `validateReconstructedMarkdown(original, translated)` method that checks the structural integrity of the translated Markdown compared to the original. Implement the main public method `translateMarkdown(markdown, translations)` that orchestrates the entire process: parsing the original Markdown to AST, applying translations, reconstructing the Markdown, and validating the result. Add comprehensive error handling and logging to identify and report any issues during the translation process. Include integration tests that verify the entire workflow with various Markdown structures and translation scenarios.\n\n<info added on 2025-05-04T18:48:46.223Z>\n## Implementation Details for Integration and Validation\n\n### `validateReconstructedMarkdown` Implementation:\n\n```python\ndef validateReconstructedMarkdown(self, original_markdown: str, reconstructed_markdown: str) -> bool:\n    \"\"\"\n    Validates the structural integrity of translated Markdown compared to original.\n    \n    Args:\n        original_markdown: The source Markdown text\n        reconstructed_markdown: The translated Markdown text\n        \n    Returns:\n        bool: True if validation passes, False otherwise\n    \"\"\"\n    try:\n        # Basic validation: ensure the translated content can be parsed\n        translated_ast = self.parse(reconstructed_markdown)\n        \n        # Structure validation: compare node types and hierarchy\n        original_ast = self.parse(original_markdown)\n        \n        # Compare structure (not content) of both ASTs\n        if not self._compare_ast_structure(original_ast, translated_ast):\n            self.logger.warning(\"Structural mismatch between original and translated Markdown\")\n            return False\n            \n        return True\n    except Exception as e:\n        self.logger.error(f\"Validation failed: {str(e)}\")\n        return False\n        \ndef _compare_ast_structure(self, original_node, translated_node):\n    \"\"\"\n    Recursively compares the structure of two AST nodes.\n    Only compares node types and hierarchy, not content.\n    \n    Returns:\n        bool: True if structures match, False otherwise\n    \"\"\"\n    if original_node['type'] != translated_node['type']:\n        return False\n        \n    # Check if both have children or neither has children\n    orig_children = original_node.get('children', [])\n    trans_children = translated_node.get('children', [])\n    \n    if len(orig_children) != len(trans_children):\n        return False\n        \n    # Recursively check children\n    for i in range(len(orig_children)):\n        if not self._compare_ast_structure(orig_children[i], trans_children[i]):\n            return False\n            \n    return True\n```\n\n### Error Handling in `translateMarkdown`:\n\n```python\ndef translateMarkdown(self, markdown_text: str, translations: Dict[str, str]) -> str:\n    \"\"\"\n    Orchestrates the entire Markdown translation process.\n    \n    Args:\n        markdown_text: Original Markdown content\n        translations: Dictionary mapping original text to translated text\n        \n    Returns:\n        str: Translated Markdown content\n        \n    Raises:\n        MarkdownProcessingError: If translation process fails\n    \"\"\"\n    try:\n        # Parse the original Markdown to AST\n        self.logger.info(\"Parsing original Markdown to AST\")\n        original_ast = self.parse(markdown_text)\n        \n        # Apply translations and reconstruct\n        self.logger.info(f\"Applying {len(translations)} translations\")\n        reconstructed_markdown = self.reassemble_markdown(original_ast, translations)\n        \n        # Validate the result\n        self.logger.info(\"Validating reconstructed Markdown\")\n        if not self.validateReconstructedMarkdown(markdown_text, reconstructed_markdown):\n            self.logger.warning(\"Validation warnings occurred but translation completed\")\n        \n        return reconstructed_markdown\n        \n    except Exception as e:\n        error_msg = f\"Markdown translation failed: {str(e)}\"\n        self.logger.error(error_msg)\n        raise MarkdownProcessingError(error_msg) from e\n```\n\n### Integration Test Example:\n\n```python\ndef test_end_to_end_translation_process():\n    \"\"\"Tests the complete Markdown translation workflow with various structures\"\"\"\n    processor = MarkdownProcessor()\n    \n    # Test case with multiple Markdown elements\n    original_markdown = \"\"\"\n# Hello World\n\nThis is a paragraph with **bold** and *italic* text.\n\n- List item 1\n- List item 2\n\n| Header 1 | Header 2 |\n|----------|----------|\n| Cell 1   | Cell 2   |\n\"\"\"\n\n    translations = {\n        \"Hello World\": \"Bonjour le Monde\",\n        \"This is a paragraph with\": \"C'est un paragraphe avec\",\n        \"bold\": \"gras\",\n        \"italic\": \"italique\",\n        \"text\": \"texte\",\n        \"List item 1\": \"lment de liste 1\",\n        \"List item 2\": \"lment de liste 2\",\n        \"Header 1\": \"En-tte 1\",\n        \"Header 2\": \"En-tte 2\",\n        \"Cell 1\": \"Cellule 1\",\n        \"Cell 2\": \"Cellule 2\"\n    }\n    \n    expected_contains = [\n        \"# Bonjour le Monde\",\n        \"C'est un paragraphe avec **gras** et *italique* texte\",\n        \"- lment de liste 1\",\n        \"- lment de liste 2\",\n        \"| En-tte 1 | En-tte 2 |\",\n        \"| Cellule 1 | Cellule 2 |\"\n    ]\n    \n    result = processor.translateMarkdown(original_markdown, translations)\n    \n    # Verify all expected content is in the result\n    for expected in expected_contains:\n        assert expected in result, f\"Expected '{expected}' not found in result\"\n```\n\n### Custom Exception Class:\n\n```python\nclass MarkdownProcessingError(Exception):\n    \"\"\"Exception raised for errors in the Markdown processing pipeline.\"\"\"\n    pass\n```\n</info added on 2025-05-04T18:48:46.223Z>"
        },
        {
          "id": 7,
          "title": "Document inline formatting limitations",
          "description": "Document the current limitations regarding inline formatting preservation in the translated output",
          "status": "done",
          "dependencies": [
            6
          ],
          "details": "Create comprehensive documentation that clearly explains the current limitations of the Markdown reconstruction process with respect to inline formatting. Document that the current implementation replaces entire inline segments and does not preserve inline formatting elements like bold, italic, and links within text. Include examples showing what works and what doesn't work in the current implementation. Add warnings in code comments and user-facing documentation to ensure developers and users are aware of these limitations until subtask 10.4 is completed."
        },
        {
          "id": 8,
          "title": "Create test cases for inline formatting preservation",
          "description": "Develop test cases that will validate inline formatting preservation once subtask 10.4 is implemented",
          "status": "done",
          "dependencies": [
            7
          ],
          "details": "Create a comprehensive set of test cases that specifically target inline formatting preservation. These tests should include various combinations of bold, italic, links, code spans, and other inline elements within translated text. The tests should be designed to fail with the current implementation but pass once subtask 10.4 is completed. Include detailed comments explaining the expected behavior and why the current implementation doesn't meet these requirements. These tests will serve as validation criteria for the completion of subtask 10.4."
        }
      ]
    },
    {
      "id": 11,
      "title": "Implement output file generation",
      "description": "Create functionality to write translated content to output files with proper directory structure.",
      "status": "done",
      "dependencies": [
        10
      ],
      "priority": "high",
      "details": "Extend FileManager to construct output paths (OUTPUT_DIR/<lang_code>/...) and create necessary subdirectories. Implement methods to generate YAML frontmatter for translated files (setting lang, orig: false, including translated fields, copying others, adding source hashes). Write the reconstructed Markdown with frontmatter to output files.",
      "testStrategy": "Process test files and verify output files are created with correct paths and content. Confirm directory structure matches expectations. Verify YAML frontmatter in output files contains all required fields.",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend FileManager to construct output paths",
          "description": "Add methods to FileManager to construct output file paths based on language code and original file path",
          "status": "done",
          "dependencies": [],
          "details": "Create a method `get_output_path(file_path, lang_code)` that takes an original file path and language code as inputs and returns the corresponding output path in the format OUTPUT_DIR/<lang_code>/.... The method should handle both absolute and relative paths correctly. Define OUTPUT_DIR as a configurable constant in the configuration file.\n\n<info added on 2025-05-04T19:16:32.526Z>\nThe implementation should handle path normalization and validation:\n\n```python\ndef get_output_path(self, file_path, lang_code):\n    \"\"\"\n    Constructs output file path based on language code and original file path.\n    \n    Args:\n        file_path (str or Path): Original file path (absolute or relative)\n        lang_code (str): Target language code (e.g., 'es', 'fr')\n        \n    Returns:\n        Path: Absolute path to the output file\n        \n    Raises:\n        ValueError: If file_path is not within input_dir\n    \"\"\"\n    file_path = Path(file_path).resolve()\n    \n    # Verify file is within input directory to prevent path traversal\n    try:\n        relative_path = file_path.relative_to(self.input_dir)\n    except ValueError:\n        raise ValueError(f\"File {file_path} is not within input directory {self.input_dir}\")\n    \n    # Construct output path\n    output_path = self.output_dir / lang_code / relative_path\n    \n    # Ensure parent directories exist\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    \n    return output_path\n```\n\nAlso add a method to create all necessary output directories for a set of language codes:\n\n```python\ndef prepare_output_directories(self, lang_codes):\n    \"\"\"\n    Creates output directories for all specified language codes.\n    \n    Args:\n        lang_codes (list): List of language codes to prepare directories for\n    \"\"\"\n    for lang_code in lang_codes:\n        (self.output_dir / lang_code).mkdir(parents=True, exist_ok=True)\n```\n</info added on 2025-05-04T19:16:32.526Z>"
        },
        {
          "id": 2,
          "title": "Implement directory creation functionality",
          "description": "Add methods to create necessary subdirectories for output files",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Create a method `ensure_output_directory(output_path)` that checks if the directory for an output file exists and creates it if necessary. This should handle nested directories and ensure proper permissions. Use the output path construction from subtask 1 to determine which directories need to be created.\n\n<info added on 2025-05-04T19:16:56.154Z>\nThe `ensure_output_directory` method should be implemented in the file_manager.py module with proper error handling and logging. Here's the implementation approach:\n\n```python\ndef ensure_output_directory(output_path: Path) -> bool:\n    \"\"\"\n    Ensures the directory for the given output path exists, creating it if necessary.\n    \n    Args:\n        output_path: Path object representing the intended output file\n        \n    Returns:\n        bool: True if directory exists or was created successfully, False otherwise\n        \n    Raises:\n        OSError: If directory creation fails due to permissions or other system issues\n    \"\"\"\n    try:\n        # Get the parent directory of the output file\n        directory = output_path.parent\n        \n        # Create directory and any missing parents with appropriate permissions\n        directory.mkdir(parents=True, exist_ok=True)\n        \n        # Verify directory exists and has write permissions\n        if not directory.exists():\n            logger.error(f\"Failed to create directory: {directory}\")\n            return False\n            \n        if not os.access(directory, os.W_OK):\n            logger.warning(f\"Directory exists but may not be writable: {directory}\")\n            \n        logger.debug(f\"Ensured directory exists: {directory}\")\n        return True\n        \n    except OSError as e:\n        logger.error(f\"Error creating directory for {output_path}: {str(e)}\")\n        raise\n```\n\nThis implementation includes permission checking after directory creation and proper logging at different severity levels based on the outcome.\n</info added on 2025-05-04T19:16:56.154Z>"
        },
        {
          "id": 3,
          "title": "Implement YAML frontmatter generation",
          "description": "Create functionality to generate YAML frontmatter for translated files",
          "status": "done",
          "dependencies": [],
          "details": "Create a method `generate_frontmatter(original_frontmatter, translated_fields, lang_code, source_hash)` that takes the original frontmatter, translated fields, language code, and source hash as inputs. The method should set 'lang' to the language code, 'orig' to false, include all translated fields, copy other fields from the original frontmatter, and add the source hash for tracking changes. Return the frontmatter as a dictionary that can be serialized to YAML.\n\n<info added on 2025-05-04T19:17:59.180Z>\nThe `generate_frontmatter` method should handle several edge cases:\n\n1. If `original_frontmatter` is None, initialize an empty dictionary\n2. Create a deep copy of the original frontmatter to avoid modifying the original\n3. Handle potential key conflicts between original frontmatter and translated fields\n4. Implement special handling for nested frontmatter structures\n\nImplementation example:\n```python\n@staticmethod\ndef generate_frontmatter(original_frontmatter, translated_fields, lang_code, source_hash=None):\n    \"\"\"\n    Generate frontmatter for translated files.\n    \n    Args:\n        original_frontmatter (dict or None): Original frontmatter dictionary\n        translated_fields (dict): Dictionary of translated fields\n        lang_code (str): Language code for the translation\n        source_hash (str, optional): Hash of source content for tracking changes\n        \n    Returns:\n        dict: New frontmatter dictionary\n    \"\"\"\n    # Initialize with empty dict if None provided\n    result = copy.deepcopy(original_frontmatter or {})\n    \n    # Set required translation metadata\n    result['lang'] = lang_code\n    result['orig'] = False\n    \n    # Add source hash if provided\n    if source_hash:\n        result['source_hash'] = source_hash\n    \n    # Add all translated fields, overriding any existing fields\n    for key, value in translated_fields.items():\n        result[key] = value\n        \n    return result\n```\n\nInclude unit tests that verify correct behavior with various input combinations, including empty frontmatter, nested structures, and edge cases.\n</info added on 2025-05-04T19:17:59.180Z>"
        },
        {
          "id": 4,
          "title": "Implement file writing functionality",
          "description": "Create methods to write translated content with frontmatter to output files",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "Create a method `write_translated_file(output_path, frontmatter, content)` that takes an output path, frontmatter dictionary, and translated content as inputs. The method should convert the frontmatter to YAML format, combine it with the content using proper delimiters (---), and write the result to the specified output path. Ensure proper error handling for file writing operations."
        },
        {
          "id": 5,
          "title": "Integrate output generation into translation workflow",
          "description": "Connect the output file generation to the translation process",
          "status": "done",
          "dependencies": [
            1,
            4
          ],
          "details": "Update the translation workflow to call the output file generation methods after translation is complete. For each translated file, construct the output path, ensure the directory exists, generate the frontmatter, and write the file. Add logging to track successful file generation and any errors that occur. Include a summary of generated files at the end of the translation process."
        }
      ]
    },
    {
      "id": 12,
      "title": "Implement source file hash update",
      "description": "Create functionality to update content_hash and yaml_hash in source files after processing.",
      "status": "done",
      "dependencies": [
        5,
        11
      ],
      "priority": "high",
      "details": "Extend FileManager to update the content_hash and yaml_hash values in source file frontmatter after successful processing. Implement careful file writing to avoid data loss. Include error handling for file write failures.",
      "testStrategy": "Process test files and verify source files are updated with new hash values. Modify content, process again, and confirm hashes are updated. Test error handling by simulating write failures.",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend FileManager with hash update method",
          "description": "Add a new method to the FileManager class that will handle updating content_hash and yaml_hash in source file frontmatter",
          "status": "done",
          "dependencies": [],
          "details": "Create a new method in FileManager called updateFileHashes(filePath, contentHash, yamlHash) that will be responsible for updating the hash values in a file's frontmatter. This method should accept the file path and the new hash values as parameters. Define the method signature and structure with appropriate documentation, but leave the implementation empty for now."
        },
        {
          "id": 2,
          "title": "Implement frontmatter parsing and modification",
          "description": "Create functionality to parse existing frontmatter, update hash values, and prepare the modified content",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Implement the core logic of the updateFileHashes method to: 1) Read the file content, 2) Parse the YAML frontmatter section, 3) Update the content_hash and yaml_hash values with the new values, 4) Reconstruct the file content with the updated frontmatter. Use existing YAML parsing libraries to handle the frontmatter. Ensure the formatting of the frontmatter is preserved as much as possible."
        },
        {
          "id": 3,
          "title": "Implement safe file writing mechanism",
          "description": "Create a safe file writing process that prevents data loss during updates",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Enhance the updateFileHashes method with a safe file writing mechanism that: 1) Creates a temporary backup of the original file, 2) Writes the modified content to a temporary file, 3) Verifies the write was successful, 4) Replaces the original file with the temporary file using atomic operations when possible. This approach ensures that if any part of the process fails, the original file remains intact."
        },
        {
          "id": 4,
          "title": "Add comprehensive error handling",
          "description": "Implement error handling for all potential failure points in the hash update process",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "Complete the updateFileHashes method by adding comprehensive error handling for: 1) File read errors, 2) YAML parsing errors, 3) File write errors, 4) File replacement errors. Each error case should be properly logged with meaningful error messages. The method should clean up any temporary files created during the process, even if an error occurs. Return appropriate success/failure status that can be used by calling code to determine if the update was successful."
        }
      ]
    },
    {
      "id": 13,
      "title": "Implement YAML field translation",
      "description": "Add support for translating specified YAML frontmatter fields.",
      "status": "done",
      "dependencies": [
        7,
        8
      ],
      "priority": "medium",
      "details": "Extend MarkdownProcessor to identify YAML fields listed in YAML_TRANSLATE_FIELDS configuration. Extract text from these fields for translation. Update the mapping structure to include YAML field locations. Modify the TranslationService to handle both body content and YAML field text.",
      "testStrategy": "Configure test files with translatable YAML fields. Process files and verify both body content and specified YAML fields are translated. Confirm non-specified YAML fields remain unchanged.",
      "subtasks": [
        {
          "id": 1,
          "title": "Add YAML_TRANSLATE_FIELDS configuration option",
          "description": "Create a configuration option that allows users to specify which YAML frontmatter fields should be translated",
          "status": "done",
          "dependencies": [],
          "details": "Add a new configuration option called YAML_TRANSLATE_FIELDS to the application's configuration system. This should be an array of strings representing field names in YAML frontmatter that should be translated. Update the configuration documentation to explain this new option. Implement validation to ensure the configuration value is an array of strings when provided."
        },
        {
          "id": 2,
          "title": "Extend MarkdownProcessor to identify translatable YAML fields",
          "description": "Modify the MarkdownProcessor to detect and extract YAML frontmatter fields that are marked for translation",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Update the MarkdownProcessor to parse YAML frontmatter at the beginning of markdown files. Add logic to check each field against the YAML_TRANSLATE_FIELDS configuration. For fields that match, extract their text content and store it separately from the main markdown body. Ensure the processor can handle various YAML data types (strings, arrays, nested objects) and extract translatable text appropriately."
        },
        {
          "id": 3,
          "title": "Update mapping structure to include YAML field locations",
          "description": "Extend the translation mapping structure to track the location and context of text from YAML fields",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Modify the existing mapping structure to distinguish between body content and YAML field content. For YAML fields, store additional metadata including the field name, path (for nested fields), and original position in the document. This mapping should allow the system to correctly replace translated content back into the appropriate YAML fields. Create a consistent format for identifying YAML field locations that can be used throughout the translation pipeline."
        },
        {
          "id": 4,
          "title": "Modify TranslationService to process YAML field text",
          "description": "Update the TranslationService to handle translation of both markdown body and YAML field content",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "Extend the TranslationService to process text from YAML fields alongside the main markdown content. Implement logic to handle different translation requirements for YAML fields (e.g., preserving formatting, handling special characters). Ensure the service can batch translate both types of content efficiently. Update any translation caching mechanisms to properly handle and identify YAML field content."
        },
        {
          "id": 5,
          "title": "Implement reassembly of translated YAML fields",
          "description": "Create functionality to reconstruct the markdown file with translated YAML fields and body content",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "Develop the logic to reassemble a complete markdown file after translation. Use the mapping structure to place translated YAML field content back into the correct fields in the frontmatter. Ensure the YAML structure and formatting is preserved during reassembly. Add validation to verify that the reconstructed document maintains the original structure with only the content changed. Test with various YAML structures including nested objects and arrays."
        }
      ]
    },
    {
      "id": 14,
      "title": "Implement change detection logic",
      "description": "Create logic to determine if translation/update is needed based on hash comparison.",
      "status": "done",
      "dependencies": [
        5,
        12
      ],
      "priority": "high",
      "details": "Implement logic to compare calculated hashes with stored hashes in source file frontmatter. Create decision logic to: 1) Trigger full translation if content_hash changes, 2) Trigger YAML-only update if only yaml_hash changes, 3) Skip file if hashes match. Include appropriate logging for each decision.",
      "testStrategy": "Create test scenarios for unchanged files, body content changes, and YAML-only changes. Verify the correct action is taken in each case. Confirm appropriate logging messages are generated.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create hash comparison function",
          "description": "Implement a function that compares calculated content and YAML hashes with stored hashes from frontmatter",
          "status": "done",
          "dependencies": [],
          "details": "Create a function that takes calculated content_hash and yaml_hash as inputs along with the existing frontmatter data. The function should extract stored hashes from the frontmatter and perform comparison between new and stored hashes. Return a comparison result object with boolean flags indicating if content_hash changed, yaml_hash changed, or both match. Handle edge cases where hashes might not exist in frontmatter yet."
        },
        {
          "id": 2,
          "title": "Implement decision logic for translation actions",
          "description": "Create a function that determines the required action based on hash comparison results",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Build a function that takes the comparison result from subtask 1 and determines the appropriate action: 1) Full translation if content_hash changed, 2) YAML-only update if only yaml_hash changed, 3) Skip file if both hashes match. The function should return an action type enum/string and any relevant metadata needed for the next steps in the workflow. Include validation to ensure the comparison result contains all required data."
        },
        {
          "id": 3,
          "title": "Add logging for change detection decisions",
          "description": "Implement detailed logging for each decision path in the change detection logic",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Enhance the decision logic function to include appropriate logging for each path: 1) Log when content has changed and full translation is needed, including the old and new hash values, 2) Log when only YAML data has changed and partial update is needed, 3) Log when file is skipped due to matching hashes, 4) Add debug logging for the comparison process itself. Use consistent log formatting and appropriate log levels (info for normal operation, debug for details, warn for potential issues)."
        },
        {
          "id": 4,
          "title": "Integrate change detection with workflow",
          "description": "Connect the change detection logic to the main translation workflow",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "Integrate the hash comparison and decision logic into the main workflow. Ensure the workflow calls the change detection functions at the appropriate time after hashes are calculated but before translation begins. Modify the workflow to branch based on the determined action: proceed with full translation, perform YAML-only update, or skip the file. Add error handling for unexpected comparison results or missing hash data. Test the integrated workflow with various scenarios to verify correct branching behavior."
        }
      ]
    },
    {
      "id": 15,
      "title": "Implement YAML-only update for existing translated files",
      "description": "Create functionality to update only the YAML frontmatter of existing translated files.",
      "status": "pending",
      "dependencies": [
        13,
        14
      ],
      "priority": "medium",
      "details": "Extend FileManager to handle the YAML-only update scenario. Implement methods to read existing translated files, update only their YAML frontmatter (with translated fields if needed), and rewrite the files. Ensure the body content remains unchanged during this process.",
      "testStrategy": "Create test scenarios with existing translated files and YAML-only changes in source files. Verify only the YAML frontmatter is updated in translated files. Confirm body content remains unchanged.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create YAML frontmatter parsing utility",
          "description": "Develop a utility function that can extract YAML frontmatter from markdown files while preserving the body content separately",
          "status": "pending",
          "dependencies": [],
          "details": "Implement a utility function that takes a file path as input and returns two separate components: the YAML frontmatter as a structured object and the body content as a string. The function should handle edge cases like missing frontmatter delimiters (---) and invalid YAML. Use existing YAML parsing libraries but create a wrapper that specifically handles the frontmatter extraction pattern in markdown files."
        },
        {
          "id": 2,
          "title": "Implement YAML frontmatter modification function",
          "description": "Create a function that can update specific fields in the YAML frontmatter without modifying others",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Build on the parsing utility to create a function that accepts the parsed YAML object, a set of fields to update with their new values, and returns the modified YAML object. This function should preserve all existing fields not specified in the update set. Include validation to ensure the updated YAML remains valid and properly formatted. The function should handle nested YAML structures and array fields appropriately."
        },
        {
          "id": 3,
          "title": "Develop file rewriting mechanism with content preservation",
          "description": "Create a function that can rewrite a file with updated YAML frontmatter while preserving the original body content",
          "status": "pending",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement a function that takes a file path, the updated YAML object, and the preserved body content, then writes them back to the file. The function should format the YAML properly with the correct delimiter markers (---), maintain any whitespace patterns between the frontmatter and content, and ensure the body content remains completely unchanged. Include error handling for file system operations and implement an atomic write pattern (write to temp file, then rename) to prevent data loss if the operation fails midway."
        },
        {
          "id": 4,
          "title": "Extend FileManager with YAML-only update method",
          "description": "Add a new method to the FileManager class that orchestrates the YAML-only update process",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Create a new method in the FileManager class named 'updateYamlOnly' that takes a file path and a set of YAML fields to update. This method should use the previously created utilities to: 1) read and parse the file, 2) update only the specified YAML fields, and 3) write the file back with the updated frontmatter and unchanged body. Include appropriate logging and error handling. The method should return a success/failure status and details about what was updated."
        },
        {
          "id": 5,
          "title": "Implement batch processing for multiple translated files",
          "description": "Create functionality to perform YAML-only updates across multiple translated files",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Extend the FileManager with a method that can process multiple files in batch. This method should accept a list of file paths and corresponding YAML updates for each, or a pattern to identify files and a common update to apply. Implement parallel processing with appropriate throttling to handle large numbers of files efficiently. Include comprehensive error handling that allows the process to continue even if individual files fail, with detailed reporting of successes and failures. Add transaction-like behavior where possible to ensure consistency across the batch operation."
        }
      ]
    },
    {
      "id": 16,
      "title": "Implement support for WikiLinks syntax",
      "description": "Add support for handling Obsidian-style WikiLinks during translation.",
      "status": "pending",
      "dependencies": [
        7,
        10
      ],
      "priority": "low",
      "details": "Extend MarkdownProcessor to recognize WikiLinks patterns ([[LinkTarget]] or [[LinkTarget|Alias]]). Implement special handling to translate only the Alias portion while preserving the LinkTarget. Update the AST traversal and text extraction logic to handle these patterns correctly.",
      "testStrategy": "Create test Markdown files with various WikiLinks patterns. Verify only the Alias portions are extracted for translation. Confirm reconstructed Markdown preserves the correct WikiLinks structure.",
      "subtasks": [
        {
          "id": 1,
          "title": "Add WikiLinks pattern recognition to MarkdownProcessor",
          "description": "Extend the MarkdownProcessor to identify and parse Obsidian-style WikiLinks patterns in markdown text",
          "status": "pending",
          "dependencies": [],
          "details": "Create regular expressions to match both WikiLinks formats ([[LinkTarget]] and [[LinkTarget|Alias]]). Implement a method to identify these patterns in the source text before AST parsing. This will serve as the foundation for special handling of these elements. Add unit tests to verify pattern recognition works correctly for various WikiLinks formats."
        },
        {
          "id": 2,
          "title": "Extend AST to represent WikiLinks nodes",
          "description": "Create specialized AST node types to represent WikiLinks with their components (target and optional alias)",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Define new AST node classes (e.g., WikiLinkNode) that can store both the link target and alias components separately. Ensure these nodes properly integrate with the existing AST structure. Implement methods to distinguish between the target (which should not be translated) and alias (which should be translated). Update the AST builder to create these specialized nodes when WikiLinks patterns are encountered."
        },
        {
          "id": 3,
          "title": "Implement text extraction for WikiLinks translation",
          "description": "Modify the text extraction logic to properly handle WikiLinks, extracting only the alias portion for translation",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "Update the text extraction visitor to recognize WikiLink nodes. For WikiLinks with aliases ([[LinkTarget|Alias]]), extract only the alias text for translation. For simple WikiLinks ([[LinkTarget]]), determine if the target should be left untranslated or if the displayed text should be translated. Add appropriate context markers in the extracted text to identify WikiLinks segments for reassembly later."
        },
        {
          "id": 4,
          "title": "Implement translation result reintegration for WikiLinks",
          "description": "Develop logic to correctly reintegrate translated text back into WikiLinks structure",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Create a specialized handler for WikiLinks during the translation reintegration phase. Ensure that only the alias portion is replaced with translated text while preserving the link target. Maintain the correct WikiLinks syntax ([[target|translated-alias]]) in the final output. Handle edge cases such as nested delimiters and escaped characters within WikiLinks."
        },
        {
          "id": 5,
          "title": "Add comprehensive testing and documentation for WikiLinks support",
          "description": "Create tests and documentation for the WikiLinks translation functionality",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Develop integration tests that verify the entire WikiLinks translation workflow from recognition to final output. Test various scenarios including simple links, links with aliases, links with special characters, and nested structures. Document the WikiLinks support in the user guide, explaining how the feature works and any limitations. Update developer documentation to explain the implementation approach and how to extend it if needed."
        }
      ]
    },
    {
      "id": 17,
      "title": "Implement comprehensive error handling",
      "description": "Add robust error handling throughout the application.",
      "status": "pending",
      "dependencies": [
        11,
        12,
        15
      ],
      "priority": "medium",
      "details": "Review all components and add comprehensive error handling. Include specific handling for: configuration errors, file access issues, parsing errors, API errors (with retries), and file writing failures. Implement a consistent error reporting mechanism. Add recovery options where possible to continue processing other files when one fails.",
      "testStrategy": "Create test scenarios that trigger various error conditions. Verify errors are caught, reported clearly, and handled appropriately. Confirm the application can continue processing other files when possible.",
      "subtasks": [
        {
          "id": 1,
          "title": "Design error handling architecture",
          "description": "Create a consistent error handling architecture and reporting mechanism",
          "status": "pending",
          "dependencies": [],
          "details": "Define error types/categories (configuration, file access, parsing, API, file writing). Create a centralized error handling utility with standardized error objects. Design error logging format with severity levels, timestamps, context data, and stack traces. Define how errors propagate through the application. Document the architecture for team reference."
        },
        {
          "id": 2,
          "title": "Implement configuration and startup error handling",
          "description": "Add error handling for application configuration and startup processes",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Implement validation for configuration files/parameters with descriptive error messages. Add try-catch blocks around startup processes. Create fallback mechanisms for missing/invalid configurations. Implement graceful shutdown procedures for critical startup failures. Add detailed logging for configuration-related errors."
        },
        {
          "id": 3,
          "title": "Implement file access and parsing error handling",
          "description": "Add error handling for file operations and data parsing",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Add try-catch blocks around file read/write operations. Implement specific error types for file not found, permission issues, and corruption. Create recovery mechanisms to skip problematic files and continue processing others. Add validation for parsed data with appropriate error messages. Implement retry logic for temporary file access issues."
        },
        {
          "id": 4,
          "title": "Implement API error handling with retry mechanism",
          "description": "Add robust error handling for all API interactions",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Implement exponential backoff retry logic for API calls. Handle different HTTP status codes appropriately. Add timeout handling for API requests. Create circuit breaker pattern to prevent cascading failures. Implement fallback mechanisms when APIs are unavailable. Add detailed logging of API errors with request/response information."
        },
        {
          "id": 5,
          "title": "Implement UI error feedback mechanisms",
          "description": "Create user-friendly error messages and recovery options in the UI",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Design user-friendly error messages that avoid technical jargon. Implement toast notifications for non-critical errors. Create modal dialogs for critical errors requiring user action. Add retry buttons for failed operations where appropriate. Implement error boundaries in UI components to prevent complete UI crashes. Create an error details expansion option for technical users."
        },
        {
          "id": 6,
          "title": "Implement global error monitoring and reporting",
          "description": "Create a system to monitor, aggregate and report application errors",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Implement a global error handler to catch unhandled exceptions. Create an error dashboard showing error frequency and patterns. Add automated alerts for critical or frequent errors. Implement error aggregation to group similar errors. Create error reports with filtering options. Add telemetry to track error resolution times and effectiveness of error handling."
        }
      ]
    },
    {
      "id": 18,
      "title": "Implement detailed logging",
      "description": "Add comprehensive logging throughout the application.",
      "status": "pending",
      "dependencies": [
        17
      ],
      "priority": "medium",
      "details": "Implement detailed logging using Python's logging module. Include information about: files found, files skipped (with reason), files processed, translation status, files written, and errors encountered. Add configuration options for log level. Ensure logs are useful for debugging and monitoring progress.",
      "testStrategy": "Run the application with various log levels and verify appropriate information is logged. Create test scenarios that trigger different log messages. Confirm logs provide useful information for debugging issues.",
      "subtasks": [
        {
          "id": 1,
          "title": "Set up basic logging infrastructure",
          "description": "Create a logging configuration system that allows for different log levels and output destinations",
          "status": "pending",
          "dependencies": [],
          "details": "Create a logging module that initializes Python's logging system. Implement configuration options for log levels (DEBUG, INFO, WARNING, ERROR, CRITICAL) that can be set via command line arguments or configuration file. Set up appropriate log formatters that include timestamp, log level, and message. Configure log output destinations (console, file, or both). Create helper functions that other parts of the application can use to get properly configured logger instances."
        },
        {
          "id": 2,
          "title": "Implement file discovery and selection logging",
          "description": "Add logging for file discovery process and selection criteria",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Add logging statements to the file discovery and selection components. Log at INFO level when scanning directories. Log at DEBUG level for each file found. Log at INFO level for files skipped with the specific reason (e.g., 'file already translated', 'file type not supported'). Include file counts in summary logs. Ensure all potential error conditions during file discovery are logged at WARNING or ERROR level as appropriate."
        },
        {
          "id": 3,
          "title": "Implement translation process logging",
          "description": "Add detailed logging for the translation process",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Add logging statements throughout the translation process. Log at INFO level when translation begins and ends for each file. Log at DEBUG level for detailed translation steps (e.g., 'extracting text', 'sending to translation API', 'received translation'). Log translation statistics at INFO level (e.g., number of segments translated, characters processed). Log any translation errors or warnings at appropriate levels. Include performance metrics such as translation time per file at DEBUG level."
        },
        {
          "id": 4,
          "title": "Implement output and error handling logging",
          "description": "Add logging for file writing operations and comprehensive error handling",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Add logging for all file writing operations. Log at INFO level when writing output files with destination paths. Log at DEBUG level for details about file content being written. Implement comprehensive error logging throughout the application, ensuring all exceptions are caught and logged with appropriate context. Create a system for aggregating errors and providing summary logs at the end of processing. Add logging for application startup and shutdown, including configuration settings at startup (at INFO level) and summary statistics at shutdown (number of files processed, success rate, etc.)."
        }
      ]
    },
    {
      "id": 19,
      "title": "Implement extensible translation provider system",
      "description": "Refactor TranslationService to support multiple translation providers.",
      "status": "pending",
      "dependencies": [
        8,
        9
      ],
      "priority": "low",
      "details": "Refactor TranslationService to use a provider pattern. Create an abstract base class or interface for translation providers. Implement the DeepL provider as the initial concrete implementation. Add factory method to create the appropriate provider based on API_PROVIDER configuration. Design for easy addition of new providers in the future.",
      "testStrategy": "Verify the DeepL provider works correctly through the new abstraction. Create a simple mock provider for testing. Confirm the factory correctly instantiates providers based on configuration.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create ITranslationProvider interface",
          "description": "Define the interface that all translation providers must implement",
          "status": "pending",
          "dependencies": [],
          "details": "Create an interface named ITranslationProvider that defines the contract for all translation providers. Include methods for translation (e.g., translateText, translateBatch), language detection, and any other common functionality. Add documentation to each method explaining its purpose, parameters, and return values. Consider adding methods for provider initialization and validation of API credentials."
        },
        {
          "id": 2,
          "title": "Implement DeepL provider",
          "description": "Create the first concrete implementation of ITranslationProvider for DeepL",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Create a DeepLTranslationProvider class that implements the ITranslationProvider interface. Move the existing DeepL-specific code from TranslationService into this new class. Ensure all interface methods are properly implemented. Handle API key configuration, request formatting, error handling, and response parsing specific to DeepL. Add unit tests to verify the implementation works correctly."
        },
        {
          "id": 3,
          "title": "Create TranslationProviderFactory",
          "description": "Implement a factory class to instantiate the appropriate provider",
          "status": "pending",
          "dependencies": [
            1,
            2
          ],
          "details": "Create a TranslationProviderFactory class with a createProvider method that returns an instance of ITranslationProvider based on configuration. Read the API_PROVIDER configuration value to determine which provider to instantiate. Initially support only the DeepL provider, but design the factory to be easily extended with additional providers. Include error handling for unknown or misconfigured providers. Consider implementing a singleton pattern if appropriate for your architecture."
        },
        {
          "id": 4,
          "title": "Refactor TranslationService",
          "description": "Update TranslationService to use the provider pattern",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Refactor the existing TranslationService to use the provider pattern. Remove DeepL-specific code that was moved to DeepLTranslationProvider. Update the service to get a provider instance from TranslationProviderFactory and delegate translation operations to it. Maintain the same public API to ensure backward compatibility with existing code. Update any configuration handling to work with the new provider system. Add appropriate error handling for provider failures."
        },
        {
          "id": 5,
          "title": "Add configuration and documentation",
          "description": "Update configuration files and add documentation for the new provider system",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Update configuration files to support the new provider system. Add documentation explaining how to configure different translation providers. Create a guide for implementing new providers, including a template or example. Update existing documentation to reflect the changes. Add comments in the code explaining the provider pattern implementation. Consider creating a simple mock provider for testing purposes. Verify that the existing functionality works with the new implementation through integration tests."
        }
      ]
    },
    {
      "id": 20,
      "title": "Implement main application workflow",
      "description": "Create the main application workflow that orchestrates all components.",
      "status": "pending",
      "dependencies": [
        14,
        15,
        17,
        18
      ],
      "priority": "high",
      "details": "Implement the main application class that orchestrates the entire workflow: load configuration, scan directories, process each file (check eligibility, calculate hashes, determine if translation is needed, extract content, translate, reconstruct, write output, update source hashes). Include progress reporting and summary statistics. Create a clean CLI interface with appropriate help text.",
      "testStrategy": "Create end-to-end tests with various scenarios (initial run, no changes, content changes, YAML changes). Verify the entire workflow functions correctly in each scenario. Test the CLI interface with various arguments and options.",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement configuration loading and validation",
          "description": "Create the initial part of the main application class that handles loading and validating configuration settings",
          "status": "pending",
          "dependencies": [],
          "details": "Implement a method to load configuration from command line arguments, environment variables, and/or config files. Include validation logic to ensure all required settings are present and valid. Handle configuration errors gracefully with informative error messages. This should include settings for source/target directories, language pairs, file types to process, and any API credentials needed for translation services."
        },
        {
          "id": 2,
          "title": "Implement directory scanning and file collection",
          "description": "Create functionality to scan directories and collect eligible files for processing",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Using the validated configuration, implement methods to recursively scan source directories and identify files that need processing. Apply file type filters based on configuration. Create a data structure to track files for processing with their metadata (path, size, last modified date). Include logic to skip files that should be excluded based on configuration rules (e.g., hidden files, specific patterns)."
        },
        {
          "id": 3,
          "title": "Implement file eligibility and hash calculation",
          "description": "Add logic to determine which files need translation by checking eligibility and calculating hashes",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "For each collected file, implement the logic to: 1) Calculate file hash, 2) Check against stored hashes to determine if translation is needed, 3) Create a prioritized queue of files that need processing. Include logic to handle file access errors gracefully. This component should prepare the final list of files that will proceed to content extraction and translation."
        },
        {
          "id": 4,
          "title": "Implement the core processing pipeline",
          "description": "Create the main processing loop that handles content extraction, translation, and reconstruction",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Implement the core processing pipeline that: 1) Extracts translatable content from each file, 2) Sends content for translation, 3) Reconstructs the translated file with the original formatting preserved, 4) Writes the output to the target location. This should handle different file types appropriately by delegating to the correct extractor/translator components. Include proper error handling for each step."
        },
        {
          "id": 5,
          "title": "Implement progress reporting and statistics",
          "description": "Add functionality to track and report progress during processing",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Enhance the processing pipeline with progress reporting. Implement: 1) Real-time progress indicators (percentage complete, files processed/remaining), 2) Logging of important events and errors, 3) Collection of statistics (files processed, translation volume, errors encountered, time taken). Design this to work in both interactive terminal environments and non-interactive contexts (like CI/CD pipelines)."
        },
        {
          "id": 6,
          "title": "Implement CLI interface and help documentation",
          "description": "Create a user-friendly command-line interface with comprehensive help text",
          "status": "pending",
          "dependencies": [
            5
          ],
          "details": "Finalize the application by implementing a clean CLI interface. Include: 1) Command-line argument parsing with sensible defaults, 2) Comprehensive help text explaining all options and their usage, 3) Examples of common use cases, 4) Version information and attribution. Ensure the interface follows CLI best practices (e.g., supporting both short and long option formats, providing clear error messages for invalid inputs)."
        }
      ]
    },
    {
      "id": 21,
      "title": "Implement Robust HTML Parsing and Reassembly in MarkdownProcessor",
      "description": "Enhance the MarkdownProcessor to properly parse HTML content within Markdown documents, extract text from allowed HTML tags, and correctly reassemble translated content while preserving structure.",
      "details": "Implement a robust HTML parsing and reassembly system in the MarkdownProcessor class with the following components:\n\n1. HTML Parser Integration:\n   - Integrate a reliable HTML parsing library (BeautifulSoup or html.parser) to identify and extract HTML blocks within Markdown content\n   - Create a configurable HTML tag whitelist system that specifies which tags and attributes should be processed\n   - Implement proper handling of nested HTML structures\n\n2. Text Extraction Strategy:\n   - Develop a configurable strategy to extract text content from specific allowed HTML tags (e.g., <p>, <h1>-<h6>, <li>, <td>, <th>)\n   - Extract text from relevant attributes of specific tags (e.g., alt from <img>, title from <a>)\n   - Preserve the original HTML structure, including all attributes and non-text content\n   - Handle special cases like HTML entities and CDATA sections correctly\n\n3. HTML Reassembly:\n   - Create a system to correctly reintegrate translated text back into the original HTML structure\n   - Ensure all original attributes, tag hierarchies, and non-translatable elements remain intact\n   - Implement proper escaping to prevent HTML injection or malformation\n   - Maintain original whitespace and formatting where appropriate\n\n4. Configuration System:\n   - Create a configuration interface that allows users to specify:\n     * Which HTML tags should have their text content extracted for translation\n     * Which attributes of which tags should be extracted for translation\n     * Any tags that should be completely ignored/preserved as-is\n\n5. Error Handling:\n   - Implement robust error handling for malformed HTML\n   - Add detailed logging for parsing and reassembly operations\n   - Ensure the system degrades gracefully when encountering unexpected HTML structures",
      "testStrategy": "1. Unit Tests:\n   - Test HTML parsing with various HTML structures (simple tags, nested tags, self-closing tags)\n   - Test extraction of text from different allowed tags and attributes\n   - Test reassembly with different complexity levels of HTML\n   - Test handling of malformed HTML input\n   - Test with HTML entities and special characters\n\n2. Integration Tests:\n   - Test the complete workflow from Markdown with embedded HTML through parsing, extraction, translation, and reassembly\n   - Verify that the original structure is preserved after processing\n   - Test with real-world examples of Markdown documents containing HTML\n\n3. Edge Cases:\n   - Test with extremely large HTML blocks\n   - Test with uncommon but valid HTML structures\n   - Test with HTML containing mixed language content\n   - Test with HTML containing code samples or other non-translatable content\n\n4. Validation Methods:\n   - Compare the DOM structure before and after processing to ensure they match\n   - Verify all attributes are preserved exactly as in the original\n   - Confirm that only the text content from allowed tags/attributes is modified\n   - Use HTML validators to ensure the output remains valid HTML\n\n5. Performance Testing:\n   - Measure and establish performance benchmarks for processing documents with varying amounts of HTML content\n   - Ensure memory usage remains reasonable with large documents",
      "status": "done",
      "dependencies": [
        7
      ],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Integrate Beautiful Soup for HTML Parsing in MarkdownProcessor",
          "description": "Add Beautiful Soup library to the project and implement core HTML parsing functionality within the MarkdownProcessor class to identify and extract HTML blocks from Markdown content.",
          "dependencies": [],
          "details": "Implementation steps:\n1. Add Beautiful Soup as a project dependency\n2. Create a new method `_parse_html(html_content)` in MarkdownProcessor that uses Beautiful Soup to parse HTML strings\n3. Implement a method to identify HTML blocks within Markdown content using regex patterns\n4. Create a method to extract HTML blocks from Markdown for separate processing\n5. Implement unit tests for HTML block identification and extraction\n\nTesting approach:\n- Create test cases with various HTML blocks embedded in Markdown\n- Verify correct identification and extraction of HTML content\n- Test edge cases like HTML comments, script tags, and malformed HTML\n\n<info added on 2025-05-04T20:33:27.480Z>\nHere's additional information for the subtask:\n\n```python\n# Implementation details:\n\n## Beautiful Soup Integration\n- Add to requirements.txt: `beautifulsoup4>=4.12.2` and `lxml>=4.9.3` (as the HTML parser backend)\n- Import in MarkdownProcessor: `from bs4 import BeautifulSoup`\n\n## HTML Processing Methods\ndef _parse_html(self, html_content):\n    \"\"\"\n    Parse HTML content using Beautiful Soup.\n    \n    Args:\n        html_content (str): Raw HTML string\n        \n    Returns:\n        BeautifulSoup: Parsed HTML object\n    \"\"\"\n    return BeautifulSoup(html_content, 'lxml')\n\ndef _identify_html_blocks(self, markdown_content):\n    \"\"\"\n    Use regex patterns to identify HTML blocks:\n    - HTML blocks starting with opening tags like <div>, <table>, etc.\n    - HTML blocks with proper closing tags\n    - Self-closing tags\n    \n    Regex pattern example: r'<([a-z][a-z0-9]*)\\b[^>]*>(.*?)</\\1>'\n    \"\"\"\n\n## Integration with markdown-it-py\n- Use the renderer extension mechanism of markdown-it-py\n- Register custom token handlers for 'html_block' and 'html_inline' token types\n- Example hook: `md.add_render_rule('html_block', self._handle_html_block)`\n\n## Error handling\n- Implement try/except blocks around BeautifulSoup parsing\n- Add fallback rendering for malformed HTML\n- Log parsing errors with context information\n```\n</info added on 2025-05-04T20:33:27.480Z>",
          "status": "done",
          "parentTaskId": 21
        },
        {
          "id": 2,
          "title": "Implement Configurable HTML Tag and Attribute Whitelist System",
          "description": "Create a configuration system that allows users to specify which HTML tags and attributes should be processed for translation, preserved as-is, or ignored completely.",
          "dependencies": [
            1
          ],
          "details": "Implementation steps:\n1. Define a default configuration dictionary with whitelisted tags and attributes\n2. Create a `HTMLProcessingConfig` class to manage HTML processing rules\n3. Implement methods to add/remove tags and attributes from whitelist\n4. Add configuration loading from external JSON/YAML files\n5. Integrate configuration with the MarkdownProcessor class\n\nTesting approach:\n- Test default configuration behavior\n- Verify custom configuration loading\n- Test tag and attribute filtering based on configuration\n- Ensure configuration changes are properly applied to processing",
          "status": "done",
          "parentTaskId": 21
        },
        {
          "id": 3,
          "title": "Develop Text Extraction Strategy for HTML Content",
          "description": "Create a system to extract translatable text from HTML tags and attributes based on the whitelist configuration while preserving the original HTML structure.",
          "dependencies": [
            1,
            2
          ],
          "details": "Implementation steps:\n1. Implement a `_extract_text_from_html(soup, config)` method that traverses the Beautiful Soup parse tree\n2. Extract text content from whitelisted tags while preserving their positions\n3. Extract text from specified attributes (e.g., alt, title) based on configuration\n4. Create a data structure to track the original positions of extracted text\n5. Handle special cases like HTML entities and CDATA sections\n\nTesting approach:\n- Test extraction from various HTML structures\n- Verify correct handling of nested tags\n- Test attribute text extraction\n- Ensure HTML entities are properly handled\n- Validate position tracking for reassembly",
          "status": "done",
          "parentTaskId": 21
        },
        {
          "id": 4,
          "title": "Implement HTML Reassembly with Translated Content",
          "description": "Create a system to reintegrate translated text back into the original HTML structure while preserving all attributes, tag hierarchies, and non-translatable elements.",
          "dependencies": [
            3
          ],
          "details": "Implementation steps:\n1. Implement a `_reassemble_html(original_soup, translated_texts, positions)` method\n2. Create a mechanism to replace text nodes in the Beautiful Soup tree with translated content\n3. Implement attribute value replacement for translated attribute content\n4. Add proper escaping to prevent HTML injection\n5. Ensure whitespace and formatting preservation\n\nTesting approach:\n- Test reassembly with various HTML structures\n- Verify attribute values are correctly updated\n- Test with special characters and HTML entities\n- Ensure tag hierarchy and structure is preserved\n- Validate proper escaping of translated content",
          "status": "done",
          "parentTaskId": 21
        },
        {
          "id": 5,
          "title": "Add Error Handling and Integration with Markdown Processing",
          "description": "Implement robust error handling for HTML parsing and integrate the HTML processing system with the existing Markdown translation workflow.",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Implementation steps:\n1. Implement try-except blocks around HTML parsing operations\n2. Add detailed logging for parsing and reassembly operations\n3. Create fallback mechanisms for handling malformed HTML\n4. Integrate HTML processing into the main Markdown translation workflow\n5. Update the MarkdownProcessor.process() method to handle HTML blocks\n\nTesting approach:\n- Test with malformed HTML to verify graceful degradation\n- Verify logging of parsing errors\n- Test end-to-end Markdown processing with embedded HTML\n- Ensure correct integration with existing Markdown translation\n- Validate performance with large documents containing HTML",
          "status": "done",
          "parentTaskId": 21
        }
      ]
    }
  ],
  "metadata": {
    "projectName": "Markdown Translation Tool",
    "totalTasks": 20,
    "sourceFile": "scripts/prd.txt",
    "generatedAt": "2023-11-10"
  }
}