{
  "tasks": [
    {
      "id": 1,
      "title": "Set up project structure and dependencies",
      "description": "Create the initial project structure for the translation tool within the 'translation-py' directory, define required dependencies, and set up configuration file templates.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "Create a Python project structure within a 'translation-py' folder. Add requirements.txt with dependencies: python-dotenv, ruamel.yaml, markdown-it-py, requests/httpx, hashlib, pathlib. Create template files for settings.txt and translate.env. Set up basic CLI entry point with argparse.",
      "testStrategy": "Verify project structure is correct within 'translation-py'. Ensure all dependencies can be installed with pip. Confirm template files are created with appropriate placeholders.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create basic project directory structure within translation-py",
          "description": "Set up the initial directory structure for the Python project following standard practices within the 'translation-py' folder.",
          "status": "done",
          "dependencies": [],
          "details": "Create the following directory structure inside the 'translation-py' folder:\n- translation-py/\n  - src/\n    - __init__.py\n  - tests/\n    - __init__.py\n  - docs/\n  - config/\n  - README.md\n  - requirements.txt\n  - .gitignore\n\nEnsure the .gitignore includes standard Python patterns (pycache, virtual environments, etc.). Create empty __init__.py files in appropriate directories to make them proper Python packages. Ensure this structure is created relative to the main project root.\n\n<info added on 2025-05-03T23:28:06.807Z>\nFor the .gitignore file, I've included standard Python patterns such as:\n```\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nenv/\nvenv/\nENV/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n.pytest_cache/\n.coverage\nhtmlcov/\n```\n\nI've created a basic README.md with sections for:\n- Project overview\n- Installation instructions\n- Usage examples\n- Development setup\n\nThe requirements.txt file includes initial dependencies:\n```\npytest>=7.0.0\nblack>=23.0.0\nisort>=5.12.0\n```\n\nI've also added a src/translation/__init__.py file with version information:\n```python\n__version__ = \"0.1.0\"\n```\n\nAnd created a basic conftest.py in the tests directory to prepare for future testing.\n</info added on 2025-05-03T23:28:06.807Z>"
        },
        {
          "id": 2,
          "title": "Define project dependencies and create requirements.txt",
          "description": "Create a requirements.txt file with all necessary dependencies for the project within the 'translation-py' directory.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Create a requirements.txt file inside the 'translation-py' directory with the following dependencies:\n\npython-dotenv>=1.0.0\nruamel.yaml>=0.17.21\nmarkdown-it-py>=2.2.0\nhttpx>=0.24.0\nrequests>=2.28.2\nhashlib\npathlib\n\nSpecify minimum versions for each dependency to ensure compatibility. Include comments explaining what each dependency is used for. Consider separating dev dependencies (like testing libraries) into a requirements-dev.txt file within 'translation-py'.\n\n<info added on 2025-05-03T23:28:33.060Z>\nHere's the content for the requirements.txt file:\n\n```\n# Environment variable management\npython-dotenv>=1.0.0\n\n# YAML parsing and manipulation\nruamel.yaml>=0.17.21\n\n# Markdown parsing\nmarkdown-it-py>=2.2.0\n\n# HTTP client for API requests (modern alternative to requests)\nhttpx>=0.24.0\n\n# HTTP client for API requests (widely used)\nrequests>=2.28.2\n\n# Note: hashlib and pathlib are built-in Python libraries and don't need to be included in requirements.txt\n```\n\nYou may also want to create a requirements-dev.txt file with:\n\n```\n# Testing\npytest>=7.3.1\npytest-cov>=4.1.0\n\n# Linting\nflake8>=6.0.0\nblack>=23.3.0\n\n# Type checking\nmypy>=1.3.0\n```\n\nRemember to run `pip install -r requirements.txt` to install the dependencies, and `pip install -r requirements-dev.txt` for development dependencies.\n</info added on 2025-05-03T23:28:33.060Z>"
        },
        {
          "id": 3,
          "title": "Create configuration file templates",
          "description": "Set up template files for settings.txt and translate.env within the 'translation-py/config' directory.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "In the 'translation-py/config/' directory, create:\n\n1. settings.txt template with placeholders for common configuration options:\n```ini\n# Application Settings\nINPUT_DIR=../docs # Default relative to translation-py\nOUTPUT_DIR=../translated_docs # Default relative to translation-py\nTARGET_LANGUAGES=DE,FR,ES\nYAML_TRANSLATE_FIELDS=title,description\nAPI_PROVIDER=DeepL\nTEST_MODE=false\n# Add other configuration parameters with default values and comments\n```\n\n2. translate.env template for environment variables:\n```\n# API Keys\nDEEPL_API_KEY=your_api_key_here\n# Other keys...\n```\n\nInclude comments explaining each configuration option and how to set them. Ensure the default paths consider the location within 'translation-py'.\n\n<info added on 2025-05-03T23:29:02.799Z>\nI've created both template files with the following enhancements:\n\nFor settings.txt, I added comprehensive comments explaining each option:\n- INPUT_DIR/OUTPUT_DIR now include absolute path examples and usage notes\n- Added FILE_EXTENSIONS=.md,.txt,.html to control which files are processed\n- Added BATCH_SIZE=50 to control API request batching\n- Added LOG_LEVEL=INFO with options (DEBUG, INFO, WARNING, ERROR)\n- Added PRESERVE_FORMATTING=true to maintain markdown/HTML formatting\n- Added SKIP_EXISTING=false to control incremental translation behavior\n\nFor translate.env, I expanded it with:\n- Added all supported API providers (GOOGLE_CLOUD_KEY, MICROSOFT_TRANSLATOR_KEY)\n- Added PROXY_URL configuration for corporate environments\n- Added API_TIMEOUT=30 setting\n- Added comments explaining credential storage best practices\n- Added warning about not committing this file to version control\n\nBoth files include proper header documentation explaining their purpose and relationship to each other.\n</info added on 2025-05-03T23:29:02.799Z>"
        },
        {
          "id": 4,
          "title": "Set up CLI entry point structure",
          "description": "Create the main CLI entry point file within 'translation-py/src'.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Create a file named 'translation-py/src/cli.py' with the following structure:\n\n```python\n#!/usr/bin/env python3\nimport argparse\nimport sys\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Markdown Content Translator Tool')\n    \n    # Add common arguments\n    parser.add_argument('-s', '--settings', default='../config/settings.txt', help='Path to settings file (relative to script location)')\n    parser.add_argument('-e', '--env', default='../config/translate.env', help='Path to environment file (relative to script location)')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Enable verbose output')\n    \n    # Add subparsers if needed for different commands later\n    # subparsers = parser.add_subparsers(dest='command', help='Commands')\n    \n    return parser.parse_args()\n\ndef main():\n    args = parse_args()\n    # Main application logic will go here\n    print(f\"Arguments: {args}\")\n    # TODO: Load config using args.settings and args.env\n    \nif __name__ == \"__main__\":\n    sys.exit(main())\n```\n\nMake the file executable with appropriate permissions. Adjust default config/env paths to be relative to the script's expected location within 'translation-py/src'.\n\n<info added on 2025-05-03T23:31:50.504Z>\nTo enhance the CLI entry point structure, I'll add implementation details about handling configuration loading and error management:\n\n```python\n# Add these imports at the top\nimport os\nimport logging\nfrom pathlib import Path\n\n# Add a function to resolve paths relative to the script location\ndef resolve_path(relative_path):\n    \"\"\"Convert a path relative to the script location to an absolute path\"\"\"\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    return os.path.abspath(os.path.join(script_dir, relative_path))\n\n# Enhance the main function with proper logging setup and error handling\ndef main():\n    args = parse_args()\n    \n    # Configure logging based on verbosity\n    log_level = logging.DEBUG if args.verbose else logging.INFO\n    logging.basicConfig(\n        level=log_level,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    logger = logging.getLogger('translation-cli')\n    \n    try:\n        # Resolve config paths\n        settings_path = resolve_path(args.settings)\n        env_path = resolve_path(args.env)\n        \n        logger.debug(f\"Using settings file: {settings_path}\")\n        logger.debug(f\"Using environment file: {env_path}\")\n        \n        # Check if files exist\n        if not os.path.exists(settings_path):\n            logger.error(f\"Settings file not found: {settings_path}\")\n            return 1\n            \n        if not os.path.exists(env_path):\n            logger.error(f\"Environment file not found: {env_path}\")\n            return 1\n            \n        # TODO: Load config using settings_path and env_path\n        return 0\n    except Exception as e:\n        logger.exception(f\"Error in main execution: {str(e)}\")\n        return 1\n```\n\nAfter creating the file, make it executable with:\n```bash\nchmod +x translation-py/src/cli.py\n```\n</info added on 2025-05-03T23:31:50.504Z>"
        },
        {
          "id": 5,
          "title": "Create package entry point and setup.py",
          "description": "Set up the main package entry point and create setup.py for installation within the 'translation-py' directory.",
          "status": "done",
          "dependencies": [
            1,
            2,
            4
          ],
          "details": "1. Create a '__main__.py' file in the 'translation-py/src' directory:\n```python\nfrom .cli import main\n\nif __name__ == \"__main__\":\n    main()\n```\n\n2. Create 'setup.py' in the 'translation-py' directory:\n```python\nfrom setuptools import setup, find_packages\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\", encoding=\"utf-8\") as fh:\n    requirements = fh.read().splitlines()\n\nsetup(\n    name=\"markdown_translator\",\n    version=\"0.1.0\",\n    author=\"Your Name / Project Team\",\n    author_email=\"your.email@example.com\",\n    description=\"Tool to translate Markdown files using LLM APIs\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    # url=\"URL to project repo subsection if applicable\",\n    package_dir={'': 'src'},\n    packages=find_packages(where='src'),\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\", # Choose appropriate license\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.8\",\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"mdtranslate=src.cli:main\",\n        ],\n    },\n)\n```\n\nUpdate the package name, author details, and entry point command name. Ensure `package_dir` and `find_packages` point correctly to the `src` directory within `translation-py`."
        }
      ]
    },
    {
      "id": 2,
      "title": "Implement ConfigLoader component",
      "description": "Create a component to load and validate configuration from settings.txt and translate.env files.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "Implement a ConfigLoader class that reads settings.txt for INPUT_DIR, OUTPUT_DIR, TARGET_LANGUAGES, YAML_TRANSLATE_FIELDS, API_PROVIDER, TEST_MODE. Use python-dotenv to read translate.env for API keys (e.g., DEEPL_API_KEY). Include validation for required fields and appropriate error handling for missing or invalid configuration.",
      "testStrategy": "Create test configuration files with valid and invalid settings. Verify ConfigLoader correctly loads settings, validates required fields, and raises appropriate exceptions for invalid configurations.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create ConfigLoader class skeleton with file path handling",
          "description": "Set up the basic structure of the ConfigLoader class with initialization and file path handling for both configuration files.",
          "status": "done",
          "dependencies": [],
          "details": "Create a new file `config_loader.py` with a ConfigLoader class. Implement the `__init__` method that accepts optional paths to settings.txt and translate.env files (with defaults). Add methods to validate file existence and readability. Include basic exception handling for file access issues and implement logging setup.\n\n<info added on 2025-05-04T00:52:33.012Z>\n```python\n# Implementation details for config_loader.py\n\nimport os\nimport logging\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any\n\nclass ConfigError(Exception):\n    \"\"\"Base exception for configuration errors.\"\"\"\n    pass\n\nclass ConfigFileNotFoundError(ConfigError):\n    \"\"\"Raised when a configuration file cannot be found.\"\"\"\n    pass\n\nclass ConfigFileNotReadableError(ConfigError):\n    \"\"\"Raised when a configuration file exists but cannot be read.\"\"\"\n    pass\n\nclass ConfigLoader:\n    def __init__(self, settings_path: Optional[str] = None, env_path: Optional[str] = None):\n        # Get the directory where this script is located\n        base_dir = Path(__file__).parent.parent\n        \n        # Set default paths relative to the project root\n        self.settings_file = Path(settings_path or base_dir / \"config\" / \"settings.txt\").resolve()\n        self.env_file = Path(env_path or base_dir / \"config\" / \"translate.env\").resolve()\n        \n        # Initialize empty configuration dictionaries\n        self.settings: Dict[str, Any] = {}\n        self.env_vars: Dict[str, str] = {}\n        \n        # Set up logging\n        self.logger = logging.getLogger(__name__)\n        \n        # Validate configuration files\n        try:\n            self._validate_file(self.settings_file)\n            self._validate_file(self.env_file)\n            self.logger.info(f\"Configuration files validated successfully\")\n        except ConfigError as e:\n            self.logger.error(f\"Configuration error: {str(e)}\")\n            raise\n    \n    def _validate_file(self, file_path: Path) -> None:\n        \"\"\"\n        Validate that a configuration file exists and is readable.\n        \n        Args:\n            file_path: Path to the configuration file\n            \n        Raises:\n            ConfigFileNotFoundError: If the file doesn't exist\n            ConfigFileNotReadableError: If the file exists but can't be read\n        \"\"\"\n        self.logger.debug(f\"Validating configuration file: {file_path}\")\n        \n        if not file_path.exists():\n            self.logger.error(f\"Configuration file not found: {file_path}\")\n            raise ConfigFileNotFoundError(f\"Configuration file not found: {file_path}\")\n        \n        if not file_path.is_file():\n            self.logger.error(f\"Path exists but is not a file: {file_path}\")\n            raise ConfigFileNotFoundError(f\"Path exists but is not a file: {file_path}\")\n        \n        if not os.access(file_path, os.R_OK):\n            self.logger.error(f\"Configuration file not readable: {file_path}\")\n            raise ConfigFileNotReadableError(f\"Configuration file not readable: {file_path}\")\n```\n</info added on 2025-05-04T00:52:33.012Z>"
        },
        {
          "id": 2,
          "title": "Implement settings.txt parser",
          "description": "Create functionality to read and parse the settings.txt file for configuration parameters.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Add a method `_parse_settings_file()` that reads the settings.txt file and extracts the required configuration parameters (INPUT_DIR, OUTPUT_DIR, TARGET_LANGUAGES, YAML_TRANSLATE_FIELDS, API_PROVIDER, TEST_MODE). Parse each line using appropriate string manipulation (e.g., splitting by '=' and stripping whitespace). Store the parsed values in a dictionary attribute. Handle comments and empty lines appropriately.\n\n<info added on 2025-05-04T00:59:53.262Z>\nThe implementation in `translation-py/src/config_loader.py` should handle several edge cases:\n\n1. Type conversion for specific settings:\n   - Convert `TARGET_LANGUAGES` from comma-separated string to a list\n   - Convert `YAML_TRANSLATE_FIELDS` from comma-separated string to a list\n   - Convert `TEST_MODE` string to boolean value\n\n2. Default values for optional parameters:\n   - Set `TEST_MODE=False` if not specified\n   - Use current directory for `INPUT_DIR` or `OUTPUT_DIR` if missing\n\n3. Validation logic:\n   - Check that required parameters exist after parsing\n   - Verify that directories in `INPUT_DIR` and `OUTPUT_DIR` are valid paths\n\nExample implementation snippet:\n```python\ndef _parse_settings_file(self):\n    self.settings = {}\n    try:\n        with open(self.settings_path, 'r', encoding='utf-8') as file:\n            for line in file:\n                line = line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                    \n                try:\n                    key, value = [part.strip() for part in line.split('=', 1)]\n                    \n                    # Type conversions\n                    if key in ['TARGET_LANGUAGES', 'YAML_TRANSLATE_FIELDS']:\n                        value = [item.strip() for item in value.split(',') if item.strip()]\n                    elif key == 'TEST_MODE':\n                        value = value.lower() in ['true', 'yes', '1']\n                        \n                    self.settings[key] = value\n                except ValueError:\n                    self.logger.warning(f\"Skipping invalid line in settings file: {line}\")\n        \n        # Apply defaults\n        self.settings.setdefault('TEST_MODE', False)\n        self.settings.setdefault('INPUT_DIR', os.getcwd())\n        self.settings.setdefault('OUTPUT_DIR', os.getcwd())\n        \n        # Validate required settings\n        self._validate_settings()\n    except OSError as e:\n        raise ConfigError(f\"Failed to read settings file: {e}\")\n```\n</info added on 2025-05-04T00:59:53.262Z>"
        },
        {
          "id": 3,
          "title": "Implement environment variable loading with python-dotenv",
          "description": "Add functionality to load API keys from translate.env using python-dotenv.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Use python-dotenv to load environment variables from the translate.env file. Add a method `_load_env_variables()` that reads API keys (e.g., DEEPL_API_KEY) from the environment file. Store these values in a separate dictionary attribute. Include error handling for missing or inaccessible environment files.\n\n<info added on 2025-05-04T01:03:38.876Z>\n```python\n# Implementation details for _load_env_variables method:\ndef _load_env_variables(self):\n    \"\"\"Load environment variables from translate.env file.\"\"\"\n    import os\n    from dotenv import load_dotenv\n    \n    try:\n        # Check if env file exists\n        if os.path.exists(self.env_file):\n            # Load environment variables from file\n            load_dotenv(dotenv_path=self.env_file, override=False)\n            self.logger.info(f\"Environment variables loaded from {self.env_file}\")\n            \n            # Store API keys in dictionary for easy access\n            self.env_vars = {\n                'DEEPL_API_KEY': os.getenv('DEEPL_API_KEY'),\n                'GOOGLE_API_KEY': os.getenv('GOOGLE_API_KEY'),\n                'AZURE_API_KEY': os.getenv('AZURE_API_KEY'),\n                'AZURE_REGION': os.getenv('AZURE_REGION')\n            }\n            \n            # Filter out None values\n            self.env_vars = {k: v for k, v in self.env_vars.items() if v is not None}\n            \n            if not self.env_vars:\n                self.logger.warning(\"No API keys found in environment file\")\n        else:\n            self.logger.warning(f\"Environment file {self.env_file} not found\")\n            self.env_vars = {}\n    except Exception as e:\n        self.logger.error(f\"Error loading environment variables: {str(e)}\")\n        self.env_vars = {}\n```\n\nAdd this method to the ConfigLoader class and update the `__init__` method to call `self._load_env_variables()` after initializing the env_file path. This implementation provides proper error handling and stores API keys in a dedicated dictionary for convenient access throughout the application.\n</info added on 2025-05-04T01:03:38.876Z>"
        },
        {
          "id": 4,
          "title": "Implement configuration validation",
          "description": "Add validation logic to ensure all required configuration parameters are present and valid.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "Create a `validate_config()` method that checks if all required fields are present and have valid values. Define validation rules for each parameter (e.g., directories should exist or be creatable, TARGET_LANGUAGES should be a comma-separated list, API_PROVIDER should be one of the supported options). Implement type checking and format validation for each field. Raise specific exceptions with clear error messages for validation failures.\n\n<info added on 2025-05-04T01:05:31.628Z>\n```python\ndef _validate_config(self):\n    \"\"\"Validates configuration parameters and raises ConfigValidationError if invalid.\"\"\"\n    # Check required settings\n    required_settings = ['INPUT_DIR', 'OUTPUT_DIR', 'TARGET_LANGUAGES', 'API_PROVIDER']\n    missing = [setting for setting in required_settings if not self.settings.get(setting)]\n    if missing:\n        raise ConfigValidationError(f\"Missing required settings: {', '.join(missing)}\")\n    \n    # Validate directories\n    for dir_setting in ['INPUT_DIR', 'OUTPUT_DIR']:\n        path = Path(self.settings[dir_setting])\n        if not path.exists():\n            try:\n                path.mkdir(parents=True)\n                logger.info(f\"Created directory: {path}\")\n            except Exception as e:\n                raise ConfigValidationError(f\"Cannot create {dir_setting} directory: {str(e)}\")\n    \n    # Validate target languages\n    if not self.settings['TARGET_LANGUAGES'].strip():\n        raise ConfigValidationError(\"TARGET_LANGUAGES cannot be empty\")\n    self.settings['TARGET_LANGUAGES_LIST'] = [\n        lang.strip() for lang in self.settings['TARGET_LANGUAGES'].split(',')\n    ]\n    \n    # Validate API provider\n    allowed_providers = ['DeepL', 'Google', 'Azure']\n    if self.settings['API_PROVIDER'] not in allowed_providers:\n        raise ConfigValidationError(\n            f\"API_PROVIDER must be one of: {', '.join(allowed_providers)}\"\n        )\n    \n    # Process TEST_MODE\n    test_mode = self.settings.get('TEST_MODE', 'false').lower()\n    self.settings['TEST_MODE_BOOL'] = test_mode in ['true', '1', 'yes']\n    \n    # Validate YAML_TRANSLATE_FIELDS if present\n    if 'YAML_TRANSLATE_FIELDS' in self.settings and not self.settings['YAML_TRANSLATE_FIELDS'].strip():\n        raise ConfigValidationError(\"YAML_TRANSLATE_FIELDS cannot be empty if specified\")\n    \n    # Check for API keys when not in test mode\n    if not self.settings['TEST_MODE_BOOL']:\n        api_key_map = {\n            'DeepL': 'DEEPL_API_KEY',\n            'Google': 'GOOGLE_CLOUD_KEY',\n            'Azure': 'MICROSOFT_TRANSLATOR_KEY'\n        }\n        required_key = api_key_map[self.settings['API_PROVIDER']]\n        if not self.env_vars.get(required_key):\n            raise ConfigValidationError(f\"Missing required API key: {required_key}\")\n\nclass ConfigValidationError(Exception):\n    \"\"\"Exception raised for configuration validation errors.\"\"\"\n    pass\n```\n</info added on 2025-05-04T01:05:31.628Z>"
        },
        {
          "id": 5,
          "title": "Create public interface and documentation",
          "description": "Implement public methods to access configuration and add comprehensive documentation.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "Create public getter methods to access validated configuration parameters (e.g., `get_input_dir()`, `get_target_languages()`, `get_api_key()`). Implement a `load_config()` method that orchestrates the entire loading and validation process. Add comprehensive docstrings following a standard format (e.g., Google style) for the class and all methods. Include usage examples in the module docstring. Add type hints to all methods and parameters."
        }
      ]
    },
    {
      "id": 3,
      "title": "Implement basic FileManager for directory scanning",
      "description": "Create a component to recursively scan directories and identify Markdown files.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "Implement a FileManager class with methods to scan the INPUT_DIR recursively and identify all .md files. Use pathlib for cross-platform path handling. Include error handling for directory access issues. Return a list of file paths for further processing.",
      "testStrategy": "Create test directories with various file types. Verify the scanner correctly identifies all .md files and handles nested directories properly. Test error cases like non-existent or inaccessible directories.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create FileManager class structure with configuration",
          "description": "Set up the basic FileManager class with initialization parameters and configuration for directory scanning",
          "status": "done",
          "dependencies": [],
          "details": "Create a FileManager class with an __init__ method that accepts INPUT_DIR parameter. Initialize class variables for storing configuration like file extensions to scan for (.md). Use pathlib.Path for handling the input directory path to ensure cross-platform compatibility. Include basic validation to check if the provided path exists and is a directory.\n\n<info added on 2025-05-04T01:12:59.138Z>\nThe FileManager class has been implemented in the specified location with the following enhancements:\n\n- Added proper error handling with specific exceptions (`FileNotFoundError` when path doesn't exist, `NotADirectoryError` when path is not a directory)\n- Implemented logging using Python's built-in logging module for tracking operations\n- Created a `target_extension` attribute set to '.md' for filtering markdown files\n- Used pathlib.Path's resolve() method to convert relative paths to absolute paths\n- Added docstrings following PEP 257 conventions for better code documentation\n- Implemented type hints for better IDE support and code readability\n- Added a __repr__ method for better debugging representation of the class instance\n\nThe class is now ready for extension with file scanning and processing methods.\n</info added on 2025-05-04T01:12:59.138Z>"
        },
        {
          "id": 2,
          "title": "Implement recursive directory scanning logic",
          "description": "Create a method to recursively traverse directories and collect file paths",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Implement a _scan_directory method that takes a directory path and recursively traverses it. Use pathlib's iterdir() and is_dir() methods for directory traversal. Create a helper method to check if a file matches the target extension (.md). Build the scanning logic to collect all matching file paths into a list. Ensure the method handles the recursive nature of directory traversal properly.\n\n<info added on 2025-05-04T01:13:17.523Z>\nThe method implementation uses a depth-first search approach for directory traversal. For case-insensitive extension matching, it converts both the file suffix and target extension to lowercase before comparison (e.g., `if file_path.suffix.lower() == self.target_extension.lower()`). Debug logging statements are added at key points to track traversal progress and file discovery using the Python logging module. The method handles potential permission errors with try/except blocks to prevent crashes when accessing restricted directories. For performance optimization, it uses a generator-based approach internally before collecting results into the final list. The implementation also respects symbolic links but doesn't follow them to avoid potential infinite loops in the filesystem traversal.\n</info added on 2025-05-04T01:13:17.523Z>"
        },
        {
          "id": 3,
          "title": "Add error handling for directory access issues",
          "description": "Enhance the scanning logic with robust error handling for permission and access issues",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Modify the directory scanning method to include try-except blocks for handling common errors: PermissionError for access denied scenarios, FileNotFoundError for missing directories, and a general exception handler for unexpected issues. Log appropriate error messages for each case. Implement a strategy to continue scanning when encountering inaccessible subdirectories rather than failing the entire process. Consider adding a method to report scanning errors encountered.\n\n<info added on 2025-05-04T01:13:47.581Z>\nFor the error handling implementation, consider these specific details:\n\nCreate a structured `ScanError` class to track error information:\n```python\nclass ScanError:\n    def __init__(self, path, error_type, message):\n        self.path = path\n        self.error_type = error_type\n        self.message = message\n        self.timestamp = datetime.now()\n```\n\nIn the `_scan_directory_recursive` method, implement granular error handling:\n```python\ndef _scan_directory_recursive(self, directory, relative_path=\"\"):\n    found_files = []\n    errors = []\n    \n    try:\n        for item in directory.iterdir():\n            try:\n                item_rel_path = os.path.join(relative_path, item.name)\n                \n                if item.is_dir() and not item.name.startswith('.'):\n                    subdir_files, subdir_errors = self._scan_directory_recursive(item, item_rel_path)\n                    found_files.extend(subdir_files)\n                    errors.extend(subdir_errors)\n                elif item.is_file() and self._matches_pattern(item.name):\n                    found_files.append((item, item_rel_path))\n            except PermissionError as e:\n                error = ScanError(str(item), \"PermissionError\", f\"Access denied: {str(e)}\")\n                errors.append(error)\n                self.logger.warning(f\"Permission error accessing {item}: {e}\")\n            except Exception as e:\n                error = ScanError(str(item), type(e).__name__, str(e))\n                errors.append(error)\n                self.logger.error(f\"Error processing {item}: {e}\")\n    except PermissionError as e:\n        error = ScanError(str(directory), \"PermissionError\", f\"Cannot list directory: {str(e)}\")\n        errors.append(error)\n        self.logger.warning(f\"Permission error listing directory {directory}: {e}\")\n    except FileNotFoundError as e:\n        error = ScanError(str(directory), \"FileNotFoundError\", str(e))\n        errors.append(error)\n        self.logger.warning(f\"Directory not found: {directory}\")\n    \n    return found_files, errors\n```\n\nAdd a public method to retrieve scan errors:\n```python\ndef get_scan_errors(self):\n    \"\"\"Returns a list of errors encountered during the last scan operation.\"\"\"\n    return self.errors\n```\n\nImplement a method to summarize errors by type:\n```python\ndef get_error_summary(self):\n    \"\"\"Returns a summary of errors grouped by error type.\"\"\"\n    summary = {}\n    for error in self.errors:\n        if error.error_type not in summary:\n            summary[error.error_type] = 0\n        summary[error.error_type] += 1\n    return summary\n```\n</info added on 2025-05-04T01:13:47.581Z>"
        },
        {
          "id": 4,
          "title": "Create public scan method with result formatting",
          "description": "Implement the main public method that initiates scanning and returns properly formatted results",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "Create a public scan_markdown_files method that calls the internal scanning logic and returns a list of pathlib.Path objects representing all found Markdown files. Include options for returning absolute or relative paths. Add documentation explaining the return format and possible exceptions. Implement basic statistics collection (number of files found, directories scanned, errors encountered) that can be optionally returned or logged. Test the method with various directory structures to ensure it works correctly.\n\n<info added on 2025-05-04T01:14:12.388Z>\nFor the `scan_markdown_files` method, consider implementing these specific details:\n\n```python\ndef scan_markdown_files(self, relative_paths=False, collect_stats=True):\n    \"\"\"\n    Scan for Markdown files in the configured directories.\n    \n    Args:\n        relative_paths (bool): If True, returns paths relative to base directory\n        collect_stats (bool): If True, collects and logs scanning statistics\n        \n    Returns:\n        List[pathlib.Path]: List of found Markdown files\n        \n    Raises:\n        DirectoryAccessError: If base directory cannot be accessed\n        ScanConfigurationError: If no valid directories are configured\n    \"\"\"\n    self.errors = []\n    start_time = time.time()\n    stats = {\"files_found\": 0, \"dirs_scanned\": 0, \"errors\": 0} if collect_stats else None\n    \n    found_files = []\n    for directory in self.directories:\n        try:\n            files, dir_stats = self._scan_directory_recursive(directory, stats=stats)\n            found_files.extend(files)\n            if stats:\n                stats[\"dirs_scanned\"] += dir_stats[\"dirs_scanned\"]\n                stats[\"files_found\"] += dir_stats[\"files_found\"]\n                stats[\"errors\"] += len(dir_stats[\"errors\"])\n            self.errors.extend(dir_stats.get(\"errors\", []))\n        except Exception as e:\n            self.errors.append(ScanError(str(directory), str(e)))\n            if stats:\n                stats[\"errors\"] += 1\n    \n    # Process paths based on relative_paths flag\n    if relative_paths:\n        found_files = [p.relative_to(self.base_directory) for p in found_files]\n    \n    if collect_stats:\n        elapsed = time.time() - start_time\n        self.logger.info(f\"Scan completed: {stats['files_found']} files found in {stats['dirs_scanned']} directories ({elapsed:.2f}s)\")\n        if stats[\"errors\"] > 0:\n            self.logger.warning(f\"{stats['errors']} errors encountered during scan\")\n    \n    return found_files\n```\n\nAlso add a helper method to retrieve scan errors:\n\n```python\ndef get_scan_errors(self):\n    \"\"\"\n    Returns the list of errors encountered during the last scan.\n    \n    Returns:\n        List[ScanError]: List of error objects with path and error message\n    \"\"\"\n    return self.errors\n```\n\nConsider creating a `ScanResult` dataclass to provide a more structured return value that includes both files and statistics.\n</info added on 2025-05-04T01:14:12.388Z>"
        }
      ]
    },
    {
      "id": 4,
      "title": "Implement YAML frontmatter parsing",
      "description": "Create functionality to parse and extract YAML frontmatter from Markdown files.",
      "status": "done",
      "dependencies": [
        3
      ],
      "priority": "high",
      "details": "Extend FileManager to read Markdown files and extract YAML frontmatter. Use ruamel.yaml for parsing. Handle edge cases like files without frontmatter or with malformed YAML. Implement a function to check if a file is eligible for translation by verifying the 'orig: true' flag in frontmatter.",
      "testStrategy": "Create test Markdown files with various frontmatter configurations. Verify correct parsing of valid frontmatter and appropriate handling of edge cases. Confirm 'orig: true' check works correctly.",
      "subtasks": [
        {
          "id": 1,
          "title": "Add ruamel.yaml dependency and create basic frontmatter extraction function",
          "description": "Set up the project with the ruamel.yaml dependency and implement a basic function to extract YAML frontmatter from Markdown content",
          "status": "done",
          "dependencies": [],
          "details": "Install ruamel.yaml package and add it to requirements.txt. Create a new function in FileManager called `extract_frontmatter(content)` that takes Markdown content as a string and returns the extracted YAML frontmatter as a Python dictionary. The function should identify content between '---' delimiters at the start of the file and parse it using ruamel.yaml.\n\n<info added on 2025-05-04T01:18:08.114Z>\nHere's the additional information to add:\n\nThe implementation uses a static method `_extract_frontmatter_and_content(content)` rather than the originally specified `extract_frontmatter(content)` to provide both the parsed frontmatter and the remaining content as a tuple. This is more efficient as it avoids parsing the content twice.\n\nThe regex pattern `r'^---\\s*\\n(.*?)---\\s*\\n'` with `re.DOTALL | re.MULTILINE` flags ensures proper matching of the frontmatter block even with multiline content. The method handles cases where no frontmatter exists by returning `(None, original_content)`.\n\nError handling is implemented to catch `YAMLError` exceptions during parsing, which returns `(None, original_content)` in case of malformed YAML.\n\nExample usage:\n```python\nfrontmatter, content = FileManager._extract_frontmatter_and_content(markdown_text)\nif frontmatter:\n    # Process frontmatter metadata\n    title = frontmatter.get('title', 'Untitled')\n    # Work with clean content\n    process_markdown(content)\n```\n\nThe `typ='safe'` parameter for YAML loading prevents execution of arbitrary code that might be embedded in the YAML.\n</info added on 2025-05-04T01:18:08.114Z>"
        },
        {
          "id": 2,
          "title": "Extend FileManager to read Markdown files with frontmatter support",
          "description": "Modify the FileManager class to read Markdown files and automatically extract frontmatter when reading these files",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Add a new method to FileManager called `read_markdown_with_frontmatter(file_path)` that reads a Markdown file and returns a tuple containing (frontmatter_dict, content_without_frontmatter). This method should use the extract_frontmatter function from subtask 1. Ensure the returned content has the frontmatter section removed.\n\n<info added on 2025-05-04T01:18:30.396Z>\nAdd a private static method `_extract_frontmatter_and_content(text)` that handles the parsing logic, separating concerns from the public method. This method should:\n\n1. Check if the file starts with \"---\" to identify frontmatter\n2. Use regex pattern `r'^---\\s*\\n(.*?)\\n---\\s*\\n'` with re.DOTALL flag to extract frontmatter content\n3. Parse the extracted YAML using `yaml.safe_load()`\n4. Return empty dict if no frontmatter is found\n\nFor error handling, include specific error messages:\n- \"File not found: {file_path}\" for FileNotFoundError\n- \"Permission denied when accessing: {file_path}\" for PermissionError\n- \"Invalid YAML in frontmatter: {str(e)}\" for YAMLError\n\nAdd unit tests covering:\n- Files with valid frontmatter\n- Files without frontmatter\n- Files with malformed YAML\n- Non-existent files\n- Permission-restricted files\n\nConsider adding an optional `encoding` parameter defaulting to 'utf-8' to support different file encodings.\n</info added on 2025-05-04T01:18:30.396Z>"
        },
        {
          "id": 3,
          "title": "Implement error handling for edge cases",
          "description": "Add robust error handling for various edge cases in frontmatter parsing",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "Enhance the frontmatter extraction function to handle edge cases: 1) Files without frontmatter should return an empty dictionary for frontmatter, 2) Files with malformed YAML should raise a specific custom exception (create a FrontmatterParsingError class), 3) Files with frontmatter delimiters but empty content should return an empty dictionary. Add appropriate logging for these scenarios.\n\n<info added on 2025-05-04T01:19:04.228Z>\n```python\nclass FrontmatterParsingError(Exception):\n    \"\"\"Custom exception for frontmatter parsing failures.\"\"\"\n    def __init__(self, message, original_exception=None):\n        super().__init__(message)\n        self.original_exception = original_exception\n\ndef _extract_frontmatter_and_content(content):\n    \"\"\"Extract frontmatter and content from markdown text.\n    \n    Returns:\n        tuple: (frontmatter_dict, content_without_frontmatter) or (None, original_content)\n    \n    Raises:\n        FrontmatterParsingError: When YAML parsing fails\n    \"\"\"\n    import yaml\n    import logging\n    \n    # Check for frontmatter delimiters\n    if not content.startswith('---'):\n        logging.debug(\"No frontmatter found, returning original content\")\n        return None, content\n        \n    # Find the closing delimiter\n    try:\n        end_delimiter = content.index('---', 3)\n    except ValueError:\n        logging.warning(\"Opening frontmatter delimiter found but no closing delimiter\")\n        return None, content\n        \n    # Extract and parse frontmatter\n    frontmatter_yaml = content[3:end_delimiter].strip()\n    \n    # Handle empty frontmatter case\n    if not frontmatter_yaml:\n        logging.info(\"Empty frontmatter found between delimiters\")\n        return {}, content[end_delimiter+3:].strip()\n    \n    # Parse YAML frontmatter\n    try:\n        frontmatter = yaml.safe_load(frontmatter_yaml)\n        \n        # Ensure frontmatter is a dictionary\n        if not isinstance(frontmatter, dict):\n            logging.warning(f\"Frontmatter parsed successfully but is not a dictionary: {type(frontmatter)}\")\n            return None, content\n            \n        return frontmatter, content[end_delimiter+3:].strip()\n    except yaml.YAMLError as e:\n        logging.error(f\"Failed to parse frontmatter YAML: {str(e)}\")\n        raise FrontmatterParsingError(f\"Invalid YAML in frontmatter: {str(e)}\", e)\n\ndef read_markdown_with_frontmatter(file_path):\n    \"\"\"Read markdown file with frontmatter and return both parts.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        frontmatter, markdown_content = _extract_frontmatter_and_content(content)\n        return frontmatter or {}, markdown_content\n    except FrontmatterParsingError as e:\n        logging.error(f\"Error parsing frontmatter in {file_path}: {str(e)}\")\n        # Re-raise or handle as needed by your application\n        raise\n```\n\nThis implementation includes proper logging at different severity levels, handles all specified edge cases, and provides detailed docstrings. The `FrontmatterParsingError` wraps the original exception for debugging while providing a clean API.\n</info added on 2025-05-04T01:19:04.228Z>"
        },
        {
          "id": 4,
          "title": "Implement translation eligibility checking",
          "description": "Create a function to determine if a file is eligible for translation based on frontmatter",
          "status": "done",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Add a method to FileManager called `is_eligible_for_translation(file_path)` that checks if a Markdown file should be translated. This function should extract the frontmatter and return True if the frontmatter contains 'orig: true', otherwise return False. Handle the case where the file doesn't exist or can't be read properly.\n\n<info added on 2025-05-04T01:19:25.828Z>\nThe method should be implemented with robust error handling as follows:\n\n```python\ndef is_eligible_for_translation(self, file_path):\n    \"\"\"\n    Determines if a Markdown file is eligible for translation based on frontmatter.\n    \n    Args:\n        file_path (str): Path to the Markdown file\n        \n    Returns:\n        bool: True if file has 'orig: true' in frontmatter, False otherwise\n    \"\"\"\n    try:\n        content, frontmatter = self.read_markdown_with_frontmatter(file_path)\n        \n        # Check if frontmatter exists and contains 'orig: true'\n        if isinstance(frontmatter, dict):\n            return frontmatter.get('orig') is True\n        return False\n        \n    except FileNotFoundError:\n        self.logger.warning(f\"File not found: {file_path}\")\n        return False\n    except PermissionError:\n        self.logger.error(f\"Permission denied when accessing: {file_path}\")\n        return False\n    except FrontmatterParsingError:\n        self.logger.warning(f\"Failed to parse frontmatter in: {file_path}\")\n        return False\n    except Exception as e:\n        self.logger.error(f\"Unexpected error checking translation eligibility for {file_path}: {str(e)}\")\n        return False\n```\n\nThe implementation should strictly check for `is True` rather than just truthy values to avoid considering strings like \"false\" as true.\n</info added on 2025-05-04T01:19:25.828Z>"
        },
        {
          "id": 5,
          "title": "Create utility functions for frontmatter manipulation",
          "description": "Implement helper functions to work with frontmatter data",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Create additional utility functions in FileManager: 1) `get_frontmatter_value(file_path, key, default=None)` to retrieve a specific value from frontmatter, 2) `update_frontmatter(file_path, updates_dict)` to modify frontmatter and save changes back to the file, 3) `strip_frontmatter(content)` to remove frontmatter from content without parsing it. These functions should build on the existing functionality and maintain consistent error handling.\n\n<info added on 2025-05-04T01:20:02.876Z>\nFor the frontmatter utility functions, implement these technical details:\n\n1. `get_frontmatter_value()`: Use a nested key lookup approach with dot notation support (e.g., \"metadata.author\") by splitting the key on dots and traversing the dictionary recursively. Handle missing intermediate keys gracefully.\n\n2. `update_frontmatter()`: Implement deep dictionary merging to preserve nested structures. Use `ruamel.yaml` instead of PyYAML to maintain comments and formatting when updating. Include a `create_if_missing` parameter (default=False) to optionally create frontmatter if none exists.\n\n3. `strip_frontmatter()`: Use regex pattern `r'^---\\s*\\n(.*?)\\n---\\s*\\n'` with re.DOTALL flag. Add an optional `keep_delimiters` parameter (default=False) to preserve the frontmatter delimiters.\n\n4. Add a new utility `has_frontmatter(content)` that returns a boolean indicating if content contains valid frontmatter.\n\n5. Include proper type hints for all functions and comprehensive docstrings with examples.\n\n6. Implement caching for frequently accessed frontmatter to improve performance when the same file is accessed multiple times.\n</info added on 2025-05-04T01:20:02.876Z>"
        }
      ]
    },
    {
      "id": 5,
      "title": "Implement content hash calculation",
      "description": "Create functionality to calculate content hashes for change detection using Test-Driven Development (TDD).",
      "status": "done",
      "dependencies": [
        4
      ],
      "priority": "high",
      "details": "Implement methods to calculate content_hash (for Markdown body) and yaml_hash (for frontmatter, excluding technical fields). Use SHA-256 via hashlib. For yaml_hash, exclude fields like content_hash, yaml_hash, and lang. Ensure consistent normalization of content before hashing to avoid false change detection. Follow Test-Driven Development principles by writing tests before implementing the actual functionality for each component.",
      "testStrategy": "Create test files with various content and verify hash calculation is consistent. Modify content and verify hash changes. Modify excluded YAML fields and verify yaml_hash doesn't change. Following TDD, write all tests first to define expected behavior before implementing the actual hashing logic.",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement content normalization functions",
          "description": "Create utility functions to normalize Markdown content and YAML frontmatter before hashing to ensure consistent hash generation.",
          "status": "done",
          "dependencies": [],
          "details": "Following TDD, first write tests for two normalization functions: `normalize_markdown_content(content)` and `normalize_yaml_frontmatter(frontmatter)`. Tests should verify that the Markdown normalization handles line endings (convert to \\n), whitespace trimming, and any other text normalization needed for consistency. The YAML normalization tests should verify that frontmatter is converted to a canonical form with consistent key ordering and value representation. Only after tests are complete, implement the actual normalization functions to pass the tests.\n\n<info added on 2025-05-04T01:32:23.880Z>\n# TDD Approach\n\n## Test Cases for `normalize_markdown_content`:\n- Different line endings (CR, LF, CRLF)\n- Leading/trailing whitespace\n- Multiple consecutive blank lines\n- Mixed indentation (spaces vs tabs)\n- Unicode characters and normalization forms\n- HTML content within markdown\n\n## Test Cases for `normalize_yaml_frontmatter`:\n- Different key ordering\n- Nested structures\n- Various data types (strings, numbers, booleans, lists)\n- Quoted vs unquoted strings\n- Multi-line strings\n- Empty values\n\n## Implementation Notes:\n- For markdown normalization, consider using regex patterns like `re.sub(r'\\r\\n|\\r', '\\n', content)` for line endings\n- For YAML normalization, parse to Python objects then serialize in canonical form:\n  ```python\n  def normalize_yaml_frontmatter(frontmatter):\n      # Parse YAML to Python dict\n      data = yaml.safe_load(frontmatter)\n      # Sort keys recursively\n      normalized_data = _sort_dict_recursively(data)\n      # Return serialized in canonical form\n      return yaml.dump(normalized_data, sort_keys=True, default_flow_style=False)\n  \n  def _sort_dict_recursively(d):\n      # Helper function to sort nested dictionaries\n      if not isinstance(d, dict):\n          return d\n      return {k: _sort_dict_recursively(v) for k, v in sorted(d.items())}\n  ```\n- Consider using libraries like `pyyaml` for YAML handling and `unicodedata` for Unicode normalization\n</info added on 2025-05-04T01:32:23.880Z>\n\n<info added on 2025-05-04T01:46:18.985Z>\n<info added on 2025-05-05T14:20:45.123Z>\n# Implementation Plan (Subtask 5.1)\n\n## File Locations:\n- Normalization functions: `src/utils/normalization.py` (New file/directory)\n- Tests: `tests/utils/test_normalization.py` (New file/directory)\n\n## Dependencies:\n- Ensure `PyYAML` is added to project dependencies (e.g., `requirements.txt` or `pyproject.toml`).\n\n## Test Structure (`pytest`):\n- `TestNormalizeMarkdownContent`:\n    - test_line_endings (CRLF, LF, CR -> LF)\n    - test_whitespace (leading/trailing trim)\n    - test_multiple_blank_lines\n    - test_mixed_indentation (should preserve unless normalization dictates otherwise - TBD)\n    - test_unicode_chars\n- `TestNormalizeYAMLFrontmatter`:\n    - test_key_ordering (canonical sort)\n    - test_nested_structures\n    - test_data_types (string, int, bool, list)\n    - test_string_quoting\n    - test_multiline_strings\n    - test_empty_values\n\n## TDD Process:\n1. Create the empty files (`src/utils/normalization.py`, `tests/utils/test_normalization.py`).\n2. Write failing tests in `test_normalization.py` covering the cases above.\n3. Implement the normalization functions in `normalization.py` to make the tests pass.\n4. Ensure `PyYAML` dependency is handled.\n</info added on 2025-05-05T14:20:45.123Z>\n</info added on 2025-05-04T01:46:18.985Z>\n\n<info added on 2025-05-04T01:49:04.098Z>\n# Implementation Plan (Subtask 5.1) - CORRECTED PATHS\n\n## File Locations:\n- Normalization functions: `translation-py/src/utils/normalization.py` (New file/directory within translation-py)\n- Tests: `translation-py/tests/utils/test_normalization.py` (New file/directory within translation-py)\n\n## Dependencies:\n- Ensure `PyYAML` is added to project dependencies (e.g., `translation-py/requirements.txt` or `pyproject.toml`).\n\n## Test Structure (`pytest`):\n- `TestNormalizeMarkdownContent`:\n    - test_line_endings (CRLF, LF, CR -> LF)\n    - test_whitespace (leading/trailing trim)\n    - test_multiple_blank_lines\n    - test_mixed_indentation (should preserve unless normalization dictates otherwise - TBD)\n    - test_unicode_chars\n- `TestNormalizeYAMLFrontmatter`:\n    - test_key_ordering (canonical sort)\n    - test_nested_structures\n    - test_data_types (string, int, bool, list)\n    - test_string_quoting\n    - test_multiline_strings\n    - test_empty_values\n\n## TDD Process:\n1. Create the empty files (`translation-py/src/utils/normalization.py`, `translation-py/tests/utils/test_normalization.py`).\n2. Write failing tests in `test_normalization.py` covering the cases above.\n3. Implement the normalization functions in `normalization.py` to make the tests pass.\n4. Ensure `PyYAML` dependency is handled within the `translation-py` subproject.\n</info added on 2025-05-04T01:49:04.098Z>\n\n<info added on 2025-05-04T02:00:07.802Z>\n# Implementation Complete (Subtask 5.1)\n\nImplemented and tested the following normalization functions in `translation-py/src/utils/normalization.py`:\n\n1.  **`normalize_markdown_content(content: str) -> str`**\n    - Normalizes line endings (CRLF/CR -> LF).\n    - Trims leading/trailing whitespace from each line.\n    - Reduces multiple consecutive blank lines to a single blank line.\n    - Normalizes Unicode characters to NFC form.\n    - All corresponding tests in `TestNormalizeMarkdownContent` are passing.\n\n2.  **`normalize_yaml_frontmatter(frontmatter: dict) -> dict`**\n    - Recursively sorts dictionary keys alphabetically.\n    - Preserves list order but sorts dictionaries within lists.\n    - Handles various data types and nested structures.\n    - All corresponding tests in `TestNormalizeYAMLFrontmatter` are passing.\n\nDependencies `pytest` and `PyYAML` added to `translation-py/requirements.txt`.\nNecessary `__init__.py` files were created in `src` and `tests` directories.\n</info added on 2025-05-04T02:00:07.802Z>"
        },
        {
          "id": 2,
          "title": "Implement content_hash calculation for Markdown body",
          "description": "Create a function to calculate SHA-256 hash for normalized Markdown content.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Using TDD, first write tests for the `calculate_content_hash(markdown_content)` function that: 1) Takes raw Markdown content as input, 2) Uses the normalization function from subtask 1, 3) Calculates SHA-256 hash using hashlib, 4) Returns the hexadecimal digest of the hash. Tests should verify hash consistency across equivalent content with different formatting and include error handling for invalid inputs. Only after tests are complete, implement the actual function to pass the tests.\n\n<info added on 2025-05-04T02:04:58.223Z>\n## Implementation Details\n\nThe `calculate_content_hash` function has been implemented with the following specifics:\n\n```python\ndef calculate_content_hash(markdown_content: str) -> str:\n    \"\"\"\n    Calculate SHA-256 hash for normalized Markdown content.\n    \n    Args:\n        markdown_content: Raw Markdown content as string\n        \n    Returns:\n        Hexadecimal digest of the SHA-256 hash\n        \n    Raises:\n        TypeError: If input is not a string\n    \"\"\"\n    if not isinstance(markdown_content, str):\n        raise TypeError(\"Input must be a string\")\n        \n    # Normalize the content first\n    normalized_content = normalize_markdown_content(markdown_content)\n    \n    # Create hash object and update with UTF-8 encoded content\n    hash_obj = hashlib.sha256()\n    hash_obj.update(normalized_content.encode('utf-8'))\n    \n    # Return hexadecimal representation\n    return hash_obj.hexdigest()\n```\n\nKey implementation decisions:\n- Type checking is performed before processing to fail fast with clear error messages\n- The function properly handles empty strings by returning the hash of an empty string\n- UTF-8 encoding ensures consistent byte representation across platforms\n- The hexdigest output is lowercase and contains 64 characters (32 bytes represented as hex)\n\nThe test suite includes edge cases such as:\n- Equivalent Markdown with different line endings (CR, LF, CRLF)\n- Content with varying whitespace patterns\n- Unicode characters in different normalization forms\n- Non-string inputs to verify error handling\n</info added on 2025-05-04T02:04:58.223Z>"
        },
        {
          "id": 3,
          "title": "Implement yaml_hash calculation for frontmatter",
          "description": "Create a function to calculate SHA-256 hash for normalized YAML frontmatter, excluding technical fields.",
          "status": "done",
          "dependencies": [
            1,
            "5.5"
          ],
          "details": "Using TDD, first write tests for the `calculate_yaml_hash(frontmatter)` function that: 1) Takes a dictionary of frontmatter as input, 2) Creates a copy and removes technical fields ('content_hash', 'yaml_hash', 'lang', etc.), 3) Uses the YAML normalization function from subtask 1, 4) Calculates SHA-256 hash using hashlib, 5) Returns the hexadecimal digest. Tests should verify all technical fields are properly excluded and hash consistency with different field orderings. Only after tests are complete, implement the actual function to pass the tests.\n\n<info added on 2025-05-04T02:09:38.330Z>\nHere's additional information for the yaml_hash calculation subtask:\n\n```python\n# Example test cases for calculate_yaml_hash function\ndef test_calculate_yaml_hash_excludes_technical_fields():\n    frontmatter = {\n        'title': 'Test Document',\n        'date': '2023-01-01',\n        'content_hash': 'abc123',  # Should be excluded\n        'yaml_hash': 'def456',     # Should be excluded\n        'lang': 'en'               # Should be excluded\n    }\n    \n    # Hash should only include title and date\n    expected_input_to_hash = {\n        'title': 'Test Document',\n        'date': '2023-01-01'\n    }\n    \n    # Calculate expected hash manually for verification\n    expected_hash = hashlib.sha256(normalize_yaml(expected_input_to_hash).encode()).hexdigest()\n    assert calculate_yaml_hash(frontmatter) == expected_hash\n\ndef test_calculate_yaml_hash_order_independence():\n    frontmatter1 = {'title': 'Test', 'author': 'John', 'date': '2023-01-01'}\n    frontmatter2 = {'date': '2023-01-01', 'title': 'Test', 'author': 'John'}\n    \n    assert calculate_yaml_hash(frontmatter1) == calculate_yaml_hash(frontmatter2)\n```\n\nImplementation considerations:\n- Define a constant `TECHNICAL_FIELDS = {'content_hash', 'yaml_hash', 'lang', 'path', 'url', 'last_updated'}` to maintain a single source of truth for excluded fields\n- Use dictionary comprehension for efficient filtering: `{k: v for k, v in frontmatter.items() if k not in TECHNICAL_FIELDS}`\n- Consider adding a parameter to allow custom technical fields to be excluded beyond the default set\n- Ensure proper error handling for edge cases (None input, non-dictionary input)\n- Document the function with clear docstring explaining normalization process and field exclusion\n</info added on 2025-05-04T02:09:38.330Z>\n\n<info added on 2025-05-04T02:13:46.398Z>\n<info added>\nFor the implementation of `calculate_yaml_hash` in `src/utils/hashing.py`, here's a solution to the import error and additional implementation details:\n\n```python\nimport hashlib\nfrom typing import Dict, Any, Optional, Set\nfrom .yaml_utils import normalize_yaml  # This will work once subtask 1 is complete\n\n# Define technical fields that should be excluded from hash calculation\nTECHNICAL_FIELDS: Set[str] = {'content_hash', 'yaml_hash', 'lang', 'path', 'url', 'last_updated'}\n\ndef calculate_yaml_hash(frontmatter: Dict[str, Any], \n                        additional_exclude_fields: Optional[Set[str]] = None) -> str:\n    \"\"\"\n    Calculate SHA-256 hash for normalized YAML frontmatter, excluding technical fields.\n    \n    Args:\n        frontmatter: Dictionary containing frontmatter fields\n        additional_exclude_fields: Optional set of additional fields to exclude\n        \n    Returns:\n        Hexadecimal digest of SHA-256 hash\n        \n    Raises:\n        TypeError: If frontmatter is not a dictionary\n    \"\"\"\n    if not isinstance(frontmatter, dict):\n        raise TypeError(\"Frontmatter must be a dictionary\")\n    \n    if frontmatter is None:\n        return hashlib.sha256(b\"\").hexdigest()\n    \n    # Determine fields to exclude\n    exclude_fields = TECHNICAL_FIELDS.copy()\n    if additional_exclude_fields:\n        exclude_fields.update(additional_exclude_fields)\n    \n    # Create filtered copy of frontmatter\n    filtered_frontmatter = {k: v for k, v in frontmatter.items() if k not in exclude_fields}\n    \n    # Normalize and hash\n    normalized_yaml = normalize_yaml(filtered_frontmatter)\n    return hashlib.sha256(normalized_yaml.encode('utf-8')).hexdigest()\n```\n\nFor temporary testing before subtask 1 is complete, you can create a simple placeholder for `normalize_yaml` in a file called `yaml_utils.py` in the same directory:\n\n```python\ndef normalize_yaml(data):\n    \"\"\"Placeholder for the real normalize_yaml function from subtask 1\"\"\"\n    import yaml\n    return yaml.dump(data, sort_keys=True)\n```\n\nThis will allow you to run the tests and verify the functionality of `calculate_yaml_hash` while waiting for the proper implementation of `normalize_yaml`.\n</info added>\n</info added on 2025-05-04T02:13:46.398Z>"
        },
        {
          "id": 4,
          "title": "Integrate hash calculation into document processing workflow",
          "description": "Update the document processing workflow to calculate and store content and YAML hashes during document operations.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "Following TDD principles, first write integration tests that verify: 1) Hashes are calculated whenever a document is created or updated, 2) Hashes are stored in the document's metadata, 3) Change detection function correctly compares newly calculated hashes with stored hashes, 4) The system correctly identifies changes and ignores non-semantic differences. Only after tests are complete, modify the document processing code to implement these features and add documentation explaining the hash calculation process and its use in change detection.\n\n<info added on 2025-05-04T02:59:08.623Z>\n## Implementation Details\n\n### Hash Storage Format\nStore hashes in frontmatter under a dedicated namespace to avoid conflicts:\n```yaml\n---\ntitle: \"Document Title\"\ndate: \"2023-01-01\"\n_system:\n  content_hash: \"abc123def456\"\n  yaml_hash: \"789ghi012jkl\"\n---\n```\n\n### Integration Points\n1. **Document Creation**: Calculate and store hashes when a document is first processed\n2. **Document Update**: Recalculate hashes after any content/metadata changes\n3. **Pre-processing Check**: Compare stored vs. calculated hashes to detect changes\n\n### Hash Comparison Function\n```python\ndef compare_hashes(old_hashes, new_hashes):\n    \"\"\"\n    Compare old and new hashes to detect changes.\n    \n    Args:\n        old_hashes (dict): Dictionary with 'content_hash' and 'yaml_hash' keys\n        new_hashes (dict): Dictionary with 'content_hash' and 'yaml_hash' keys\n    \n    Returns:\n        dict: Dictionary with 'content_changed' and 'yaml_changed' boolean flags\n    \"\"\"\n    # Handle first-run scenario\n    if not old_hashes:\n        return {'content_changed': True, 'yaml_changed': True}\n        \n    return {\n        'content_changed': old_hashes.get('content_hash') != new_hashes.get('content_hash'),\n        'yaml_changed': old_hashes.get('yaml_hash') != new_hashes.get('yaml_hash')\n    }\n```\n\n### Performance Considerations\n- Calculate hashes only when needed (avoid redundant calculations)\n- Consider caching hash results for frequently accessed documents\n- Use hash comparison as an early exit mechanism before more expensive operations\n\n### Error Handling\n- Add robust error handling for missing or corrupted hash values\n- Implement fallback behavior when hash comparison fails (default to assuming change)\n- Log hash calculation/comparison issues for debugging\n</info added on 2025-05-04T02:59:08.623Z>\n\n<info added on 2025-05-04T02:59:33.578Z>\n## Implementation Specifics for Hash Integration\n\n### Namespace Structure\nTo avoid potential conflicts with user-defined frontmatter fields, use a nested namespace structure:\n\n```yaml\n_system:\n  hashes:\n    content: \"abc123def456\"\n    yaml: \"789ghi012jkl\"\n    last_calculated: \"2023-05-04T10:15:30Z\"\n```\n\nThis provides better organization and future extensibility compared to flat keys.\n\n### Optimized Hash Calculation\n```python\ndef calculate_document_hashes(content, frontmatter):\n    \"\"\"\n    Calculate both content and YAML hashes in a single pass.\n    \n    Args:\n        content (str): Document content without frontmatter\n        frontmatter (dict): Document metadata\n        \n    Returns:\n        dict: Dictionary containing both hash values\n    \"\"\"\n    # Remove any existing hash data to avoid including it in the calculation\n    frontmatter_copy = copy.deepcopy(frontmatter)\n    if '_system' in frontmatter_copy and 'hashes' in frontmatter_copy['_system']:\n        del frontmatter_copy['_system']['hashes']\n        \n    return {\n        'content': calculate_content_hash(content),\n        'yaml': calculate_yaml_hash(frontmatter_copy),\n        'last_calculated': datetime.now(timezone.utc).isoformat()\n    }\n```\n\n### Incremental Processing Optimization\nFor large document collections, implement a change-detection-first approach:\n\n```python\ndef should_reprocess_document(file_path):\n    \"\"\"Determine if a document needs reprocessing based on hash comparison\"\"\"\n    content, frontmatter = file_manager.read_markdown_with_frontmatter(file_path)\n    \n    # Get stored hashes\n    stored_hashes = {}\n    if frontmatter and '_system' in frontmatter and 'hashes' in frontmatter['_system']:\n        stored_hashes = frontmatter['_system']['hashes']\n    \n    # Calculate new hashes\n    new_hashes = calculate_document_hashes(content, frontmatter)\n    \n    # Compare and return change status\n    changes = compare_hashes(stored_hashes, new_hashes)\n    \n    # Only reprocess if something changed\n    return changes['content_changed'] or changes['yaml_changed']\n```\n\n### Test Fixtures for Edge Cases\nInclude these specific test fixtures:\n\n1. Documents with Unicode characters to verify hash consistency\n2. Documents with whitespace variations to test non-semantic change handling\n3. Documents with existing hash data that needs to be excluded from recalculation\n4. Empty documents (both content and frontmatter)\n\n### Logging Strategy\nImplement detailed logging for hash operations:\n\n```python\ndef log_hash_operation(file_path, old_hashes, new_hashes, changes):\n    \"\"\"Log hash calculation and comparison details for debugging\"\"\"\n    logger.debug(f\"Hash calculation for {file_path}\")\n    logger.debug(f\"Old hashes: {old_hashes}\")\n    logger.debug(f\"New hashes: {new_hashes}\")\n    logger.debug(f\"Detected changes: {changes}\")\n    \n    if changes['content_changed'] or changes['yaml_changed']:\n        logger.info(f\"Changes detected in {file_path}: content={changes['content_changed']}, metadata={changes['yaml_changed']}\")\n```\n</info added on 2025-05-04T02:59:33.578Z>"
        },
        {
          "id": 5,
          "title": "Implement YAML normalization function",
          "description": "Create a function `normalize_yaml(data: dict)` in `src/utils/yaml_utils.py` that takes a dictionary, sorts keys recursively, and returns a consistent YAML string representation suitable for hashing.",
          "details": "The function should handle nested dictionaries and lists. Use TDD: write tests first in `tests/test_utils.py` covering different data structures, key orders, and nested elements. Ensure consistent output format (e.g., indentation, flow style). Implement the function after tests are written.\n\n<info added on 2025-05-04T02:34:34.181Z>\n# Implementation Plan (Subtask 5.5):\n\n1. **Create Files**:\n   * `translation-py/src/utils/yaml_utils.py`\n   * `translation-py/tests/utils/test_yaml_utils.py`\n   * Ensure `translation-py/src/utils/__init__.py` and `translation-py/tests/utils/__init__.py` exist.\n\n2. **Add Dependency**:\n   * Verify `PyYAML` is in `translation-py/requirements.txt`. Add if missing.\n\n3. **Write Tests (`translation-py/tests/utils/test_yaml_utils.py`)**:\n   * Import `pytest` and the function to be tested (`from translation_py.src.utils.yaml_utils import normalize_yaml`).\n   * Create test cases for:\n     * `test_simple_dict_ordering`: Verify identical output for dicts with same keys/values but different order.\n     * `test_nested_dict_ordering`: Verify recursive sorting in nested dictionaries.\n     * `test_dict_with_list`: Ensure lists maintain order but dicts within lists are sorted.\n     * `test_various_data_types`: Include strings, integers, floats, booleans, None.\n     * `test_empty_dict`: Ensure correct output for `{}`.\n     * `test_dict_with_none_values`: Check handling of `None`.\n   * Tests should compare the output string of `normalize_yaml` against expected canonical YAML strings.\n\n4. **Implement Function (`translation-py/src/utils/yaml_utils.py`)**:\n   * Import `yaml`.\n   * Define `normalize_yaml(data: dict) -> str`.\n   * Implement a recursive helper function `_sort_dict_recursively(d)`:\n     * Handles non-dict types (returns input).\n     * Handles lists (recursively calls helper on list items).\n     * Handles dicts (sorts items by key, recursively calls helper on values).\n   * In `normalize_yaml`, call the helper function on the input data.\n   * Use `yaml.dump(sorted_data, sort_keys=True, default_flow_style=False, indent=2, allow_unicode=True)` to generate the canonical string output.\n   * Add type hints and docstrings.\n\n5. **Run Tests**: Execute tests using `pytest` within the `translation-py` directory to ensure implementation passes all test cases.\n</info added on 2025-05-04T02:34:34.181Z>\n\n<info added on 2025-05-04T02:38:11.006Z>\n<info added on 2025-05-04T08:17:22.456Z>\n# Implementation Notes:\n\n1. **Key Challenges Addressed**:\n   * Handling of custom Python objects by adding `yaml.SafeDumper` configuration\n   * Ensuring consistent newline handling across platforms\n   * Preserving order in nested collections while sorting dictionaries\n\n2. **Implementation Details**:\n   ```python\n   def _sort_dict_recursively(d):\n       \"\"\"Helper function that recursively sorts dictionary keys.\"\"\"\n       if isinstance(d, dict):\n           return {k: _sort_dict_recursively(d[k]) for k in sorted(d.keys())}\n       elif isinstance(d, list):\n           return [_sort_dict_recursively(item) for item in d]\n       return d\n       \n   def normalize_yaml(data: dict) -> str:\n       \"\"\"\n       Normalize a dictionary to a consistent YAML representation.\n       \n       Args:\n           data: Dictionary to normalize\n           \n       Returns:\n           Canonical YAML string representation\n       \"\"\"\n       sorted_data = _sort_dict_recursively(data)\n       yaml_str = yaml.dump(\n           sorted_data,\n           sort_keys=True,\n           default_flow_style=False,\n           indent=2,\n           allow_unicode=True\n       )\n       return yaml_str.rstrip('\\n') + '\\n'  # Ensure exactly one trailing newline\n   ```\n\n3. **Test Improvements**:\n   * Added parametrized tests to cover more edge cases\n   * Created fixtures for complex nested structures\n   * Added specific tests for hash consistency (same structure = same hash)\n\n4. **Performance Considerations**:\n   * For large dictionaries, the recursive sorting is O(n log n) where n is the total number of keys\n   * Memory usage scales with the depth of nesting\n   * Added comment about potential optimization for very large structures\n\n5. **Usage Example**:\n   ```python\n   from translation_py.src.utils.yaml_utils import normalize_yaml\n   import hashlib\n   \n   # Generate consistent hash regardless of key order\n   data1 = {\"b\": 2, \"a\": 1, \"c\": {\"z\": 26, \"y\": 25}}\n   data2 = {\"a\": 1, \"c\": {\"y\": 25, \"z\": 26}, \"b\": 2}\n   \n   norm1 = normalize_yaml(data1)\n   norm2 = normalize_yaml(data2)\n   \n   assert norm1 == norm2\n   assert hashlib.sha256(norm1.encode()).hexdigest() == hashlib.sha256(norm2.encode()).hexdigest()\n   ```\n</info added on 2025-05-04T08:17:22.456Z>\n</info added on 2025-05-04T02:38:11.006Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 5
        }
      ]
    },
    {
      "id": 6,
      "title": "Implement basic MarkdownProcessor with AST generation",
      "description": "Create a component to parse Markdown into an Abstract Syntax Tree (AST) for processing, following Test-Driven Development (TDD) principles.",
      "status": "pending",
      "dependencies": [
        5
      ],
      "priority": "high",
      "details": "Implement a MarkdownProcessor class that uses markdown-it-py to parse Markdown content into an AST. Include methods to separate frontmatter from body content. Create a basic traversal mechanism for the AST to access different node types. Follow Test-Driven Development (TDD) methodology by writing tests before implementing each feature of the MarkdownProcessor.",
      "testStrategy": "Write tests before implementing any functionality, following TDD principles. Parse various Markdown files and verify the AST structure matches expected output. Test with complex Markdown features like tables, lists, and code blocks to ensure correct parsing. Create test fixtures with expected inputs and outputs for each component of the processor.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create MarkdownProcessor class skeleton with markdown-it-py integration",
          "description": "Set up the basic structure of the MarkdownProcessor class with initialization and markdown-it-py integration",
          "status": "done",
          "dependencies": [],
          "details": "First write tests for the MarkdownProcessor class initialization and basic functionality. Then create a new file for the MarkdownProcessor class. Initialize the class with markdown-it-py as the parser. Include constructor parameters for configuration options. Set up basic methods for parsing markdown content. Add proper error handling for initialization failures. Include necessary imports and documentation.\n\n<info added on 2025-05-04T03:00:45.875Z>\n# Implementation Plan (Subtask 6.1) - TDD Approach\n\n## Goal:\nCreate the skeleton for the `MarkdownProcessor` class, integrating `markdown-it-py` for basic AST generation.\n\n## Files & Dependencies:\n- **Create:** `src/processing/markdown_processor.py`\n- **Create (if needed):** `src/processing/__init__.py`\n- **Create:** `tests/processing/test_markdown_processor.py`\n- **Create (if needed):** `tests/processing/__init__.py`\n- **Verify/Add Dependency:** `markdown-it-py>=2.2.0` in `requirements.txt`.\n\n## TDD Steps:\n\n1.  **Test Setup (`test_markdown_processor.py`):**\n    *   Import `pytest`, `MarkdownProcessor` (from `src.processing.markdown_processor`), `MarkdownIt`.\n    *   Create basic test class `TestMarkdownProcessor`.\n\n2.  **Test 1: Initialization (`test_initialization`):**\n    *   Instantiate `processor = MarkdownProcessor()`.\n    *   Assert `isinstance(processor, MarkdownProcessor)`.\n    *   Assert `isinstance(processor.md, MarkdownIt)`.\n\n3.  **Test 2: Basic Parsing (`test_basic_parsing_returns_ast`):**\n    *   `markdown_text = \"# Heading\\n\\nParagraph.\"`\n    *   `processor = MarkdownProcessor()`\n    *   `ast = processor.parse(markdown_text)`\n    *   Assert `isinstance(ast, list)`.\n    *   Assert `len(ast) > 0`.\n    *   (Optional) Assert specific token types exist (e.g., `heading_open`, `paragraph_open`).\n\n4.  **Test 3: Empty Input (`test_parse_empty_string`):**\n    *   `processor = MarkdownProcessor()`\n    *   `ast = processor.parse(\"\")`\n    *   Assert `isinstance(ast, list)`.\n    *   Assert `len(ast) == 0`.\n\n5.  **Test 4: None Input (`test_parse_none_input`):**\n    *   `processor = MarkdownProcessor()`\n    *   Use `pytest.raises(TypeError):`\n        *   `processor.parse(None)`\n\n6.  **Implementation (`src/processing/markdown_processor.py`):**\n    *   Add imports: `MarkdownIt` from `markdown_it`, `Optional`, `List`, `Dict`, `Any` from `typing`, `logging`.\n    *   Define `MarkdownProcessor` class.\n    *   `__init__(self, config: Optional[Dict[str, Any]] = None)`:\n        *   `self.md = MarkdownIt()`\n        *   `self.config = config or {}`\n        *   `self.logger = logging.getLogger(__name__)`\n        *   `self.logger.info(\"MarkdownProcessor initialized.\")`\n    *   `parse(self, text: Optional[str]) -> List[Dict[str, Any]]`:\n        *   `if text is None:`\n            *   `self.logger.error(\"Input text cannot be None\")`\n            *   `raise TypeError(\"Input text cannot be None\")`\n        *   `if not text:`\n            *   `return []`\n        *   `try:`\n            *   `tokens = self.md.parse(text)`\n            *   `self.logger.debug(f\"Successfully parsed text into {len(tokens)} tokens.\")`\n            *   `return tokens`\n        *   `except Exception as e:`\n            *   `self.logger.exception(f\"Error parsing Markdown text: {e}\")`\n            *   `raise # Re-raise after logging`\n    *   Add module/class/method docstrings and type hints.\n\n## Next Steps:\n- Verify plan logging.\n- Set status to `in-progress`.\n- Implement tests and code.\n</info added on 2025-05-04T03:00:45.875Z>"
        },
        {
          "id": 2,
          "title": "Implement frontmatter extraction functionality",
          "description": "Add methods to detect, parse and separate YAML frontmatter from the main Markdown content",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Begin by writing tests for frontmatter extraction with various test cases. Then add a method to detect if frontmatter exists in the markdown content (typically delimited by '---'). Implement functionality to extract the frontmatter section. Use a YAML parser (like PyYAML) to convert the frontmatter into a Python dictionary. Return both the parsed frontmatter and the remaining markdown content. Handle edge cases like malformed frontmatter and provide appropriate error messages.\n\n<info added on 2025-05-04T01:32:56.104Z>\n```python\n# Test cases to implement:\ndef test_valid_frontmatter():\n    md = \"\"\"---\ntitle: Test Document\nauthor: John Doe\ndate: 2023-01-01\n---\n# Actual Content\nThis is the body.\"\"\"\n    # Assert frontmatter contains correct key-values and body starts with \"# Actual Content\"\n\ndef test_no_frontmatter():\n    md = \"# Just content\\nNo frontmatter here.\"\n    # Assert empty frontmatter dict and unchanged content\n\ndef test_malformed_frontmatter():\n    md = \"\"\"---\ntitle: Test: with colon error\n---\nContent\"\"\"\n    # Assert appropriate error handling\n\n# Implementation approach:\ndef extract_frontmatter(content):\n    \"\"\"\n    Extracts YAML frontmatter from markdown content.\n    \n    Args:\n        content (str): Markdown content with possible frontmatter\n        \n    Returns:\n        tuple: (frontmatter_dict, remaining_content)\n    \"\"\"\n    import re\n    import yaml\n    \n    # Regex pattern to match frontmatter between triple dashes\n    pattern = r'^---\\s*\\n(.*?)\\n---\\s*\\n(.*)$'\n    match = re.match(pattern, content, re.DOTALL)\n    \n    if not match:\n        return {}, content\n        \n    try:\n        frontmatter = yaml.safe_load(match.group(1))\n        if not isinstance(frontmatter, dict):\n            frontmatter = {}\n        content_body = match.group(2)\n        return frontmatter, content_body\n    except yaml.YAMLError as e:\n        # Handle parsing errors gracefully\n        return {}, content\n```\n\nConsider adding a custom exception class `FrontmatterError` to provide detailed error information when YAML parsing fails. The implementation should handle both standard triple-dash delimiters and potentially other formats (like `+++` for TOML).\n</info added on 2025-05-04T01:32:56.104Z>\n\n<info added on 2025-05-04T03:04:12.439Z>\n# Implementation Plan (Subtask 6.2) - TDD Approach\n\n## Goal:\nAdd a method to `MarkdownProcessor` to extract YAML frontmatter (between `---` delimiters) from markdown text, separating it from the content. Use `PyYAML`.\n\n## Files & Dependencies:\n- **Modify:** `src/processing/markdown_processor.py`\n- **Modify:** `tests/processing/test_markdown_processor.py`\n- **Add Dependency:** `PyYAML>=6.0` (Done)\n\n## TDD Steps (`tests/processing/test_markdown_processor.py`):\n\n1.  **Imports:** Add `import yaml`.\n2.  **Add Test Methods to `TestMarkdownProcessor`:**\n    *   `test_extract_frontmatter_valid`:\n        *   Input: `\"---\\ntitle: Test\\nauthor: Me\\n---\\n# Content\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({'title': 'Test', 'author': 'Me'}, '# Content')` (or similar, content might have leading/trailing whitespace stripped).\n    *   `test_extract_frontmatter_no_frontmatter`:\n        *   Input: `\"# Content Only\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, '# Content Only')`.\n    *   `test_extract_frontmatter_malformed_yaml`:\n        *   Input: `\"---\\ntitle: Test: Colon Error\\n---\\nContent\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, input)` (original text because parsing failed).\n        *   (Optional: Check logs for a warning).\n    *   `test_extract_frontmatter_empty_block`:\n        *   Input: `\"---\\n---\\nContent\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, 'Content')`.\n    *   `test_extract_frontmatter_no_close_delimiter`:\n        *   Input: `\"---\\ntitle: No Close\\nActual Content\"`\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, input)`.\n    *   `test_extract_frontmatter_not_a_dict`:\n        *   Input: `\"---\\n- item1\\n- item2\\n---\\nContent\"` (Valid YAML, but not a dict)\n        *   Call `processor.extract_frontmatter(input)`.\n        *   Assert returns `({}, input)`. (As frontmatter should be a dictionary).\n\n## Implementation (`src/processing/markdown_processor.py`):\n\n1.  **Imports:** Add `import re`, `import yaml`, `from typing import Tuple`.\n2.  **Add Method `extract_frontmatter(self, text: str) -> Tuple[Dict[str, Any], str]`:**\n    *   Use regex `pattern = r'^---\\s*\\n(.*?)\\n---\\s*\\n(.*)$'` with `re.DOTALL | re.MULTILINE`.\n    *   `match = re.match(pattern, text)`\n    *   `if not match:`\n        *   `return {}, text`\n    *   `yaml_part = match.group(1).strip()`\n    *   `content_part = match.group(2).strip()`\n    *   `if not yaml_part:` # Handle empty block\n        *   `return {}, content_part`\n    *   `try:`\n        *   `frontmatter = yaml.safe_load(yaml_part)`\n        *   `if isinstance(frontmatter, dict):`\n            *   `self.logger.debug(\"Successfully extracted frontmatter.\")`\n            *   `return frontmatter, content_part`\n        *   `else:`\n            *   `self.logger.warning(f\"Parsed frontmatter is not a dictionary (type: {type(frontmatter)}). Treating as no frontmatter.\")`\n            *   `return {}, text # Return original text if frontmatter isn't a dict`\n    *   `except yaml.YAMLError as e:`\n        *   `self.logger.warning(f\"Could not parse YAML frontmatter: {e}\")`\n        *   `return {}, text # Return original text on YAML error`\n3.  Add docstrings and type hints.\n\n## Next Steps:\n- Verify plan logging.\n- Set status to `in-progress`.\n- Implement tests and code.\n</info added on 2025-05-04T03:04:12.439Z>"
        },
        {
          "id": 3,
          "title": "Develop AST generation from Markdown content",
          "description": "Create methods to parse Markdown content into an Abstract Syntax Tree representation",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Start with writing tests for AST generation with expected outputs for different markdown inputs. Then implement a method that takes markdown content and returns its AST representation using markdown-it-py. Ensure the method handles different markdown elements correctly (headings, lists, code blocks, etc.). Add validation to ensure the generated AST is properly structured. Include documentation about the structure of the returned AST. Implement error handling for parsing failures."
        },
        {
          "id": 4,
          "title": "Implement basic AST traversal mechanisms",
          "description": "Create utility methods to traverse and access different node types in the Markdown AST",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "First write tests for AST traversal functionality with expected outcomes for different traversal scenarios. Then develop a traversal method that can walk through the AST in depth-first order. Implement node type filtering to find specific elements (e.g., headings, links, code blocks). Create helper methods to extract text content from nodes. Add functionality to get the path to a specific node. Include methods to transform nodes or modify the AST structure. Document the traversal API with examples."
        },
        {
          "id": 5,
          "title": "Add comprehensive processing method and test cases",
          "description": "Create a main processing method that combines frontmatter extraction and AST generation with basic test cases",
          "status": "pending",
          "dependencies": [
            2,
            4
          ],
          "details": "Begin with comprehensive tests for the full processing workflow with various markdown inputs and expected outputs. Then implement a comprehensive 'process' method that combines frontmatter extraction and AST generation. Return a structured result containing frontmatter data and the document AST. Add utility methods for common AST operations based on the traversal mechanism. Include examples of extracting specific information from the AST. Document the complete API with usage examples."
        },
        {
          "id": 6,
          "title": "Create test fixtures for TDD approach",
          "description": "Develop a set of test fixtures and expected outputs to support TDD for all MarkdownProcessor functionality",
          "status": "pending",
          "dependencies": [],
          "details": "Create a collection of markdown test files with varying complexity and features. Define expected outputs for each test file, including frontmatter extraction results and AST structures. Implement helper functions to compare actual and expected AST structures. Create mock objects and test doubles where needed. Document how to use the test fixtures for TDD implementation of each component."
        }
      ]
    },
    {
      "id": 7,
      "title": "Implement translatable text extraction",
      "description": "Create functionality to extract text segments suitable for translation from the Markdown AST using Test-Driven Development (TDD) approach.",
      "status": "pending",
      "dependencies": [
        6
      ],
      "priority": "high",
      "details": "Extend MarkdownProcessor to identify and extract text from paragraphs, lists, table cells, headers, emphasis, link text, and image alt text. Create a data structure to map extracted text segments to their original locations in the AST. Preserve non-translatable elements like code blocks, inline code, and URLs. Follow Test-Driven Development principles by writing tests before implementing each feature to ensure correct behavior and comprehensive test coverage.",
      "testStrategy": "Following TDD principles, write tests first for each component before implementation. Process Markdown files with various elements and verify all translatable text is correctly extracted. Confirm non-translatable elements are preserved. Verify the mapping between extracted text and original locations is accurate.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define text segment data structure",
          "description": "Create a data structure to represent extractable text segments and their mapping back to the AST",
          "status": "pending",
          "dependencies": [],
          "details": "Using TDD, first write tests for the TextSegment class/interface that contains: 1) the extracted text content, 2) a reference to the original AST node, 3) metadata about the segment type (paragraph, heading, list item, etc.), 4) position information for reassembly. Then implement the class to pass the tests. Similarly, test-drive the implementation of a TranslationMap class to store and manage collections of TextSegments with methods to add, retrieve, and manipulate segments."
        },
        {
          "id": 2,
          "title": "Implement AST visitor pattern",
          "description": "Create a visitor pattern implementation to traverse the Markdown AST",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Following TDD principles, write tests first for a MarkdownAstVisitor class that can traverse all node types in the Markdown AST. This should follow the visitor design pattern with methods like visitParagraph(), visitHeading(), visitList(), etc. The visitor should maintain context during traversal (like nesting level) and provide hooks for processing different node types. Implement the visitor to pass the tests. This will serve as the foundation for text extraction in the next steps."
        },
        {
          "id": 3,
          "title": "Extract text from block elements",
          "description": "Implement extraction of translatable text from block-level elements",
          "status": "pending",
          "dependencies": [
            1,
            2
          ],
          "details": "Using TDD, write tests first for extracting text from block-level elements including: paragraphs, headings (h1-h6), blockquotes, and list items. Then extend the MarkdownProcessor to implement this functionality. For each element type, identify the text content, create TextSegment instances, and add them to the TranslationMap. Ensure proper handling of nested structures, especially for lists and blockquotes. Skip code blocks entirely as they are non-translatable."
        },
        {
          "id": 4,
          "title": "Extract text from inline elements",
          "description": "Implement extraction of translatable text from inline formatting elements",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Following TDD, write tests first for extracting text from inline elements including: emphasis (bold/italic), links (extract link text but not URLs), image alt text, and other inline formatting. Then implement the functionality to pass these tests. Create appropriate TextSegment instances for these elements, maintaining their relationship to parent block elements. Ensure inline code spans are preserved and not extracted for translation."
        },
        {
          "id": 5,
          "title": "Implement table content extraction",
          "description": "Add support for extracting text from table headers and cells",
          "status": "pending",
          "dependencies": [
            1,
            2
          ],
          "details": "Using TDD, write tests first for extracting text from tables, including both header cells and body cells. Then extend the MarkdownProcessor to implement this functionality. Create TextSegment instances for each cell's content, maintaining information about the table structure (row/column position). Handle any inline formatting within table cells by leveraging the inline extraction logic. Ensure table structure metadata is preserved for reassembly."
        },
        {
          "id": 6,
          "title": "Create segment reassembly functionality",
          "description": "Implement functionality to replace translated text back into the original AST",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Following TDD principles, write tests first for reassembling translated content back into the original document. Then implement methods in the MarkdownProcessor to take translated TextSegments and update the original AST with the translated content. Implement a reassembly algorithm that uses the stored node references and position information to correctly place translated text while preserving all non-translatable elements and document structure. Add validation to ensure all segments are accounted for and the document structure remains intact."
        }
      ]
    },
    {
      "id": 8,
      "title": "Implement TranslationService with DeepL integration",
      "description": "Create a service to send text to the DeepL API for translation.",
      "status": "pending",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "Implement a TranslationService class that sends extracted text to the DeepL API. Use the API key from configuration. Handle API response parsing, error handling, and retries. Include support for multiple target languages as specified in configuration.",
      "testStrategy": "Create mock API responses to test successful translations and error handling. If possible, test with actual DeepL API using small text samples. Verify correct handling of multiple target languages.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create TranslationService class structure and configuration",
          "description": "Set up the basic class structure for TranslationService and implement configuration loading for the DeepL API key and supported languages",
          "status": "pending",
          "dependencies": [],
          "details": "Create a TranslationService class with constructor that loads configuration. Implement methods to retrieve the API key from configuration and load the list of supported target languages. Add appropriate interfaces and dependency injection support. Include configuration validation to ensure required settings are present."
        },
        {
          "id": 2,
          "title": "Implement core translation method with DeepL API integration",
          "description": "Create the primary translation method that sends text to the DeepL API and processes the basic response",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Implement a TranslateText method that takes source text and target language code as parameters. Create the HTTP client to communicate with DeepL API. Build the request with proper authentication headers using the API key. Send the request to the DeepL translation endpoint and parse the basic successful response. Return the translated text."
        },
        {
          "id": 3,
          "title": "Add comprehensive error handling for API responses",
          "description": "Enhance the translation method with proper error handling for various API response scenarios",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "Implement error handling for different HTTP status codes (401 for authentication issues, 429 for rate limiting, etc.). Create custom exception types for different error scenarios. Parse error messages from the DeepL API response. Add logging for errors with appropriate severity levels. Ensure the service provides meaningful error messages to callers."
        },
        {
          "id": 4,
          "title": "Implement retry mechanism for transient failures",
          "description": "Add a retry mechanism to handle temporary failures when communicating with the DeepL API",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Implement an exponential backoff retry strategy for transient errors (network issues, 5xx responses, etc.). Configure retry count and delay parameters from configuration. Add circuit breaker pattern to prevent overwhelming the API during extended outages. Implement timeout handling. Add detailed logging for retry attempts."
        },
        {
          "id": 5,
          "title": "Add batch translation support and language detection",
          "description": "Extend the service with methods for batch translation and automatic language detection",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Implement a BatchTranslate method to efficiently translate multiple texts in a single API call. Add support for automatic source language detection using DeepL's detection capabilities. Create methods to validate if a target language is supported. Implement caching for frequently translated content to reduce API calls. Add usage statistics tracking to monitor API consumption."
        }
      ]
    },
    {
      "id": 9,
      "title": "Implement test mode for TranslationService",
      "description": "Add a test mode that simulates translation without making API calls.",
      "status": "pending",
      "dependencies": [
        8
      ],
      "priority": "medium",
      "details": "Extend TranslationService to check the TEST_MODE configuration flag. When enabled, return the original text instead of making API calls. Add logging to indicate when test mode is active. Ensure the same processing flow is followed regardless of test mode status.",
      "testStrategy": "Enable test mode and verify no API calls are made. Confirm the original text is returned as the 'translation'. Verify logging indicates test mode is active.",
      "subtasks": [
        {
          "id": 1,
          "title": "Add TEST_MODE configuration flag",
          "description": "Create a configuration flag that determines whether the translation service should operate in test mode",
          "status": "pending",
          "dependencies": [],
          "details": "Add a boolean TEST_MODE flag to the application's configuration system. This could be in an environment variable, configuration file, or wherever other configuration settings are stored. Default value should be false. Document the new configuration option in relevant documentation. Create a method in the configuration service to access this flag."
        },
        {
          "id": 2,
          "title": "Modify TranslationService to check for test mode",
          "description": "Update the TranslationService to check the TEST_MODE flag before processing translations",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Inject or access the configuration service in the TranslationService. At the beginning of the translation method, add logic to check if TEST_MODE is enabled. Create a private method like 'isTestModeEnabled()' that encapsulates this check for better readability and reuse. Ensure this check happens before any API call preparation."
        },
        {
          "id": 3,
          "title": "Implement test mode translation logic",
          "description": "Add conditional logic to return original text when in test mode instead of making API calls",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "In the main translation method, add a conditional branch that executes when test mode is enabled. In this branch, skip the API call and instead return the original text wrapped in the same response object structure that would normally be returned after a successful API call. Ensure all the same data transformations and validations still occur so the return signature is identical regardless of mode."
        },
        {
          "id": 4,
          "title": "Add logging for test mode operation",
          "description": "Implement logging to indicate when translations are being processed in test mode",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Add appropriate log statements that indicate when the service is operating in test mode. Log at INFO level when test mode is active and a translation is being simulated. Include relevant details such as the original text length, language pairs, and any other contextual information that would be useful for debugging. Ensure logs clearly distinguish between real translations and test mode translations."
        }
      ]
    },
    {
      "id": 10,
      "title": "Implement Markdown reconstruction from translated segments",
      "description": "Create functionality to rebuild Markdown content using translated text segments.",
      "status": "pending",
      "dependencies": [
        7,
        8
      ],
      "priority": "high",
      "details": "Extend MarkdownProcessor to replace original text segments in the AST with translated segments based on the mapping created during extraction. Implement methods to reconstruct the full Markdown content from the modified AST. Ensure all formatting, links, and non-translated elements are preserved correctly.",
      "testStrategy": "Create a test workflow with extraction, simulated translation, and reconstruction. Compare the reconstructed Markdown with expected output to verify structure preservation. Test with complex Markdown features.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create AST node replacement method",
          "description": "Implement a method in MarkdownProcessor that can replace text content in AST nodes while preserving node structure and attributes",
          "status": "pending",
          "dependencies": [],
          "details": "Create a method called `replaceNodeContent(node, originalText, translatedText)` that takes an AST node, the original text, and the translated text as parameters. The method should update the node's content with the translated text while preserving all node attributes, formatting, and structure. Handle different node types (paragraph, heading, list item, etc.) appropriately. Include unit tests to verify the method works correctly for various node types."
        },
        {
          "id": 2,
          "title": "Implement segment mapping lookup functionality",
          "description": "Create a mechanism to efficiently look up translated segments based on original text and node context",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Implement a `SegmentMapper` class that manages the mapping between original text segments and their translations. Include methods like `getTranslationForSegment(originalText, nodeContext)` that retrieves the appropriate translation based on the original text and contextual information about the node (like node type, position, etc.). This will handle cases where the same text might have different translations in different contexts. Add appropriate caching to optimize performance for repeated lookups."
        },
        {
          "id": 3,
          "title": "Develop AST traversal and modification algorithm",
          "description": "Create a recursive algorithm to traverse the Markdown AST and apply translations to each text node",
          "status": "pending",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement a method called `traverseAndTranslateAST(ast, segmentMapper)` that recursively walks through the AST, identifies text nodes that need translation, and applies the appropriate translations using the segment mapper and node replacement method. The algorithm should handle nested structures correctly and maintain the hierarchical relationships between nodes. Include special handling for nodes that should not be modified (like code blocks or already processed custom elements)."
        },
        {
          "id": 4,
          "title": "Implement special element preservation logic",
          "description": "Create handlers for preserving special Markdown elements like links, images, and formatting during reconstruction",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Extend the AST traversal algorithm with specialized handlers for complex Markdown elements. Implement methods like `preserveLinks(node)`, `preserveImages(node)`, and `preserveFormatting(node)` that ensure these elements are correctly maintained during translation. For links, ensure the link text is translated while the URL is preserved. For images, translate alt text while preserving the image path. For formatting (bold, italic, etc.), ensure the formatting markers are correctly applied to the translated text."
        },
        {
          "id": 5,
          "title": "Create Markdown reconstruction method",
          "description": "Implement functionality to convert the modified AST back into valid Markdown text",
          "status": "pending",
          "dependencies": [
            3,
            4
          ],
          "details": "Create a method called `reconstructMarkdown(ast)` that takes the modified AST and generates valid Markdown text. This should handle all Markdown syntax elements correctly, including headings, lists, code blocks, tables, and formatting. Ensure proper indentation and line breaks are maintained. The method should produce Markdown that is semantically equivalent to the original but with translated text segments."
        },
        {
          "id": 6,
          "title": "Add integration and validation functionality",
          "description": "Implement methods to validate the reconstructed Markdown and integrate the entire translation process",
          "status": "pending",
          "dependencies": [
            5
          ],
          "details": "Create a `validateReconstructedMarkdown(original, translated)` method that checks the structural integrity of the translated Markdown compared to the original. Implement the main public method `translateMarkdown(markdown, translations)` that orchestrates the entire process: parsing the original Markdown to AST, applying translations, reconstructing the Markdown, and validating the result. Add comprehensive error handling and logging to identify and report any issues during the translation process. Include integration tests that verify the entire workflow with various Markdown structures and translation scenarios."
        }
      ]
    },
    {
      "id": 11,
      "title": "Implement output file generation",
      "description": "Create functionality to write translated content to output files with proper directory structure.",
      "status": "pending",
      "dependencies": [
        10
      ],
      "priority": "high",
      "details": "Extend FileManager to construct output paths (OUTPUT_DIR/<lang_code>/...) and create necessary subdirectories. Implement methods to generate YAML frontmatter for translated files (setting lang, orig: false, including translated fields, copying others, adding source hashes). Write the reconstructed Markdown with frontmatter to output files.",
      "testStrategy": "Process test files and verify output files are created with correct paths and content. Confirm directory structure matches expectations. Verify YAML frontmatter in output files contains all required fields.",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend FileManager to construct output paths",
          "description": "Add methods to FileManager to construct output file paths based on language code and original file path",
          "status": "pending",
          "dependencies": [],
          "details": "Create a method `get_output_path(file_path, lang_code)` that takes an original file path and language code as inputs and returns the corresponding output path in the format OUTPUT_DIR/<lang_code>/.... The method should handle both absolute and relative paths correctly. Define OUTPUT_DIR as a configurable constant in the configuration file."
        },
        {
          "id": 2,
          "title": "Implement directory creation functionality",
          "description": "Add methods to create necessary subdirectories for output files",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Create a method `ensure_output_directory(output_path)` that checks if the directory for an output file exists and creates it if necessary. This should handle nested directories and ensure proper permissions. Use the output path construction from subtask 1 to determine which directories need to be created."
        },
        {
          "id": 3,
          "title": "Implement YAML frontmatter generation",
          "description": "Create functionality to generate YAML frontmatter for translated files",
          "status": "pending",
          "dependencies": [],
          "details": "Create a method `generate_frontmatter(original_frontmatter, translated_fields, lang_code, source_hash)` that takes the original frontmatter, translated fields, language code, and source hash as inputs. The method should set 'lang' to the language code, 'orig' to false, include all translated fields, copy other fields from the original frontmatter, and add the source hash for tracking changes. Return the frontmatter as a dictionary that can be serialized to YAML."
        },
        {
          "id": 4,
          "title": "Implement file writing functionality",
          "description": "Create methods to write translated content with frontmatter to output files",
          "status": "pending",
          "dependencies": [
            2,
            3
          ],
          "details": "Create a method `write_translated_file(output_path, frontmatter, content)` that takes an output path, frontmatter dictionary, and translated content as inputs. The method should convert the frontmatter to YAML format, combine it with the content using proper delimiters (---), and write the result to the specified output path. Ensure proper error handling for file writing operations."
        },
        {
          "id": 5,
          "title": "Integrate output generation into translation workflow",
          "description": "Connect the output file generation to the translation process",
          "status": "pending",
          "dependencies": [
            1,
            4
          ],
          "details": "Update the translation workflow to call the output file generation methods after translation is complete. For each translated file, construct the output path, ensure the directory exists, generate the frontmatter, and write the file. Add logging to track successful file generation and any errors that occur. Include a summary of generated files at the end of the translation process."
        }
      ]
    },
    {
      "id": 12,
      "title": "Implement source file hash update",
      "description": "Create functionality to update content_hash and yaml_hash in source files after processing.",
      "status": "pending",
      "dependencies": [
        5,
        11
      ],
      "priority": "high",
      "details": "Extend FileManager to update the content_hash and yaml_hash values in source file frontmatter after successful processing. Implement careful file writing to avoid data loss. Include error handling for file write failures.",
      "testStrategy": "Process test files and verify source files are updated with new hash values. Modify content, process again, and confirm hashes are updated. Test error handling by simulating write failures.",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend FileManager with hash update method",
          "description": "Add a new method to the FileManager class that will handle updating content_hash and yaml_hash in source file frontmatter",
          "status": "pending",
          "dependencies": [],
          "details": "Create a new method in FileManager called updateFileHashes(filePath, contentHash, yamlHash) that will be responsible for updating the hash values in a file's frontmatter. This method should accept the file path and the new hash values as parameters. Define the method signature and structure with appropriate documentation, but leave the implementation empty for now."
        },
        {
          "id": 2,
          "title": "Implement frontmatter parsing and modification",
          "description": "Create functionality to parse existing frontmatter, update hash values, and prepare the modified content",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Implement the core logic of the updateFileHashes method to: 1) Read the file content, 2) Parse the YAML frontmatter section, 3) Update the content_hash and yaml_hash values with the new values, 4) Reconstruct the file content with the updated frontmatter. Use existing YAML parsing libraries to handle the frontmatter. Ensure the formatting of the frontmatter is preserved as much as possible."
        },
        {
          "id": 3,
          "title": "Implement safe file writing mechanism",
          "description": "Create a safe file writing process that prevents data loss during updates",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "Enhance the updateFileHashes method with a safe file writing mechanism that: 1) Creates a temporary backup of the original file, 2) Writes the modified content to a temporary file, 3) Verifies the write was successful, 4) Replaces the original file with the temporary file using atomic operations when possible. This approach ensures that if any part of the process fails, the original file remains intact."
        },
        {
          "id": 4,
          "title": "Add comprehensive error handling",
          "description": "Implement error handling for all potential failure points in the hash update process",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Complete the updateFileHashes method by adding comprehensive error handling for: 1) File read errors, 2) YAML parsing errors, 3) File write errors, 4) File replacement errors. Each error case should be properly logged with meaningful error messages. The method should clean up any temporary files created during the process, even if an error occurs. Return appropriate success/failure status that can be used by calling code to determine if the update was successful."
        }
      ]
    },
    {
      "id": 13,
      "title": "Implement YAML field translation",
      "description": "Add support for translating specified YAML frontmatter fields.",
      "status": "pending",
      "dependencies": [
        7,
        8
      ],
      "priority": "medium",
      "details": "Extend MarkdownProcessor to identify YAML fields listed in YAML_TRANSLATE_FIELDS configuration. Extract text from these fields for translation. Update the mapping structure to include YAML field locations. Modify the TranslationService to handle both body content and YAML field text.",
      "testStrategy": "Configure test files with translatable YAML fields. Process files and verify both body content and specified YAML fields are translated. Confirm non-specified YAML fields remain unchanged.",
      "subtasks": [
        {
          "id": 1,
          "title": "Add YAML_TRANSLATE_FIELDS configuration option",
          "description": "Create a configuration option that allows users to specify which YAML frontmatter fields should be translated",
          "status": "pending",
          "dependencies": [],
          "details": "Add a new configuration option called YAML_TRANSLATE_FIELDS to the application's configuration system. This should be an array of strings representing field names in YAML frontmatter that should be translated. Update the configuration documentation to explain this new option. Implement validation to ensure the configuration value is an array of strings when provided."
        },
        {
          "id": 2,
          "title": "Extend MarkdownProcessor to identify translatable YAML fields",
          "description": "Modify the MarkdownProcessor to detect and extract YAML frontmatter fields that are marked for translation",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Update the MarkdownProcessor to parse YAML frontmatter at the beginning of markdown files. Add logic to check each field against the YAML_TRANSLATE_FIELDS configuration. For fields that match, extract their text content and store it separately from the main markdown body. Ensure the processor can handle various YAML data types (strings, arrays, nested objects) and extract translatable text appropriately."
        },
        {
          "id": 3,
          "title": "Update mapping structure to include YAML field locations",
          "description": "Extend the translation mapping structure to track the location and context of text from YAML fields",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "Modify the existing mapping structure to distinguish between body content and YAML field content. For YAML fields, store additional metadata including the field name, path (for nested fields), and original position in the document. This mapping should allow the system to correctly replace translated content back into the appropriate YAML fields. Create a consistent format for identifying YAML field locations that can be used throughout the translation pipeline."
        },
        {
          "id": 4,
          "title": "Modify TranslationService to process YAML field text",
          "description": "Update the TranslationService to handle translation of both markdown body and YAML field content",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Extend the TranslationService to process text from YAML fields alongside the main markdown content. Implement logic to handle different translation requirements for YAML fields (e.g., preserving formatting, handling special characters). Ensure the service can batch translate both types of content efficiently. Update any translation caching mechanisms to properly handle and identify YAML field content."
        },
        {
          "id": 5,
          "title": "Implement reassembly of translated YAML fields",
          "description": "Create functionality to reconstruct the markdown file with translated YAML fields and body content",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Develop the logic to reassemble a complete markdown file after translation. Use the mapping structure to place translated YAML field content back into the correct fields in the frontmatter. Ensure the YAML structure and formatting is preserved during reassembly. Add validation to verify that the reconstructed document maintains the original structure with only the content changed. Test with various YAML structures including nested objects and arrays."
        }
      ]
    },
    {
      "id": 14,
      "title": "Implement change detection logic",
      "description": "Create logic to determine if translation/update is needed based on hash comparison.",
      "status": "pending",
      "dependencies": [
        5,
        12
      ],
      "priority": "high",
      "details": "Implement logic to compare calculated hashes with stored hashes in source file frontmatter. Create decision logic to: 1) Trigger full translation if content_hash changes, 2) Trigger YAML-only update if only yaml_hash changes, 3) Skip file if hashes match. Include appropriate logging for each decision.",
      "testStrategy": "Create test scenarios for unchanged files, body content changes, and YAML-only changes. Verify the correct action is taken in each case. Confirm appropriate logging messages are generated.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create hash comparison function",
          "description": "Implement a function that compares calculated content and YAML hashes with stored hashes from frontmatter",
          "status": "pending",
          "dependencies": [],
          "details": "Create a function that takes calculated content_hash and yaml_hash as inputs along with the existing frontmatter data. The function should extract stored hashes from the frontmatter and perform comparison between new and stored hashes. Return a comparison result object with boolean flags indicating if content_hash changed, yaml_hash changed, or both match. Handle edge cases where hashes might not exist in frontmatter yet."
        },
        {
          "id": 2,
          "title": "Implement decision logic for translation actions",
          "description": "Create a function that determines the required action based on hash comparison results",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Build a function that takes the comparison result from subtask 1 and determines the appropriate action: 1) Full translation if content_hash changed, 2) YAML-only update if only yaml_hash changed, 3) Skip file if both hashes match. The function should return an action type enum/string and any relevant metadata needed for the next steps in the workflow. Include validation to ensure the comparison result contains all required data."
        },
        {
          "id": 3,
          "title": "Add logging for change detection decisions",
          "description": "Implement detailed logging for each decision path in the change detection logic",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "Enhance the decision logic function to include appropriate logging for each path: 1) Log when content has changed and full translation is needed, including the old and new hash values, 2) Log when only YAML data has changed and partial update is needed, 3) Log when file is skipped due to matching hashes, 4) Add debug logging for the comparison process itself. Use consistent log formatting and appropriate log levels (info for normal operation, debug for details, warn for potential issues)."
        },
        {
          "id": 4,
          "title": "Integrate change detection with workflow",
          "description": "Connect the change detection logic to the main translation workflow",
          "status": "pending",
          "dependencies": [
            2,
            3
          ],
          "details": "Integrate the hash comparison and decision logic into the main workflow. Ensure the workflow calls the change detection functions at the appropriate time after hashes are calculated but before translation begins. Modify the workflow to branch based on the determined action: proceed with full translation, perform YAML-only update, or skip the file. Add error handling for unexpected comparison results or missing hash data. Test the integrated workflow with various scenarios to verify correct branching behavior."
        }
      ]
    },
    {
      "id": 15,
      "title": "Implement YAML-only update for existing translated files",
      "description": "Create functionality to update only the YAML frontmatter of existing translated files.",
      "status": "pending",
      "dependencies": [
        13,
        14
      ],
      "priority": "medium",
      "details": "Extend FileManager to handle the YAML-only update scenario. Implement methods to read existing translated files, update only their YAML frontmatter (with translated fields if needed), and rewrite the files. Ensure the body content remains unchanged during this process.",
      "testStrategy": "Create test scenarios with existing translated files and YAML-only changes in source files. Verify only the YAML frontmatter is updated in translated files. Confirm body content remains unchanged.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create YAML frontmatter parsing utility",
          "description": "Develop a utility function that can extract YAML frontmatter from markdown files while preserving the body content separately",
          "status": "pending",
          "dependencies": [],
          "details": "Implement a utility function that takes a file path as input and returns two separate components: the YAML frontmatter as a structured object and the body content as a string. The function should handle edge cases like missing frontmatter delimiters (---) and invalid YAML. Use existing YAML parsing libraries but create a wrapper that specifically handles the frontmatter extraction pattern in markdown files."
        },
        {
          "id": 2,
          "title": "Implement YAML frontmatter modification function",
          "description": "Create a function that can update specific fields in the YAML frontmatter without modifying others",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Build on the parsing utility to create a function that accepts the parsed YAML object, a set of fields to update with their new values, and returns the modified YAML object. This function should preserve all existing fields not specified in the update set. Include validation to ensure the updated YAML remains valid and properly formatted. The function should handle nested YAML structures and array fields appropriately."
        },
        {
          "id": 3,
          "title": "Develop file rewriting mechanism with content preservation",
          "description": "Create a function that can rewrite a file with updated YAML frontmatter while preserving the original body content",
          "status": "pending",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement a function that takes a file path, the updated YAML object, and the preserved body content, then writes them back to the file. The function should format the YAML properly with the correct delimiter markers (---), maintain any whitespace patterns between the frontmatter and content, and ensure the body content remains completely unchanged. Include error handling for file system operations and implement an atomic write pattern (write to temp file, then rename) to prevent data loss if the operation fails midway."
        },
        {
          "id": 4,
          "title": "Extend FileManager with YAML-only update method",
          "description": "Add a new method to the FileManager class that orchestrates the YAML-only update process",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Create a new method in the FileManager class named 'updateYamlOnly' that takes a file path and a set of YAML fields to update. This method should use the previously created utilities to: 1) read and parse the file, 2) update only the specified YAML fields, and 3) write the file back with the updated frontmatter and unchanged body. Include appropriate logging and error handling. The method should return a success/failure status and details about what was updated."
        },
        {
          "id": 5,
          "title": "Implement batch processing for multiple translated files",
          "description": "Create functionality to perform YAML-only updates across multiple translated files",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Extend the FileManager with a method that can process multiple files in batch. This method should accept a list of file paths and corresponding YAML updates for each, or a pattern to identify files and a common update to apply. Implement parallel processing with appropriate throttling to handle large numbers of files efficiently. Include comprehensive error handling that allows the process to continue even if individual files fail, with detailed reporting of successes and failures. Add transaction-like behavior where possible to ensure consistency across the batch operation."
        }
      ]
    },
    {
      "id": 16,
      "title": "Implement support for WikiLinks syntax",
      "description": "Add support for handling Obsidian-style WikiLinks during translation.",
      "status": "pending",
      "dependencies": [
        7,
        10
      ],
      "priority": "low",
      "details": "Extend MarkdownProcessor to recognize WikiLinks patterns ([[LinkTarget]] or [[LinkTarget|Alias]]). Implement special handling to translate only the Alias portion while preserving the LinkTarget. Update the AST traversal and text extraction logic to handle these patterns correctly.",
      "testStrategy": "Create test Markdown files with various WikiLinks patterns. Verify only the Alias portions are extracted for translation. Confirm reconstructed Markdown preserves the correct WikiLinks structure.",
      "subtasks": [
        {
          "id": 1,
          "title": "Add WikiLinks pattern recognition to MarkdownProcessor",
          "description": "Extend the MarkdownProcessor to identify and parse Obsidian-style WikiLinks patterns in markdown text",
          "status": "pending",
          "dependencies": [],
          "details": "Create regular expressions to match both WikiLinks formats ([[LinkTarget]] and [[LinkTarget|Alias]]). Implement a method to identify these patterns in the source text before AST parsing. This will serve as the foundation for special handling of these elements. Add unit tests to verify pattern recognition works correctly for various WikiLinks formats."
        },
        {
          "id": 2,
          "title": "Extend AST to represent WikiLinks nodes",
          "description": "Create specialized AST node types to represent WikiLinks with their components (target and optional alias)",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Define new AST node classes (e.g., WikiLinkNode) that can store both the link target and alias components separately. Ensure these nodes properly integrate with the existing AST structure. Implement methods to distinguish between the target (which should not be translated) and alias (which should be translated). Update the AST builder to create these specialized nodes when WikiLinks patterns are encountered."
        },
        {
          "id": 3,
          "title": "Implement text extraction for WikiLinks translation",
          "description": "Modify the text extraction logic to properly handle WikiLinks, extracting only the alias portion for translation",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "Update the text extraction visitor to recognize WikiLink nodes. For WikiLinks with aliases ([[LinkTarget|Alias]]), extract only the alias text for translation. For simple WikiLinks ([[LinkTarget]]), determine if the target should be left untranslated or if the displayed text should be translated. Add appropriate context markers in the extracted text to identify WikiLinks segments for reassembly later."
        },
        {
          "id": 4,
          "title": "Implement translation result reintegration for WikiLinks",
          "description": "Develop logic to correctly reintegrate translated text back into WikiLinks structure",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Create a specialized handler for WikiLinks during the translation reintegration phase. Ensure that only the alias portion is replaced with translated text while preserving the link target. Maintain the correct WikiLinks syntax ([[target|translated-alias]]) in the final output. Handle edge cases such as nested delimiters and escaped characters within WikiLinks."
        },
        {
          "id": 5,
          "title": "Add comprehensive testing and documentation for WikiLinks support",
          "description": "Create tests and documentation for the WikiLinks translation functionality",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Develop integration tests that verify the entire WikiLinks translation workflow from recognition to final output. Test various scenarios including simple links, links with aliases, links with special characters, and nested structures. Document the WikiLinks support in the user guide, explaining how the feature works and any limitations. Update developer documentation to explain the implementation approach and how to extend it if needed."
        }
      ]
    },
    {
      "id": 17,
      "title": "Implement comprehensive error handling",
      "description": "Add robust error handling throughout the application.",
      "status": "pending",
      "dependencies": [
        11,
        12,
        15
      ],
      "priority": "medium",
      "details": "Review all components and add comprehensive error handling. Include specific handling for: configuration errors, file access issues, parsing errors, API errors (with retries), and file writing failures. Implement a consistent error reporting mechanism. Add recovery options where possible to continue processing other files when one fails.",
      "testStrategy": "Create test scenarios that trigger various error conditions. Verify errors are caught, reported clearly, and handled appropriately. Confirm the application can continue processing other files when possible.",
      "subtasks": [
        {
          "id": 1,
          "title": "Design error handling architecture",
          "description": "Create a consistent error handling architecture and reporting mechanism",
          "status": "pending",
          "dependencies": [],
          "details": "Define error types/categories (configuration, file access, parsing, API, file writing). Create a centralized error handling utility with standardized error objects. Design error logging format with severity levels, timestamps, context data, and stack traces. Define how errors propagate through the application. Document the architecture for team reference."
        },
        {
          "id": 2,
          "title": "Implement configuration and startup error handling",
          "description": "Add error handling for application configuration and startup processes",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Implement validation for configuration files/parameters with descriptive error messages. Add try-catch blocks around startup processes. Create fallback mechanisms for missing/invalid configurations. Implement graceful shutdown procedures for critical startup failures. Add detailed logging for configuration-related errors."
        },
        {
          "id": 3,
          "title": "Implement file access and parsing error handling",
          "description": "Add error handling for file operations and data parsing",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Add try-catch blocks around file read/write operations. Implement specific error types for file not found, permission issues, and corruption. Create recovery mechanisms to skip problematic files and continue processing others. Add validation for parsed data with appropriate error messages. Implement retry logic for temporary file access issues."
        },
        {
          "id": 4,
          "title": "Implement API error handling with retry mechanism",
          "description": "Add robust error handling for all API interactions",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Implement exponential backoff retry logic for API calls. Handle different HTTP status codes appropriately. Add timeout handling for API requests. Create circuit breaker pattern to prevent cascading failures. Implement fallback mechanisms when APIs are unavailable. Add detailed logging of API errors with request/response information."
        },
        {
          "id": 5,
          "title": "Implement UI error feedback mechanisms",
          "description": "Create user-friendly error messages and recovery options in the UI",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Design user-friendly error messages that avoid technical jargon. Implement toast notifications for non-critical errors. Create modal dialogs for critical errors requiring user action. Add retry buttons for failed operations where appropriate. Implement error boundaries in UI components to prevent complete UI crashes. Create an error details expansion option for technical users."
        },
        {
          "id": 6,
          "title": "Implement global error monitoring and reporting",
          "description": "Create a system to monitor, aggregate and report application errors",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Implement a global error handler to catch unhandled exceptions. Create an error dashboard showing error frequency and patterns. Add automated alerts for critical or frequent errors. Implement error aggregation to group similar errors. Create error reports with filtering options. Add telemetry to track error resolution times and effectiveness of error handling."
        }
      ]
    },
    {
      "id": 18,
      "title": "Implement detailed logging",
      "description": "Add comprehensive logging throughout the application.",
      "status": "pending",
      "dependencies": [
        17
      ],
      "priority": "medium",
      "details": "Implement detailed logging using Python's logging module. Include information about: files found, files skipped (with reason), files processed, translation status, files written, and errors encountered. Add configuration options for log level. Ensure logs are useful for debugging and monitoring progress.",
      "testStrategy": "Run the application with various log levels and verify appropriate information is logged. Create test scenarios that trigger different log messages. Confirm logs provide useful information for debugging issues.",
      "subtasks": [
        {
          "id": 1,
          "title": "Set up basic logging infrastructure",
          "description": "Create a logging configuration system that allows for different log levels and output destinations",
          "status": "pending",
          "dependencies": [],
          "details": "Create a logging module that initializes Python's logging system. Implement configuration options for log levels (DEBUG, INFO, WARNING, ERROR, CRITICAL) that can be set via command line arguments or configuration file. Set up appropriate log formatters that include timestamp, log level, and message. Configure log output destinations (console, file, or both). Create helper functions that other parts of the application can use to get properly configured logger instances."
        },
        {
          "id": 2,
          "title": "Implement file discovery and selection logging",
          "description": "Add logging for file discovery process and selection criteria",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Add logging statements to the file discovery and selection components. Log at INFO level when scanning directories. Log at DEBUG level for each file found. Log at INFO level for files skipped with the specific reason (e.g., 'file already translated', 'file type not supported'). Include file counts in summary logs. Ensure all potential error conditions during file discovery are logged at WARNING or ERROR level as appropriate."
        },
        {
          "id": 3,
          "title": "Implement translation process logging",
          "description": "Add detailed logging for the translation process",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Add logging statements throughout the translation process. Log at INFO level when translation begins and ends for each file. Log at DEBUG level for detailed translation steps (e.g., 'extracting text', 'sending to translation API', 'received translation'). Log translation statistics at INFO level (e.g., number of segments translated, characters processed). Log any translation errors or warnings at appropriate levels. Include performance metrics such as translation time per file at DEBUG level."
        },
        {
          "id": 4,
          "title": "Implement output and error handling logging",
          "description": "Add logging for file writing operations and comprehensive error handling",
          "status": "pending",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Add logging for all file writing operations. Log at INFO level when writing output files with destination paths. Log at DEBUG level for details about file content being written. Implement comprehensive error logging throughout the application, ensuring all exceptions are caught and logged with appropriate context. Create a system for aggregating errors and providing summary logs at the end of processing. Add logging for application startup and shutdown, including configuration settings at startup (at INFO level) and summary statistics at shutdown (number of files processed, success rate, etc.)."
        }
      ]
    },
    {
      "id": 19,
      "title": "Implement extensible translation provider system",
      "description": "Refactor TranslationService to support multiple translation providers.",
      "status": "pending",
      "dependencies": [
        8,
        9
      ],
      "priority": "low",
      "details": "Refactor TranslationService to use a provider pattern. Create an abstract base class or interface for translation providers. Implement the DeepL provider as the initial concrete implementation. Add factory method to create the appropriate provider based on API_PROVIDER configuration. Design for easy addition of new providers in the future.",
      "testStrategy": "Verify the DeepL provider works correctly through the new abstraction. Create a simple mock provider for testing. Confirm the factory correctly instantiates providers based on configuration.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create ITranslationProvider interface",
          "description": "Define the interface that all translation providers must implement",
          "status": "pending",
          "dependencies": [],
          "details": "Create an interface named ITranslationProvider that defines the contract for all translation providers. Include methods for translation (e.g., translateText, translateBatch), language detection, and any other common functionality. Add documentation to each method explaining its purpose, parameters, and return values. Consider adding methods for provider initialization and validation of API credentials."
        },
        {
          "id": 2,
          "title": "Implement DeepL provider",
          "description": "Create the first concrete implementation of ITranslationProvider for DeepL",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Create a DeepLTranslationProvider class that implements the ITranslationProvider interface. Move the existing DeepL-specific code from TranslationService into this new class. Ensure all interface methods are properly implemented. Handle API key configuration, request formatting, error handling, and response parsing specific to DeepL. Add unit tests to verify the implementation works correctly."
        },
        {
          "id": 3,
          "title": "Create TranslationProviderFactory",
          "description": "Implement a factory class to instantiate the appropriate provider",
          "status": "pending",
          "dependencies": [
            1,
            2
          ],
          "details": "Create a TranslationProviderFactory class with a createProvider method that returns an instance of ITranslationProvider based on configuration. Read the API_PROVIDER configuration value to determine which provider to instantiate. Initially support only the DeepL provider, but design the factory to be easily extended with additional providers. Include error handling for unknown or misconfigured providers. Consider implementing a singleton pattern if appropriate for your architecture."
        },
        {
          "id": 4,
          "title": "Refactor TranslationService",
          "description": "Update TranslationService to use the provider pattern",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Refactor the existing TranslationService to use the provider pattern. Remove DeepL-specific code that was moved to DeepLTranslationProvider. Update the service to get a provider instance from TranslationProviderFactory and delegate translation operations to it. Maintain the same public API to ensure backward compatibility with existing code. Update any configuration handling to work with the new provider system. Add appropriate error handling for provider failures."
        },
        {
          "id": 5,
          "title": "Add configuration and documentation",
          "description": "Update configuration files and add documentation for the new provider system",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Update configuration files to support the new provider system. Add documentation explaining how to configure different translation providers. Create a guide for implementing new providers, including a template or example. Update existing documentation to reflect the changes. Add comments in the code explaining the provider pattern implementation. Consider creating a simple mock provider for testing purposes. Verify that the existing functionality works with the new implementation through integration tests."
        }
      ]
    },
    {
      "id": 20,
      "title": "Implement main application workflow",
      "description": "Create the main application workflow that orchestrates all components.",
      "status": "pending",
      "dependencies": [
        14,
        15,
        17,
        18
      ],
      "priority": "high",
      "details": "Implement the main application class that orchestrates the entire workflow: load configuration, scan directories, process each file (check eligibility, calculate hashes, determine if translation is needed, extract content, translate, reconstruct, write output, update source hashes). Include progress reporting and summary statistics. Create a clean CLI interface with appropriate help text.",
      "testStrategy": "Create end-to-end tests with various scenarios (initial run, no changes, content changes, YAML changes). Verify the entire workflow functions correctly in each scenario. Test the CLI interface with various arguments and options.",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement configuration loading and validation",
          "description": "Create the initial part of the main application class that handles loading and validating configuration settings",
          "status": "pending",
          "dependencies": [],
          "details": "Implement a method to load configuration from command line arguments, environment variables, and/or config files. Include validation logic to ensure all required settings are present and valid. Handle configuration errors gracefully with informative error messages. This should include settings for source/target directories, language pairs, file types to process, and any API credentials needed for translation services."
        },
        {
          "id": 2,
          "title": "Implement directory scanning and file collection",
          "description": "Create functionality to scan directories and collect eligible files for processing",
          "status": "pending",
          "dependencies": [
            1
          ],
          "details": "Using the validated configuration, implement methods to recursively scan source directories and identify files that need processing. Apply file type filters based on configuration. Create a data structure to track files for processing with their metadata (path, size, last modified date). Include logic to skip files that should be excluded based on configuration rules (e.g., hidden files, specific patterns)."
        },
        {
          "id": 3,
          "title": "Implement file eligibility and hash calculation",
          "description": "Add logic to determine which files need translation by checking eligibility and calculating hashes",
          "status": "pending",
          "dependencies": [
            2
          ],
          "details": "For each collected file, implement the logic to: 1) Calculate file hash, 2) Check against stored hashes to determine if translation is needed, 3) Create a prioritized queue of files that need processing. Include logic to handle file access errors gracefully. This component should prepare the final list of files that will proceed to content extraction and translation."
        },
        {
          "id": 4,
          "title": "Implement the core processing pipeline",
          "description": "Create the main processing loop that handles content extraction, translation, and reconstruction",
          "status": "pending",
          "dependencies": [
            3
          ],
          "details": "Implement the core processing pipeline that: 1) Extracts translatable content from each file, 2) Sends content for translation, 3) Reconstructs the translated file with the original formatting preserved, 4) Writes the output to the target location. This should handle different file types appropriately by delegating to the correct extractor/translator components. Include proper error handling for each step."
        },
        {
          "id": 5,
          "title": "Implement progress reporting and statistics",
          "description": "Add functionality to track and report progress during processing",
          "status": "pending",
          "dependencies": [
            4
          ],
          "details": "Enhance the processing pipeline with progress reporting. Implement: 1) Real-time progress indicators (percentage complete, files processed/remaining), 2) Logging of important events and errors, 3) Collection of statistics (files processed, translation volume, errors encountered, time taken). Design this to work in both interactive terminal environments and non-interactive contexts (like CI/CD pipelines)."
        },
        {
          "id": 6,
          "title": "Implement CLI interface and help documentation",
          "description": "Create a user-friendly command-line interface with comprehensive help text",
          "status": "pending",
          "dependencies": [
            5
          ],
          "details": "Finalize the application by implementing a clean CLI interface. Include: 1) Command-line argument parsing with sensible defaults, 2) Comprehensive help text explaining all options and their usage, 3) Examples of common use cases, 4) Version information and attribution. Ensure the interface follows CLI best practices (e.g., supporting both short and long option formats, providing clear error messages for invalid inputs)."
        }
      ]
    }
  ],
  "metadata": {
    "projectName": "Markdown Translation Tool",
    "totalTasks": 20,
    "sourceFile": "scripts/prd.txt",
    "generatedAt": "2023-11-10"
  }
}