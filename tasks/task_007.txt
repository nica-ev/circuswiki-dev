# Task ID: 7
# Title: Implement translatable text extraction
# Status: pending
# Dependencies: 6
# Priority: high
# Description: Create functionality to extract translatable text segments from Markdown content, including complex inline syntax like WikiLinks, Obsidian attributes, and potentially HTML, using a TDD approach. This involves parsing the AST and potentially implementing a secondary inline parser.
# Details:
Extend MarkdownProcessor or implement a secondary parsing stage to handle complex inline sequences within blocks (paragraphs, lists, etc.). Identify and extract only translatable text from: paragraphs, lists, table cells, headers, emphasis, standard link text, image alt text, WikiLink aliases (`[[target|alias]]`), and potentially content within specified HTML tags. Create a robust data structure mapping extracted segments to their precise original locations (including within inline sequences) for accurate reassembly. Preserve all non-translatable elements: code blocks, inline code, URLs, WikiLink targets (`[[target]]`), Obsidian attributes (`{ .some-attr }`), HTML tags (unless configured for translation), and other non-text syntax. Follow TDD principles rigorously, especially for parsing inline sequences and reassembly.

# Test Strategy:
Following TDD principles, write tests first for each component before implementation. Process Markdown files with various elements and verify all translatable text is correctly extracted. Confirm non-translatable elements are preserved. Verify the mapping between extracted text and original locations is accurate. Add emphasis on testing complex inline sequences: Process Markdown files containing mixed text, WikiLinks, attributes, inline code, standard links/images, and HTML. Verify correct extraction of only translatable parts (e.g., WikiLink alias, text nodes) and accurate preservation of all non-translatable syntax. Verify the mapping enables perfect reconstruction after simulated translation.

# Subtasks:
## 1. Define text segment data structure [done]
### Dependencies: None
### Description: Create a data structure to represent extractable text segments and their mapping back to the AST
### Details:
Using TDD, first write tests for the TextSegment class/interface that contains: 1) the extracted text content, 2) a reference to the original AST node, 3) metadata about the segment type (paragraph, heading, list item, etc.), 4) position information for reassembly. Then implement the class to pass the tests. Similarly, test-drive the implementation of a TranslationMap class to store and manage collections of TextSegments with methods to add, retrieve, and manipulate segments.

<info added on 2025-05-04T03:37:30.837Z>
The reference/position information must be highly granular to support accurate reassembly of complex inline sequences (mixed text, links, code, attributes). Consider using character offsets, node paths within inline parses, or unique segment IDs tied to a detailed parse tree. For inline elements, implement a nested segment approach where parent segments contain references to child segments with precise boundary information. Use a combination of absolute document position and relative node position to handle cases where the same text appears multiple times. Consider implementing a "breadcrumb trail" of node types and indices to ensure exact placement during reassembly, especially for nested inline formatting like `**bold _italic_ text**` where order matters. Test with complex cases involving nested formatting, links with formatted text, and code blocks with annotations.
</info added on 2025-05-04T03:37:30.837Z>

<info added on 2025-05-04T03:47:59.766Z>
<info added on 2025-05-05T14:22:45.123Z>
## Implementation Details

### TextSegment Interface
```typescript
interface TextSegment {
  id: string;                    // Unique identifier
  content: string;               // Extracted text content
  astNode: Node;                 // Reference to original AST node
  segmentType: SegmentType;      // Type classification
  position: PositionInfo;        // Position metadata
  parentId?: string;             // Optional reference to parent segment
  childIds?: string[];           // Optional references to child segments
}

enum SegmentType {
  PARAGRAPH,
  HEADING,
  LIST_ITEM,
  CODE_BLOCK,
  INLINE_CODE,
  BOLD,
  ITALIC,
  LINK,
  IMAGE,
  TABLE_CELL,
  // Add other types as needed
}

interface PositionInfo {
  startOffset: number;           // Character offset in document
  endOffset: number;             // End character offset
  nodePath: string[];            // Path of indices to node in AST
  breadcrumbs: NodeBreadcrumb[]; // Detailed path information
}

interface NodeBreadcrumb {
  nodeType: string;              // Type of node
  index: number;                 // Index within parent's children
  attributes?: Record<string, any>; // Any attributes to preserve
}
```

### TranslationMap Class
```typescript
class TranslationMap {
  private segments: Map<string, TextSegment> = new Map();
  private orderedIds: string[] = [];
  
  addSegment(segment: TextSegment): void {
    if (this.segments.has(segment.id)) {
      throw new Error(`Segment with ID ${segment.id} already exists`);
    }
    this.segments.set(segment.id, segment);
    this.orderedIds.push(segment.id);
    
    // If this segment has a parent, update the parent's childIds
    if (segment.parentId && this.segments.has(segment.parentId)) {
      const parent = this.segments.get(segment.parentId)!;
      parent.childIds = parent.childIds || [];
      parent.childIds.push(segment.id);
    }
  }
  
  getSegment(id: string): TextSegment | undefined {
    return this.segments.get(id);
  }
  
  getAllSegments(): TextSegment[] {
    return this.orderedIds.map(id => this.segments.get(id)!);
  }
  
  getNestedSegments(): TextSegment[] {
    // Return only top-level segments (those without parents)
    return this.getAllSegments().filter(segment => !segment.parentId);
  }
  
  getChildSegments(parentId: string): TextSegment[] {
    const parent = this.segments.get(parentId);
    if (!parent || !parent.childIds) return [];
    return parent.childIds.map(id => this.segments.get(id)!);
  }
}
```

### Segment ID Generation Strategy
Implement a deterministic ID generation function that combines:
1. Document-level position information
2. Node type
3. Content hash (for uniqueness when same content appears multiple times)

```typescript
function generateSegmentId(node: Node, position: PositionInfo, content: string): string {
  const contentHash = createHash('md5').update(content).digest('hex').substring(0, 8);
  return `${position.startOffset}-${position.endOffset}-${node.type}-${contentHash}`;
}
```

This approach ensures reliable reassembly even with complex nested structures and repeated content patterns.
</info added on 2025-05-05T14:22:45.123Z>
</info added on 2025-05-04T03:47:59.766Z>

<info added on 2025-05-04T03:48:28.866Z>
<info added on 2025-05-06T15:30:12.456Z>
## Implementation Plan (TDD Approach)

### Phase 1: Setup and Basic Implementation
1. **Setup Files**:
   - Create `src/processing/segments.ts`
   - Create `src/processing/segments.test.ts`

2. **Define Types/Interfaces (`segments.ts`)**:
   - Define basic `TextSegment` structure (start simple: id, content, type, basic position)
   - Define basic `TranslationMap` structure (start simple: addSegment, getSegmentById)

3. **Write Initial Tests (`segments.test.ts`)**:
   ```typescript
   describe('TextSegment', () => {
     it('should create a valid segment with minimal data', () => {
       const segment = {
         id: 'seg1',
         content: 'Sample text',
         segmentType: SegmentType.PARAGRAPH,
         astNode: mockAstNode,
         position: { startOffset: 0, endOffset: 11, nodePath: [], breadcrumbs: [] }
       };
       expect(segment.id).toBe('seg1');
       expect(segment.content).toBe('Sample text');
     });
   });

   describe('TranslationMap', () => {
     it('should add and retrieve segments', () => {
       const map = new TranslationMap();
       const segment = createTestSegment('seg1', 'Sample text');
       
       map.addSegment(segment);
       expect(map.getSegment('seg1')).toEqual(segment);
     });
   });
   ```

4. **Implement to Pass Tests (`segments.ts`)**:
   - Implement the minimal `TextSegment` interface and `TranslationMap` class

### Phase 2: Refinement and Advanced Features
5. **Refine Position Information**:
   - Test: Write tests for complex position data handling
   ```typescript
   it('should store and retrieve detailed position information', () => {
     const breadcrumbs = [
       { nodeType: 'document', index: 0 },
       { nodeType: 'paragraph', index: 2, attributes: { className: 'intro' } }
     ];
     const segment = createTestSegment('seg2', 'Text with position', {
       startOffset: 45,
       endOffset: 62,
       nodePath: ['0', '2', '0'],
       breadcrumbs
     });
     
     const map = new TranslationMap();
     map.addSegment(segment);
     const retrieved = map.getSegment('seg2');
     
     expect(retrieved?.position.breadcrumbs).toEqual(breadcrumbs);
     expect(retrieved?.position.nodePath).toEqual(['0', '2', '0']);
   });
   ```

6. **Implement Nested Segments**:
   - Test: Write tests for parent-child relationships
   ```typescript
   it('should handle parent-child segment relationships', () => {
     const map = new TranslationMap();
     const parent = createTestSegment('parent1', 'Parent text');
     const child1 = createTestSegment('child1', 'Child 1', undefined, 'parent1');
     const child2 = createTestSegment('child2', 'Child 2', undefined, 'parent1');
     
     map.addSegment(parent);
     map.addSegment(child1);
     map.addSegment(child2);
     
     expect(map.getChildSegments('parent1')).toHaveLength(2);
     expect(map.getChildSegments('parent1')[0].id).toBe('child1');
     expect(map.getNestedSegments()).toHaveLength(1); // Only parent is top-level
   });
   ```

7. **Test Edge Cases**:
   ```typescript
   it('should throw error when adding duplicate segment ID', () => {
     const map = new TranslationMap();
     const segment1 = createTestSegment('dup', 'First');
     const segment2 = createTestSegment('dup', 'Second');
     
     map.addSegment(segment1);
     expect(() => map.addSegment(segment2)).toThrow(/already exists/);
   });
   
   it('should handle complex nested formatting correctly', () => {
     // Create a complex structure with multiple nesting levels
     const boldSegment = createTestSegment('bold1', '**bold _italic_ text**');
     const italicSegment = createTestSegment('italic1', '_italic_', undefined, 'bold1');
     
     const map = new TranslationMap();
     map.addSegment(boldSegment);
     map.addSegment(italicSegment);
     
     // Test retrieval and relationships
     expect(map.getSegment('bold1')?.childIds).toContain('italic1');
     expect(map.getSegment('italic1')?.parentId).toBe('bold1');
   });
   ```

8. **Test ID Generation**:
   ```typescript
   describe('generateSegmentId', () => {
     it('should generate deterministic IDs for the same input', () => {
       const node = mockAstNode;
       const position = { startOffset: 10, endOffset: 20, nodePath: ['0', '1'], breadcrumbs: [] };
       const content = 'Test content';
       
       const id1 = generateSegmentId(node, position, content);
       const id2 = generateSegmentId(node, position, content);
       
       expect(id1).toBe(id2);
     });
     
     it('should generate different IDs for different content', () => {
       const node = mockAstNode;
       const position = { startOffset: 10, endOffset: 20, nodePath: ['0', '1'], breadcrumbs: [] };
       
       const id1 = generateSegmentId(node, position, 'Content A');
       const id2 = generateSegmentId(node, position, 'Content B');
       
       expect(id1).not.toBe(id2);
     });
   });
   ```

### Helper Functions for Testing
```typescript
function createTestSegment(
  id: string, 
  content: string, 
  position?: Partial<PositionInfo>,
  parentId?: string
): TextSegment {
  return {
    id,
    content,
    astNode: mockAstNode,
    segmentType: SegmentType.PARAGRAPH,
    position: {
      startOffset: 0,
      endOffset: content.length,
      nodePath: [],
      breadcrumbs: [],
      ...position
    },
    parentId,
    childIds: []
  };
}

const mockAstNode = { type: 'paragraph', children: [] };
```
</info added on 2025-05-06T15:30:12.456Z>
</info added on 2025-05-04T03:48:28.866Z>

<info added on 2025-05-04T03:48:40.938Z>
## TDD Implementation Plan (Attempt 3)

**Phase 1: Setup & Basics**
1. Create `src/processing/segments.ts` and `src/processing/segments.test.ts`.
2. Define initial `TextSegment`, `PositionInfo`, `SegmentType`, `NodeBreadcrumb` interfaces and `TranslationMap` class structure in `segments.ts`.
3. Write basic tests in `segments.test.ts`:
   - `TextSegment` creation.
   - `TranslationMap.addSegment`.
   - `TranslationMap.getSegment`.
4. Implement minimal code in `segments.ts` to make basic tests pass.

**Phase 2: Feature Implementation (Iterative)**
5. **Complex Positions**: Test -> Implement -> Test complex `PositionInfo` (breadcrumbs, nodePath).
6. **Segment Ordering**: Test -> Implement -> Test `TranslationMap.getAllSegments` returns in added order.
7. **Nesting**: Test -> Implement -> Test `TextSegment` `parentId`/`childIds` & `TranslationMap.getChildSegments` / `getNestedSegments`.
8. **Duplicate Check**: Test -> Implement -> Test `TranslationMap.addSegment` throws on duplicate ID.
9. **ID Generation**: Test -> Implement -> Test `generateSegmentId` utility (if created separately).

**Phase 3: Finalize**
10. Review all code and tests.
11. Ensure all requirements from the initial description and previous notes are met.
</info added on 2025-05-04T03:48:40.938Z>

## 2. Implement AST visitor pattern [done]
### Dependencies: 7.1
### Description: Create a visitor pattern implementation to traverse the Markdown AST
### Details:
Following TDD principles, write tests first for a MarkdownAstVisitor class that can traverse all node types in the Markdown AST. This should follow the visitor design pattern with methods like visitParagraph(), visitHeading(), visitList(), etc. The visitor should maintain context during traversal (like nesting level) and provide hooks for processing different node types. Implement the visitor to pass the tests. This will serve as the foundation for text extraction in the next steps.

<info added on 2025-05-04T03:38:43.613Z>
The visitor should integrate with the Granular Inline Parser (Subtask 7.12) when processing block-level nodes that contain inline content. Specifically:

1. For nodes like paragraphs, list items, table cells, and blockquotes, the visitor should delegate inline content processing to the Granular Inline Parser.

2. Implement a method like `processInlineContent(String content)` that utilizes the inline parser to generate a token stream of inline elements (emphasis, links, code spans, etc.).

3. The visitor should maintain a stack-based context that tracks not only nesting level but also the current inline parsing state when traversing mixed content.

4. Add callback hooks that allow consumers to intercept both block-level nodes and the fine-grained inline tokens.

5. Consider implementing a composite pattern where block visitors can contain inline visitors, allowing for specialized processing at different structural levels.

This approach ensures consistent handling of complex inline formatting across all block contexts and provides maximum flexibility for downstream processors.
</info added on 2025-05-04T03:38:43.613Z>

<info added on 2025-05-04T03:54:17.908Z>
<info added on 2025-05-05T14:22:10.000Z>
## Implementation Details for AST Visitor Pattern

### Core Visitor Architecture
- Implement a double-dispatch mechanism where nodes accept visitors via an `accept(visitor)` method, complementing the visitor's `visit_*` methods
- Use Python's `getattr()` with fallback for dynamic method dispatch: `getattr(self, f"visit_{node_type}", self.visit_default)(node)`

### Context Management
- Implement a `VisitorContext` class to track:
  ```python
  class VisitorContext:
      def __init__(self):
          self.depth = 0
          self.path = []  # Stack of node types representing current position
          self.ancestors = []  # Stack of actual parent nodes
          self.inline_state = None  # For inline parsing state
  ```

- Add context manipulation methods:
  ```python
  def enter_node(self, node):
      self.context.depth += 1
      self.context.path.append(node['type'])
      self.context.ancestors.append(node)
      
  def exit_node(self, node):
      self.context.depth -= 1
      self.context.path.pop()
      self.context.ancestors.pop()
  ```

### Visitor Implementation Pattern
- For each node type, implement a pattern like:
  ```python
  def visit_heading_open(self, node):
      self.enter_node(node)
      # Process heading attributes (level, etc.)
      result = self.process_node(node)
      # Visit children if any
      self.visit_children(node)
      self.exit_node(node)
      return result
  ```

### Testing Strategy
- Create mock nodes for testing without requiring full parser:
  ```python
  def create_test_ast():
      return [
          {'type': 'heading_open', 'level': 1, 'children': []},
          {'type': 'text', 'content': 'Heading text'},
          {'type': 'heading_close'},
          # More nodes...
      ]
  ```

- Test context maintenance with nested structures:
  ```python
  def test_context_tracking():
      visitor = TestVisitor()
      visitor.visit(create_nested_test_ast())
      assert visitor.max_depth == 3  # Expected max nesting
      assert visitor.path_history == [['root'], ['root', 'list'], ['root', 'list', 'item']]
  ```

- Test visitor extension through inheritance:
  ```python
  class CustomVisitor(MarkdownAstVisitor):
      def visit_code_block(self, node):
          # Custom handling for code blocks
          self.code_blocks.append(node['content'])
          return super().visit_code_block(node)
  ```
</info added on 2025-05-05T14:22:10.000Z>
</info added on 2025-05-04T03:54:17.908Z>

## 3. Extract text from block elements [done]
### Dependencies: 7.1, 7.2
### Description: Implement extraction of translatable text from block-level elements
### Details:
Using TDD, write tests first for extracting text from block-level elements including: paragraphs, headings (h1-h6), blockquotes, and list items. Then extend the MarkdownProcessor to implement this functionality. For each element type, identify the text content, create TextSegment instances, and add them to the TranslationMap. Ensure proper handling of nested structures, especially for lists and blockquotes. Skip code blocks entirely as they are non-translatable.

<info added on 2025-05-04T15:16:52.063Z>
```typescript
// Implementation details for extracting text from block elements

// Test structure example
describe('MarkdownProcessor - block element extraction', () => {
  test('extracts text from paragraphs', () => {
    const markdown = '## Header\n\nThis is a paragraph.\n\nThis is another paragraph.';
    const processor = new MarkdownProcessor();
    const result = processor.extractTranslatableSegments(markdown);
    
    expect(result.segments.length).toBe(3);
    expect(result.segments[1].text).toBe('This is a paragraph.');
    expect(result.segments[1].type).toBe('paragraph');
  });
  
  test('handles nested blockquotes correctly', () => {
    const markdown = '> Outer quote\n> > Inner quote\n> More outer';
    // Assertions for proper nesting handling
  });
  
  test('skips code blocks', () => {
    const markdown = 'Text\n\n```js\nconst x = 1;\n```\n\nMore text';
    // Verify only "Text" and "More text" are extracted
  });
});

// Implementation approach
function extractBlockElements($: CheerioAPI, translationMap: TranslationMap): void {
  // Process paragraphs
  $('p').each((_, element) => {
    if (!isWithinCodeBlock(element)) {
      const text = $(element).text().trim();
      if (text) {
        translationMap.addSegment(new TextSegment(text, 'paragraph', getElementPath(element)));
      }
    }
  });
  
  // Process headings with level detection
  $('h1, h2, h3, h4, h5, h6').each((_, element) => {
    const level = element.name.substring(1); // Extract heading level (1-6)
    const text = $(element).text().trim();
    if (text) {
      translationMap.addSegment(new TextSegment(text, `heading-${level}`, getElementPath(element)));
    }
  });
  
  // Helper function to check if element is within a code block
  function isWithinCodeBlock(element: CheerioElement): boolean {
    return $(element).parents('pre, code').length > 0;
  }
  
  // Helper to generate path for element location tracking
  function getElementPath(element: CheerioElement): string {
    // Implementation to create a path like "body > article > p:nth-child(3)"
  }
}
```
</info added on 2025-05-04T15:16:52.063Z>

## 4. Extract text from inline elements [done]
### Dependencies: 7.1, 7.2, 7.3
### Description: Implement extraction of translatable text from inline formatting elements
### Details:
Following TDD, write tests first for extracting text from inline elements including: emphasis (bold/italic), links (extract link text but not URLs), image alt text, and other inline formatting. Then implement the functionality to pass these tests. Create appropriate TextSegment instances for these elements, maintaining their relationship to parent block elements. Ensure inline code spans are preserved and not extracted for translation.

<info added on 2025-05-04T03:38:54.851Z>
For this subtask, I'll add information about processing the Granular Inline Parser output and handling WikiLink aliases:

The implementation should now consume the structured output from the Granular Inline Parser (7.12) rather than parsing inline elements directly. Create a mapping system that tracks the relationship between original inline elements and their extracted TextSegment instances to facilitate accurate reassembly later.

When processing inline elements:
- For WikiLink aliases (e.g., `[[Page|Displayed Text]]`), extract only the display text portion ("Displayed Text") as translatable
- Maintain metadata about each inline element's type and position in the document structure
- Implement special handling for nested inline elements (e.g., bold text within links)
- Create a clear separation between translatable content and non-translatable syntax elements like Obsidian-specific attributes
- Design the extraction to be reversible, ensuring translated content can be correctly reinserted into the original inline structure

Update dependencies to include subtask 7.12 (Granular Inline Parser) as this subtask now processes its output rather than performing its own parsing.
</info added on 2025-05-04T03:38:54.851Z>

<info added on 2025-05-04T15:47:15.412Z>
For this subtask implementation, I'll add specific technical details about handling inline elements:

When implementing the `_extract_inline_text` method:

```python
def _extract_inline_text(self, inline_tokens, parent_segment=None):
    """Process inline tokens to extract translatable text segments."""
    segments = []
    current_text = ""
    
    for token in inline_tokens:
        if token.type == 'text':
            current_text += token.content
        elif token.type in ('em_open', 'strong_open', 'link_open'):
            # Process nested content within emphasis/links
            nested_segments = self._extract_inline_text(token.children, parent_segment)
            segments.extend(nested_segments)
        elif token.type == 'image':
            # Extract alt text if present
            if token.attrs.get('alt'):
                alt_segment = TextSegment(
                    text=token.attrs['alt'],
                    segment_type=SegmentType.INLINE_ALT_TEXT,
                    parent=parent_segment
                )
                segments.append(alt_segment)
        elif token.type == 'code_inline':
            # Skip code spans - they shouldn't be translated
            pass
    
    # Create segment for accumulated text if any
    if current_text:
        text_segment = TextSegment(
            text=current_text,
            segment_type=SegmentType.INLINE_TEXT,
            parent=parent_segment
        )
        segments.append(text_segment)
    
    return segments
```

Key implementation considerations:
- Store position metadata (start/end indices) with each segment to enable accurate reassembly
- Create a bidirectional mapping between original markdown and extracted segments using a dictionary structure
- For nested elements, maintain proper hierarchy using parent references
- Implement special handling for markdown-it-py's token structure, particularly for nested tokens
- Use custom segment types (enum values) to differentiate between regular text, link text, and alt text
- Consider implementing a visitor pattern for cleaner token processing logic
- Add context attributes to segments to preserve formatting information needed during reassembly

When consuming Granular Inline Parser output, implement a transformation layer that converts the parser's AST into TextSegment instances while preserving the hierarchical relationships.
</info added on 2025-05-04T15:47:15.412Z>

## 5. Implement table content extraction [done]
### Dependencies: 7.1, 7.2
### Description: Add support for extracting text from table headers and cells
### Details:
Using TDD, write tests first for extracting text from tables, including both header cells and body cells. Then extend the MarkdownProcessor to implement this functionality. Create TextSegment instances for each cell's content, maintaining information about the table structure (row/column position). Handle any inline formatting within table cells by leveraging the inline extraction logic. Ensure table structure metadata is preserved for reassembly.

<info added on 2025-05-04T15:57:32.076Z>
```
Implementation Plan (Subtask 7.5 - Table Extraction):
1. **TDD - Write Tests (`tests/processing/test_markdown_processor.py`):**
    - Test cases for simple tables (headers `<th>`, data cells `<td>`).
    - Include tests with inline formatting within cells.
    - Verify empty cells produce no segments.
    - Check segment types: `table_header_cell`, `table_data_cell`.
    - Attempt to include basic row/column info in the path (e.g., `table_1 > tr_2 > td_3`).
2. **Implement Extraction (`src/processing/markdown_processor.py`):**
    - Enhance `extract_translatable_segments` to detect table tokens (`table_open/close`, `thead_open/close`, `tbody_open/close`, `tr_open/close`, `th_open/close`, `td_open/close`).
    - Maintain context (current row/cell index) while inside a table.
    - For `th_open`/`td_open`, locate the next `inline` token.
    - Extract text using `_extract_inline_text` from the `inline` token's children.
    - Create `TextSegment` with correct type and path including row/col info.
    - Add segments to `TranslationMap`.
    - Ensure correct `token_stack` management for table elements.
3. **Test & Refine:** Run `pytest` and iterate on implementation.
4. **Log Progress:** Update subtask details if needed.

**Technical Considerations:**
- Table structure in markdown-it-py tokens follows HTML structure: `table > thead/tbody > tr > th/td > inline > text/code_inline/etc`
- Track table context with variables like `current_table_id`, `current_row`, `current_cell`
- For complex tables, consider creating a `TableContext` class to manage state
- Store cell coordinates in segment metadata: `{"row": row_idx, "col": col_idx, "is_header": bool}`
- Handle merged cells by checking for `colspan` and `rowspan` attributes in tokens
- For reassembly, maintain original token indices to preserve exact table structure
```
</info added on 2025-05-04T15:57:32.076Z>

## 6. Create segment reassembly functionality [done]
### Dependencies: 7.1, 7.2, 7.3, 7.4, 7.5
### Description: Implement functionality to replace translated text back into the original AST
### Details:
Following TDD principles, write tests first for reassembling translated content back into the original document. Then implement methods in the MarkdownProcessor to take translated TextSegments and update the original AST with the translated content. Implement a reassembly algorithm that uses the stored node references and position information to correctly place translated text while preserving all non-translatable elements and document structure. Add validation to ensure all segments are accounted for and the document structure remains intact.

<info added on 2025-05-04T03:39:04.859Z>
The reassembly process must accurately reconstruct complex inline sequences by leveraging the detailed mapping created in task 7.1 and the structured output from the Granular Inline Parser (7.12). When reassembling, the algorithm should:

1. Use position markers and node references to precisely interleave translated text segments with preserved non-translatable syntax elements
2. Maintain the integrity of inline code blocks, WikiLink targets, HTML tag attributes, URLs, and other elements identified as non-translatable in task 7.13
3. Implement a verification step that confirms all inline elements are correctly positioned relative to each other after reassembly
4. Handle nested structures (like formatting within links, or links within formatting) by respecting the hierarchy established in the original AST
5. Preserve whitespace patterns between inline elements to maintain document formatting

Consider implementing a two-phase reassembly approach: first reconstruct inline sequences at the leaf nodes, then rebuild the document hierarchy from bottom up. This ensures complex inline formatting is preserved while maintaining the overall document structure.

Dependencies should be updated to include tasks 7.12 and 7.13, as the reassembly logic depends on both the granular parsing of inline content and the non-translatable element preservation strategy.
</info added on 2025-05-04T03:39:04.859Z>

<info added on 2025-05-04T15:59:42.340Z>
<info added on 2025-05-05T14:22:18.000Z>
## Implementation Details for Segment Reassembly

### Core Data Structures
1. Create a `ReassemblyContext` class to track state during reassembly:
   ```python
   class ReassemblyContext:
       def __init__(self):
           self.path_stack = []
           self.current_path = ""
           self.in_table = False
           self.row_index = 0
           self.col_index = 0
           self.pending_tokens = []  # For buffering tokens during reassembly
   ```

2. Implement a `SegmentMap` class to efficiently lookup translated segments:
   ```python
   class SegmentMap:
       def __init__(self, translated_segments: Dict[str, str]):
           self.segments = translated_segments
           self.used_segments = set()  # Track which segments were used
           
       def get(self, path: str) -> Optional[str]:
           if path in self.segments:
               self.used_segments.add(path)
               return self.segments[path]
           return None
           
       def verify_all_used(self) -> List[str]:
           """Returns list of unused segment paths"""
           return [p for p in self.segments if p not in self.used_segments]
   ```

### Algorithm Implementation
The reassembly algorithm should follow these steps:

1. **Token Stream Processing**:
   ```python
   def reassemble_markdown(self, original_markdown: str, translated_segments: Dict[str, str]) -> str:
       tokens = self.md.parse(original_markdown)
       segment_map = SegmentMap(translated_segments)
       context = ReassemblyContext()
       result = []
       
       for i, token in enumerate(tokens):
           # Update context based on token
           self._update_reassembly_context(token, context)
           
           # Handle token based on type
           if self._is_translatable_content_token(token):
               translated = self._process_translatable_token(token, context, segment_map)
               result.append(translated)
           else:
               # Non-translatable token, preserve as-is
               result.append(self._render_token(token))
               
       # Verify all segments were used
       unused = segment_map.verify_all_used()
       if unused:
           self.logger.warning(f"Unused segments during reassembly: {unused}")
           
       return ''.join(result)
   ```

2. **Inline Content Handling**:
   ```python
   def _process_translatable_token(self, token, context, segment_map):
       if token.type == 'inline':
           # Get translated content for this path
           translated_text = segment_map.get(context.current_path)
           if translated_text:
               # Create a modified token with translated content
               return self._reassemble_inline_content(token, translated_text)
           
       return self._render_token(token)
   ```

3. **Inline Reassembly**:
   ```python
   def _reassemble_inline_content(self, token, translated_text):
       # For simple cases, just replace the content
       # For complex inline sequences (to be enhanced in 7.12/7.13),
       # this will need to parse the translated text and merge with
       # non-translatable elements from the original
       
       # Simple implementation for now:
       token_copy = copy.deepcopy(token)
       token_copy.content = translated_text
       return self.md.renderer.renderToken([token_copy], 0, {})
   ```

### Validation & Error Handling
1. Implement validation to ensure document integrity:
   ```python
   def _validate_reassembled_document(self, reassembled_markdown):
       """Verify the reassembled document is valid markdown"""
       try:
           tokens = self.md.parse(reassembled_markdown)
           # Additional structural validation can be added here
           return True
       except Exception as e:
           self.logger.error(f"Reassembly validation failed: {str(e)}")
           return False
   ```

2. Add error recovery for missing segments:
   ```python
   # In _process_translatable_token:
   translated_text = segment_map.get(context.current_path)
   if not translated_text:
       self.logger.warning(f"Missing translation for segment: {context.current_path}")
       # Fall back to original content
       translated_text = token.content
   ```

This implementation provides a solid foundation for basic reassembly while setting up the structure for the more complex inline handling that will be implemented in tasks 7.12 and 7.13.
</info added on 2025-05-05T14:22:18.000Z>
</info added on 2025-05-04T15:59:42.340Z>

<info added on 2025-05-04T16:09:34.090Z>
## Reassembly Implementation Challenges and Strategy

The reassembly task presents several technical challenges that justify its deferred status:

1. **Source Mapping Limitations**: 
   - Markdown-it-py's token system doesn't maintain perfect bidirectional mapping between source text and tokens
   - Position information is often approximate, especially with nested inline elements
   - Whitespace handling differs between parsing and rendering phases

2. **Alternative Implementation Approaches**:
   - **AST Transformation**: Instead of string replacement, consider working directly with the AST, replacing content at the node level before rendering
   - **Token Stream Modification**: Implement a token stream processor that replaces content in-place before rendering
   - **Custom Renderer**: Develop a Markdown-specific renderer (not HTML) that reconstructs Markdown syntax from modified tokens

3. **Error Recovery Strategy**:
   - Implement fallback mechanisms for when segment boundaries don't align perfectly
   - Create a validation system that can detect and report potential structure corruption
   - Consider a diff-based approach to identify and resolve structural inconsistencies

4. **Testing Framework**:
   - Develop specialized test fixtures for complex nested structures
   - Create property-based tests that verify structural invariants are maintained
   - Implement round-trip testing (original → segmented → translated → reassembled → parsed) to verify equivalence

This task should be revisited after the Granular Inline Parser is complete, as it will provide the necessary foundation for accurate segment boundary tracking and reconstruction.
</info added on 2025-05-04T16:09:34.090Z>

<info added on 2025-05-04T17:31:05.417Z>
<info added on 2025-05-06T09:14:23.000Z>
## Implementation Update: Block-Level Reassembly Strategy

The initial implementation of `reassemble_markdown` has been completed using Strategy A+ with the following approach:

```python
def reassemble_markdown(self, original_ast, translated_segments):
    """Replace content in the original AST with translated segments."""
    for path, translation in translated_segments.items():
        # Find the target token using the path
        target_token = self._find_token_by_path(original_ast, path)
        if not target_token or target_token.type != 'inline':
            self.logger.warning(f"Could not find valid inline token at path: {path}")
            continue
            
        # Replace content in the first child text token and remove subsequent children
        if target_token.children and len(target_token.children) > 0:
            # Find first text child
            for i, child in enumerate(target_token.children):
                if child.type == 'text':
                    # Replace content of first text token with full translation
                    child.content = translation
                    # Remove all subsequent children to prevent duplication
                    target_token.children = target_token.children[:i+1]
                    break
    
    # Render the modified AST back to markdown
    return self.renderer.render(original_ast)
```

Key implementation details:

1. **Path Resolution Fix**: Resolved inconsistencies between path generation during extraction and reassembly by:
   - Simplifying stack management in the extraction phase
   - Ensuring `get_element_path()` uses persistent counters across function calls
   - Adding path normalization to handle edge cases with list items and nested blocks

2. **Token Replacement Strategy**:
   - Locates the target inline token based on the stored path
   - Replaces only the content of the first child 'text' token with the full translation
   - Removes subsequent children within that inline token to prevent duplication
   - This approach preserves block-level structure while treating inline content as atomic units

3. **Error Handling**:
   - Added validation to detect missing tokens during reassembly
   - Implemented logging for path resolution failures
   - Added recovery mechanism to skip problematic segments rather than failing completely

All tests for block-level reassembly now pass successfully. The more complex task of preserving internal formatting within inline elements (like bold/italic within translated text) has been explicitly deferred to Subtask 7.11, which will implement the fine-grained inline reassembly using the Granular Inline Parser.
</info added on 2025-05-06T09:14:23.000Z>
</info added on 2025-05-04T17:31:05.417Z>

## 7. Implement WikiLink extraction [done]
### Dependencies: 7.1, 7.2, 7.4
### Description: Add support for extracting translatable text from WikiLinks
### Details:
Using TDD, write tests first for identifying and extracting translatable text from WikiLinks (e.g., the alias portion in `[[target|alias]]`). Implement a parser or extend the existing inline parser to correctly identify WikiLink components. Create TextSegment instances for the translatable portions (aliases) while preserving the non-translatable targets. Ensure the mapping maintains the relationship between extracted text and its precise location within the WikiLink syntax for accurate reassembly.

<info added on 2025-05-04T16:16:19.494Z>
Here's additional implementation information for the WikiLink extraction subtask:

```python
# Example WikiLink parser implementation structure
def wikilinks_plugin(md):
    # Regular expression for matching WikiLinks
    WIKILINK_RE = r'\[\[([^\]\|]+)(?:\|([^\]]+))?\]\]'
    
    def tokenize(state, silent):
        # Match WikiLink pattern
        match = re.search(WIKILINK_RE, state.src[state.pos:])
        if not match:
            return False
            
        # Skip if in code block
        if state.isInCode:
            return False
            
        # Extract target and alias
        target = match.group(1).strip()
        alias = match.group(2).strip() if match.group(2) else target
        
        # Create token sequence
        token_open = state.push('wikilink_open', '', 1)
        
        token_target = state.push('wikilink_target', '', 0)
        token_target.content = target
        token_target.translatable = False  # Mark as non-translatable
        
        if match.group(2):  # If alias exists
            token_sep = state.push('wikilink_separator', '|', 0)
            token_sep.translatable = False
            
            token_alias = state.push('wikilink_alias', '', 0)
            token_alias.content = alias
            token_alias.translatable = True  # Mark as translatable
        
        token_close = state.push('wikilink_close', '', -1)
        
        # Update parser position
        state.pos += match.end()
        return True
    
    md.inline.ruler.push('wikilink', tokenize)
```

For the text segment creation logic:

```python
def _extract_inline_text(self, tokens):
    segments = []
    for token in tokens:
        if token.type == 'wikilink_alias' and hasattr(token, 'translatable') and token.translatable:
            # Create a TextSegment for the alias portion
            segment = TextSegment(
                text=token.content,
                context=f"WikiLink alias for target: {self._get_wikilink_target(token)}",
                metadata={
                    "type": "wikilink_alias",
                    "target": self._get_wikilink_target(token)
                }
            )
            segments.append(segment)
    return segments

def _get_wikilink_target(self, alias_token):
    """Helper to find the target associated with an alias token"""
    # Implementation depends on token structure, but generally:
    # Look backward in the token stream to find the wikilink_target token
    # that precedes this alias token
    pass
```

When reassembling translated content, ensure the WikiLink structure is preserved:

```python
def _reassemble_wikilink(self, target, translated_alias):
    """Reconstruct a WikiLink with the translated alias"""
    return f"[[{target}|{translated_alias}]]"
```

Consider edge cases:
- WikiLinks with no alias (`[[target]]`) - decide whether to extract the target as translatable
- Nested formatting within aliases (`[[target|**bold** text]]`) - handle markdown within aliases
- Case sensitivity in targets - preserve original casing for targets
</info added on 2025-05-04T16:16:19.494Z>

## 8. Implement Obsidian attributes handling [done]
### Dependencies: 7.1, 7.2, 7.4
### Description: Add support for preserving Obsidian attributes during extraction
### Details:
Following TDD principles, write tests first for correctly identifying and preserving Obsidian attributes (e.g., `{ .some-attr }`) during text extraction. Implement functionality to recognize these attributes as non-translatable elements and ensure they are properly preserved during both extraction and reassembly processes. Update the visitor or parser to handle these special syntax elements.

<info added on 2025-05-04T16:20:23.933Z>
## Implementation Details for Obsidian Attributes Handling

### Regex Pattern Design
- Use a pattern like `/{([^{}]|\{[^{}]*\})*}/` to handle basic nesting
- Consider edge cases: escaped braces `\{`, inline code with braces, and attributes within other inline elements

### Attribute Structure Parsing
- Parse attributes into structured data:
  ```python
  def parse_attributes(attr_content):
      """Parse Obsidian attribute content into structured data"""
      attributes = {}
      # Handle class attributes (.class-name)
      class_match = re.findall(r'\.([a-zA-Z0-9_-]+)', attr_content)
      if class_match:
          attributes['class'] = class_match
          
      # Handle key-value pairs (key=value or key="quoted value")
      kv_matches = re.findall(r'([a-zA-Z0-9_-]+)=(?:"([^"]*)"|([^ }]*))', attr_content)
      for key, quoted_val, unquoted_val in kv_matches:
          attributes[key] = quoted_val if quoted_val else unquoted_val
          
      return attributes
  ```

### Token Handling Strategy
- Create a token type that preserves the entire attribute block as non-translatable
- Store original attribute text in token metadata for exact reconstruction
- Consider implementing a `reconstruct_attributes()` function to ensure proper reassembly

### Integration with Markdown-It
- Position the rule carefully in the parsing chain (before emphasis but after code_inline)
- Use `state.push()` with appropriate nesting levels
- Set `token.markup = '{}'` for proper syntax highlighting in debug views

### Testing Edge Cases
- Test attributes on headings: `## Heading {.class}`
- Test attributes on links: `[link](url){.class}`
- Test attributes with special characters: `{.class-name data-test="value with spaces"}`
- Test malformed attributes: `{incomplete` or `text { not closed properly`
</info added on 2025-05-04T16:20:23.933Z>

<info added on 2025-05-04T16:38:38.284Z>
## Implementation Approach for Obsidian Attributes

### Visitor Pattern Enhancement
- Extend the `MarkdownVisitor` class to recognize attribute tokens:
  ```python
  def visit_attribute(self, token, children):
      # Skip attribute tokens entirely during extraction
      # They should be preserved verbatim in the original document
      return None
  ```

### Attribute Position Tracking
- Implement position tracking to handle attributes attached to different elements:
  ```python
  class AttributeTracker:
      def __init__(self):
          self.attached_elements = {}  # Maps element_id to attribute data
          
      def register_attribute(self, element_id, attribute_data):
          self.attached_elements[element_id] = attribute_data
          
      def get_attributes_for(self, element_id):
          return self.attached_elements.get(element_id, None)
  ```

### Reassembly Considerations
- During reassembly, attributes must be reattached to the correct elements:
  ```python
  def reassemble_with_attributes(self, element, translated_text):
      element_id = element.get('id')
      attributes = self.attribute_tracker.get_attributes_for(element_id)
      
      if attributes:
          # Handle different attachment positions based on element type
          if element.get('type') == 'block':
              return f"{translated_text} {attributes}"
          elif element.get('type') == 'inline':
              # For inline elements, attributes typically follow immediately
              return f"{translated_text}{attributes}"
      return translated_text
  ```

### Performance Optimization
- Cache compiled regex patterns for attribute detection
- Use a two-pass approach: first identify all attributes, then process content
- Consider implementing a lookup table for common attribute patterns

### Compatibility Testing
- Test with various Obsidian plugins that extend attribute functionality
- Verify compatibility with CommonMark and GitHub Flavored Markdown
- Create test cases for attributes in multilingual documents
</info added on 2025-05-04T16:38:38.284Z>

## 9. Implement HTML content extraction [done]
### Dependencies: 7.1, 7.2, 7.4
### Description: Add support for handling HTML tags and extracting translatable content from within specified tags
### Details:
Using TDD, write tests first for identifying HTML tags within Markdown content and extracting translatable text from within specified tags. Implement functionality to parse HTML content, determine which parts should be translated based on configuration, and create appropriate TextSegment instances. Ensure HTML tags themselves are preserved as non-translatable elements. Add configuration options to specify which HTML tags should have their content extracted for translation.

<info added on 2025-05-04T17:00:48.988Z>
## Implementation Strategy for HTML Content Extraction

### HTML Parsing Approach
- Use Python's `html.parser.HTMLParser` or a library like `BeautifulSoup4` to properly parse HTML content within Markdown
- Create a custom HTML handler that can distinguish between tags that should be preserved vs. those whose content should be extracted

### Configuration Structure
```python
html_config = {
    "extract_content_from_tags": ["p", "h1", "h2", "h3", "h4", "h5", "h6", "li", "td", "th", "figcaption"],
    "preserve_tags": ["code", "pre", "script", "style"],
    "attribute_handling": {
        "alt": ["img"],  # Extract text from alt attributes of img tags
        "title": ["a", "img"]  # Extract text from title attributes
    }
}
```

### Processing Algorithm
1. When encountering HTML content, parse it into a DOM-like structure
2. Traverse the structure recursively:
   - For tags in `extract_content_from_tags`, create TextSegments for their content
   - For tags in `preserve_tags`, skip their content entirely
   - For specified attributes in `attribute_handling`, extract their values as separate TextSegments

### Edge Cases to Handle
- Nested HTML tags with different extraction rules
- Malformed HTML that might break the parser
- HTML entities that need proper decoding before translation
- Inline HTML mixed with Markdown formatting

### Testing Considerations
- Create test cases with complex nested HTML structures
- Test HTML with various attributes and entity encodings
- Verify correct handling of self-closing tags like `<img>` and `<br>`
</info added on 2025-05-04T17:00:48.988Z>

<info added on 2025-05-04T17:14:07.181Z>
<info added on 2025-05-05T09:15:22.456Z>
## Current Implementation Status and Limitations

### Implemented Features
- Basic detection and skipping of HTML block elements (`html_block`) in the Markdown AST
- Identification and skipping of inline HTML tags (`html_inline`) during text extraction
- Configuration option to control HTML handling behavior

### Known Limitations
- Text content within inline HTML tags (e.g., `<span>text</span>`) is currently extracted as part of the surrounding text block
- No differentiation yet between translatable and non-translatable HTML tags
- HTML attributes (like `alt` and `title`) aren't separately extracted

### Next Implementation Steps
1. Integrate proper HTML parsing using BeautifulSoup4
2. Implement the tag-specific extraction rules from the configuration
3. Add HTML attribute extraction according to the `attribute_handling` config
4. Create specialized TextSegment subclass for HTML content that preserves tag structure

### Example of Current vs. Target Behavior

**Current behavior with this markdown:**
```markdown
Some text with <span class="highlight">highlighted content</span> and more text.
```

Currently extracts as: `"Some text with highlighted content and more text."`

**Target behavior:**
```python
[
    TextSegment("Some text with "),
    HTMLSegment("<span class=\"highlight\">", "highlighted content", "</span>"),
    TextSegment(" and more text.")
]
```

This will be addressed in the upcoming implementation phases.
</info added on 2025-05-05T09:15:22.456Z>
</info added on 2025-05-04T17:14:07.181Z>

## 10. Enhance inline sequence parsing [done]
### Dependencies: 7.1, 7.2, 7.4, 7.7, 7.8, 7.9
### Description: Implement a secondary inline parser for complex mixed content
### Details:
Following TDD principles, write tests first for handling complex mixed inline sequences containing combinations of regular text, WikiLinks, attributes, HTML, and standard Markdown formatting. Implement a secondary parsing stage that can accurately identify and extract only the translatable portions while maintaining precise position information. Ensure the parser can handle nested structures and edge cases. Update the mapping data structure to accommodate these complex inline sequences for perfect reassembly.

<info added on 2025-05-04T17:23:57.000Z>
The refactored approach uses the token stream from `markdown-it-py` directly, which provides better context and position tracking. Key implementation details:

1. Modified `_extract_inline_text` to traverse the token tree and identify translatable segments while preserving their original positions.

2. Added token type handlers:
   - Links: Extracts text content but preserves URLs
   - Images: Captures alt text only
   - WikiLinks: Processes only the display portion (alias)
   - Code blocks, HTML, and attribute markers: Explicitly skipped

3. Position tracking now uses token metadata to maintain accurate source mapping, which solves the reconstruction issues we faced with the separate parser approach.

4. Added helper methods to determine token translatability based on both type and context.

5. Implemented a more robust text reassembly algorithm that uses the position information to correctly place translated content back into the original structure.

This approach significantly reduces complexity while maintaining the ability to handle nested structures and edge cases.
</info added on 2025-05-04T17:23:57.000Z>

## 11. Update reassembly for complex inline sequences [done]
### Dependencies: 7.6, 7.7, 7.8, 7.9, 7.10
### Description: Enhance the reassembly functionality to handle complex inline sequences
### Details:
Using TDD, write tests first for reassembling translated content back into complex inline sequences. Extend the existing reassembly functionality to handle the more complex mapping created for WikiLinks, Obsidian attributes, HTML content, and mixed inline sequences. Implement algorithms that can precisely reconstruct these elements with translated content while preserving all non-translatable syntax. Add validation to ensure perfect reconstruction of the original format with only the translatable text replaced.

<info added on 2025-05-04T17:32:19.375Z>
The simplified Strategy B approach makes sense as an initial implementation, but we should document its limitations and future improvements:

For reassembly of complex inline sequences, the current implementation replaces entire inline token children with a single text token containing the translated content. This approach:

1. Preserves document structure at block level (paragraphs, headers, lists)
2. Maintains correct placement of translated content
3. Trades off preservation of inline formatting for implementation simplicity

Known limitations:
- Internal formatting (bold, italic, links) within translated text is lost
- WikiLinks, attributes, and HTML tags are treated as opaque blocks

Future enhancement path:
- Implement token-level mapping between source and translated content
- Store formatting metadata during extraction phase
- Use alignment algorithms to reapply formatting to translated text
- Consider implementing format-preserving translation markers that survive the translation process
- Add unit tests specifically for format preservation scenarios

This simplified approach provides a working solution while deferring the more complex token-level formatting preservation to a future iteration.
</info added on 2025-05-04T17:32:19.375Z>

## 12. Implement Granular Inline Parser [done]
### Dependencies: 7.1
### Description: Develop a secondary parser specifically for inline content within blocks (paragraphs, list items, etc.).
### Details:
Using TDD, implement a parser that tokenizes inline content into distinct units: plain text, standard Markdown (bold, italic), inline code, standard links, images, WikiLinks (target and alias), Obsidian attributes (`{...}`). It should handle nested structures and produce a detailed token stream or mini-AST for each inline sequence, preserving non-translatable syntax precisely. This parser will be used by the main AST visitor.

<info added on 2025-05-04T16:10:09.980Z>
# Implementation Plan (Subtask 7.12 - Granular Inline Parser)

## Token Types and Structure
```python
from enum import Enum, auto
from dataclasses import dataclass
from typing import List, Optional, Dict, Any

class InlineTokenType(Enum):
    TEXT = auto()
    EMPHASIS = auto()  # *italic*
    STRONG = auto()    # **bold**
    CODE = auto()      # `code`
    LINK = auto()      # [text](url)
    IMAGE = auto()     # ![alt](src)
    WIKILINK = auto()  # [[target]] or [[target|alias]]
    ATTRIBUTE = auto() # {key: value}

@dataclass
class InlineToken:
    type: InlineTokenType
    content: str  # Processed content
    raw: str      # Original raw text
    attrs: Dict[str, Any] = None  # Additional attributes
    children: List['InlineToken'] = None  # For nested tokens
```

## Parser Implementation Strategy
The parser should use a state machine approach with these states:
- NORMAL: Processing regular text
- EMPHASIS: Inside `*` markers
- STRONG: Inside `**` markers
- CODE: Inside backticks
- LINK_TEXT: Inside `[...]`
- LINK_URL: Inside `(...)`
- WIKILINK: Inside `[[...]]`
- ATTRIBUTE: Inside `{...}`

Key implementation considerations:
- Use a stack to track nested formatting states
- Handle escaped characters (`\*`, `\[`, etc.)
- Process WikiLinks with regex like `\[\[(.*?)(?:\|(.*?))?\]\]`
- For attributes, parse JSON-like syntax while preserving original format

## Example Test Cases
```python
def test_mixed_formatting():
    parser = InlineParser()
    text = "This is **bold with *italic* inside** and `code with **ignored** formatting`"
    tokens = parser.parse(text)
    
    assert len(tokens) == 3
    assert tokens[0].type == InlineTokenType.TEXT
    assert tokens[0].content == "This is "
    
    assert tokens[1].type == InlineTokenType.STRONG
    assert tokens[1].content == "bold with italic inside"
    assert len(tokens[1].children) == 3  # Text, Emphasis, Text
    assert tokens[1].children[1].type == InlineTokenType.EMPHASIS
    
    assert tokens[2].type == InlineTokenType.CODE
    assert tokens[2].content == "code with **ignored** formatting"
```

## Integration with AST Visitor
When integrating with the main AST visitor:
- Pass block content to `InlineParser.parse()`
- Use visitor pattern to process inline tokens
- Preserve raw content for non-translatable elements
- Map token types to appropriate HTML/output elements
</info added on 2025-05-04T16:10:09.980Z>

<info added on 2025-05-04T17:21:20.708Z>
<info added on 2025-05-05T09:15:22.103Z>
# Implementation Progress Update

## Current Implementation Status
The basic `InlineParser` has been implemented with the following components:

```python
class InlineParser:
    def __init__(self):
        self.tokens = []
        self.state_stack = []
        self.current_token = None
        self.buffer = ""
        
    def parse(self, text):
        # Main parsing logic
        # Returns list of InlineToken objects
```

## Key Features Implemented
- State machine transitions for basic formatting elements
- Stack-based approach for handling nested formatting
- Regex-based extraction for WikiLinks with pattern `\[\[(.*?)(?:\|(.*?))?\]\]`
- Simple attribute parsing with `{key: value}` format support
- Character-by-character processing with lookahead for ambiguous syntax

## Current Limitations and Next Steps
1. **Complex Nesting**: Need to improve handling of deeply nested structures like `***bold italic***` or `**_italic bold_**`
2. **Reference Links**: Not yet supporting Markdown reference-style links `[text][ref]`
3. **HTML Entities**: Need to properly handle HTML entities like `&amp;`, `&#39;`, etc.
4. **Performance Optimization**: Current character-by-character approach may be inefficient for large documents
5. **Attribute Validation**: Need more robust parsing of complex attribute structures

## Example Usage
```python
from processing.inline_parser import InlineParser

parser = InlineParser()
tokens = parser.parse("Check out [[Project X|this project]] with **important** details")

# Access parsed tokens
for token in tokens:
    if token.type == InlineTokenType.WIKILINK:
        target = token.attrs.get("target")
        alias = token.attrs.get("alias")
        # Process WikiLink...
```

## Test Coverage
Current test suite covers:
- Basic formatting (emphasis, strong, code)
- Simple nesting (bold with italic inside)
- WikiLinks with and without aliases
- Attribute parsing for simple key-value pairs
- Escaped character handling

Additional tests needed for edge cases and complex combinations.
</info added on 2025-05-05T09:15:22.103Z>
</info added on 2025-05-04T17:21:20.708Z>

## 13. Define and Implement HTML Handling Strategy [done]
### Dependencies: 7.1
### Description: Define and implement the strategy for handling raw HTML tags within Markdown content.
### Details:
Decide on the HTML handling strategy (e.g., ignore all HTML, translate content of specific tags, preserve all HTML). Implement the chosen strategy within the text extraction and reassembly logic. If extracting content, ensure the mapping correctly identifies text originating from HTML. Add configuration options if the strategy needs to be user-adjustable. Use TDD.

<info added on 2025-05-04T17:35:24.500Z>
Based on the confirmed strategy, we'll implement a simple 'skip HTML' approach with the following implementation details:

1. For `html_block` tokens: Completely ignore these blocks during text extraction, treating them as non-translatable content.

2. For `html_inline` tokens: Skip the HTML tags themselves but extract any inner text content if it exists between tags.

3. Implementation approach:
   - In the token processor, add specific handlers for `html_block` and `html_inline` token types
   - For `html_block`, return an empty string or None to exclude it from extraction
   - For `html_inline`, use regex pattern like `r'>([^<]+)<'` to extract only text between tags if needed

4. Add a configuration flag `skip_html` (default: True) to allow users to toggle this behavior.

5. Document this limitation clearly in the README, noting that HTML content will not be translated in the current implementation.

6. Add unit tests specifically for HTML handling scenarios:
   - Test with inline HTML like `This is <span>important</span> text`
   - Test with HTML blocks like `<div>Block content</div>`
   - Test with nested HTML structures

This approach prioritizes simplicity while acknowledging the more robust solution will come in task #21.
</info added on 2025-05-04T17:35:24.500Z>

## 14. Fix Failing Utility Tests (Hashing & Normalization) [done]
### Dependencies: None
### Description: Address the 5 failing tests identified in `translation-py/tests/utils/test_hashing.py` and `translation-py/tests/utils/test_normalization.py`. These tests cover edge cases for YAML hashing (None input, invalid types, exclude fields, non-string keys) and Markdown blank line normalization.
### Details:


